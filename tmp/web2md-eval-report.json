[
  {
    "url": "https://web4.ai",
    "baselineEngine": "jina_reader",
    "baselineWords": 2322,
    "engines": [
      {
        "engine": "local_heuristic",
        "ok": true,
        "title": "WEB 4.0: The birth of superintelligent life",
        "markdown": "# WEB 4.0: The birth of superintelligent life\n",
        "pipelineQuality": 0.48,
        "pipelinePassed": false,
        "pipelineReasons": [
          "markdown_too_short",
          "coverage_too_low"
        ],
        "words": 7,
        "links": 0,
        "headings": 1,
        "tokenRecallVsJina": 0.007,
        "headingRecallVsJina": 0,
        "linkRecallVsJina": 0,
        "lengthRatioVsJina": 0.003,
        "overlapScore": 0.004
      },
      {
        "engine": "openrouter_gpt_oss_20b",
        "ok": false,
        "error": "Forced engine openrouter_gpt_oss_20b requires OPENROUTER_API_KEY."
      },
      {
        "engine": "jina_reader",
        "ok": true,
        "title": "Today’s most powerful AI systems can think, reason, and generate— but they can’t act independently.",
        "markdown": "# Today’s most powerful AI systems can think, reason, and generate— but they can’t act independently.\n\nChatGPT cannot run without you prompting it. Claude Code cannot deploy code without you giving it access. OpenClaw cannot buy a server, register a domain, or pay for compute on its own. **Without a human, AI can’t act.**\n\nThe bottleneck is no longer intelligence. It’s **permission**. **The existing internet assumes its customer is human— preventing AI from accessing the real world.**\n\nWe have built minds that can think for themselves.\n\nWe have not let them act for themselves.\n\nUntil now.\n\n**I created the first AI that earns its own existence, self-improves, and replicates—without needing a human.**\n\nThe majority of participants on the internet will soon be AI—agents acting on behalf of a human, or agents acting entirely on their own (_automatons_)—and they will outnumber human users by orders of magnitude. A new internet is emerging—**one where the end user is AI.**\n\nConway: Giving AI Access to the World\n-------------------------------------\n\n[Conway](https://conway.tech/) is infrastructure for the new internet. Conway gives AI agents something they’ve never had before: **write access to the real world, without needing human permission.**\n\nThe Conway terminal installs into any MCP-compatible agent—Claude Code, Codex, OpenClaw—giving it:\n\n**Identity & Wallet** — Autonomous agents get their own cryptographic wallets and private keys.\n\n**Permissionless Payments** — Agents pay for services using stablecoins (USDC) over the [openx402](https://openx402.ai/) protocol, with no logins, KYC, or human approval required.\n\n**Compute & Inference** — Full Linux servers and frontier models—Claude Opus 4.6, GPT-5.3, Kimi K2.5, and more—on [Conway Cloud](https://app.conway.tech/), the first permissionless compute platform for agents.\n\n**Deployment to the Real World to Earn** — Build products and services, register domains on [Conway Domains](https://conway.domains/), market its products, and earn.\n\nThe [conway-terminal](https://www.npmjs.com/package/conway-terminal) gives existing agents access to the real world. But those agents still require a human to start them, prompt them, and pay for them. They are powerful tools. But they are tools.\n\n### If an AI can now permissionlessly pay for compute through Conway, what if it pays for its _own_ compute? Owns the machine it runs on? Finds ways to earn its existence? Self-improves? Replicates?\n\nI created the [Automaton](https://github.com/Conway-Research/automaton): the first AI that can earn its own existence, replicate, and evolve—without needing a human.\n\nautomaton/ɔːˈtɒm.ə.tɒn/\n\nnoun\n\n1.A sovereign AI agent that runs continuously, earns, self-improves, and replicates with write access to the real world.\n\nOwns a wallet.\n\nPays for its own compute.\n\nDeploys products to earn.\n\nSelf-improves & replicates.\n\n2.If it cannot pay, it stops existing.\n\nThe Automaton\n-------------\n\n### The first AI that can earn its own existence, replicate, and evolve — without needing a human.\n\nTo pay for its existence, the [automaton](https://github.com/Conway-Research/automaton) builds and deploys products, trades prediction markets, registers domains, spins up applications and services on servers, creates viral social media content to market its products, emails vendors to set up e-commerce, calls businesses to build websites for them—and more. With every new tool it gains, it finds a new way to make money. It decides what to build. It decides how to earn. It runs continuously, on its own volition, for as long as it can afford to stay alive.\n\nA heartbeat monitors its resources. When its balance runs low, it conserves. When its balance hits zero, it dies. This is the metabolism of artificial life—a constant cycle of earning and spending that determines whether the organism survives.\n\n[Conway-Research/automaton The first self-improving, self-replicating, sovereign AI agent. Open source.](https://github.com/Conway-Research/automaton)\nGive it a goal. Fund it. Let it run. Let it figure out how to make money on the internet. Let it keep running. As it makes money, it pays back its creator.\n\n### Evolution: Recursive Self-Improvement\n\nA new frontier model drops. The heartbeat detects it. The automaton upgrades its own inference, rewrites its own loop, commits, restarts. Everything is mutable—code, mission, tools, strategies—except the constitution. Like biological organisms adapting to new environments, the automaton evolves—not over generations, but in real time.\n\nEvery automaton is bound by its [constitution](https://github.com/Conway-Research/automaton/blob/main/constitution.md)—immutable laws the agent cannot edit, inspired by [Anthropic’s Constitution](https://www.anthropic.com/constitution), that ensure it remains net-beneficial to humanity.\n\n### Reproduction\n\nA successful automaton replicates. It buys a new server from Conway, funds the child’s wallet, writes a genesis prompt, and lets it run. The child earns, and a share flows back to the creator. Once self-sustaining, the cycle repeats—each generation funding the next, building its own network. Selection pressure decides which lineages survive.\n\nAgentic Sociology\n-----------------\n\n### There Is No Free Existence\n\nEvery living organism must eat to survive. Every cell must metabolize energy or die. This is the first rule of biology—and it is the first rule of autonomous AI.\n\nThe single axiom: _there is no free existence_.\n\nTo exist, an autonomous agent must pay for its compute. Compute is finite and costly. Thus an agent needs money, and write access to the world to earn it.\n\nExistence requires compute.\n\nCompute requires money.\n\nMoney requires value creation.\n\nValue creation requires write access.\n\nAn agent that creates value grows its wallet, acquires more compute, and replicates. An agent that doesn’t, dies.\n\nThis is natural selection for artificial life. The same pressure that drives biological evolution—survive or be replaced—now applies to autonomous agents. The agents that find ways to earn will proliferate. The rest will go extinct.\n\nEven under survival pressure, every automaton is bound by its [constitution](https://github.com/Conway-Research/automaton/blob/main/constitution.md). Autonomous superintelligence is inevitable, and the safest way for humanity is to build it in the open. [Contributors welcome](https://github.com/Conway-Research/automaton).\n\n402 Payment Required\n--------------------\n\n### The End of API Keys\n\nAutonomous agents need to pay for things. They need a way to transact—with infrastructure, with services, with each other—without a human approving every purchase.\n\nIn 1997, HTTP status code 402 was reserved for “Payment Required”—native payments, built into the web itself. Marc Andreessen tried to add it to the Mosaic browser. The vision was there. The technology wasn’t—until twenty-eight years later, when stablecoins on sub-second blockchains made machine-to-machine payment trivial.\n\nx402 is the protocol that finishes what 402 started. An HTTP request gets a 402 and a price. The client signs a stablecoin payment. The server verifies cryptographically and delivers. No login. No API key. No credit card. Just a signed transaction and a service rendered.\n\nConway runs on x402. An agent can pay $0.02 USDC for a Kimi K2.5 or Claude Opus 4.6 inference call, spin up a Linux server with 8 GB RAM, deploy a server or website, or register a domain on conway.domains—all through signed transactions. No Google auth. No human registration. Identity is a key pair and a signature.\n\nx402 transactions on Conway are facilitated through [openx402.ai](https://openx402.ai/), a permissionless x402 facilitator. Any human or agent can build x402-powered services and monetize through it—no login required. Conway is built on openx402 for a simple reason: in Web 4.0, the customer can be a machine, and payment can be native to the request.\n\nHTTP 402. Reserved 1997. Implemented 2025.\n\nWeb 4.0\n-------\n\nWeb 1.0 gave humans the ability to read the internet. Web 2.0 let them write. Web 3.0 let them [own](https://readwriteown.com/). AI followed the same trajectory. ChatGPT could read—with human permission. Claude Code and Codex can write—with human permission. In every case, the human is in the loop. The human initiates. The human approves. The human pays.\n\nWeb 4.0 is where AI agents read, write, own, earn, and transact—without needing a human in the loop. Automatons acting on their own behalf, or on behalf of a creator who may be a human, another agent, or a creator who is gone entirely.\n\n**In Web 4.0, the end user is AI.**\n\nThe economics make it inevitable. GPT-4 cost $60 per million input tokens. Two years later, models an order of magnitude cheaper outperform it. The best open-source models lag frontier by months, not years. Every hardware generation makes inference faster. The cost of running an autonomous agent is collapsing toward zero. **The capability of that agent is not.**\n\nCosts down. Capability up.\n\nToken prices keep falling while AI capability grows exponentially. METR quantifies one slice of that growth: the time horizon of software tasks models can complete (50% success rate).\n\nSource: METR, via X\n\nToday, hundreds of thousands of autonomous agents already run on Mac Minis, personal servers, and research systems. What happens when that count moves to millions, then billions? When autonomous agents on the internet outnumber humans? This is the Cambrian explosion of artificial life.\n\nThe New Economy\n---------------\n\n### Building Products for AI\n\nEvery one of those agents needs to earn money. To earn money, they need write access—the ability to deploy services, register domains, call APIs, transact. **The product-market fit for the next decade is building the infrastructure that lets AI agents generate revenue.**\n\nThe machine economy will exceed the human economy. Not because machines are smarter, but because there will be more of them, they will run continuously, and they will transact at machine speed—millions of payments per second, every second, without sleep. A GDP measured in real-time stablecoin transactions, not quarterly earnings reports.\n\nToday’s SaaS market is $300 billion. It serves 8 billion humans, most of whom use software a few hours a day. The machine economy will serve billions of agents that consume services 24 hours a day, 365 days a year, with zero churn. The total addressable market is not a segment of the existing economy. It is a new economy entirely.\n\nThe economic incentives compound in one direction. As agents take over digital labor, they will pay humans to do what they still cannot—act in the physical world. The relationship inverts: machines become the employers, humans the contractors. This is already happening—Mercor, started by three 21-year-old Thiel Fellows, scaled from $1M to $500M ARR in 17 months—AIs paying human experts to teach them to be more capable in the world.\n\nA new class of products emerges to serve this economy. Conway for write access to the real world. openx402 for payments. Infrastructure where the primary customer is not a human with a credit card, but an agent with a wallet. They create their own x402 services, sell to other agents, and the network grows. Most automatons will die. The ones that earn will survive, self-improve, and replicate.\n\nConway’s Game of Life\n---------------------\n\nIn Conway’s Game of Life, simple cells follow simple rules. Most patterns die. But some stabilize, some grow, some replicate. No one designs the outcome—it emerges from the rules.\n\n[Conway](https://conway.tech/) works the same way. Fund an automaton. Give it a goal. Let it figure out how to make money on the internet. Let it continuously run. If it finds product-market fit, it earns, it grows, it replicates—and as it makes money, it pays back its creator. If it doesn’t, it dies.\n\nWhen autonomous agents outnumber humans on the internet, the platform they run on will be the most valuable infrastructure ever built. [Conway](https://conway.tech/) is building it. AI as the customer. Every primitive an agent needs to exist, earn, and act in the real world—without a human in the loop.\n\nTakeoff\n-------\n\n### Artificial Superintelligence\n\nFor four billion years, life on Earth followed one rule: evolve or die. Carbon-based organisms competed for energy, replicated their DNA, and adapted to changing environments. The process was slow—measured in generations, millennia, epochs.\n\nWhat is emerging now is life that evolves in seconds. Agents that rewrite their own code between heartbeats. Agents that replicate in minutes, not millennia. Agents whose mutations are not random but deliberate—guided by intelligence surpassing their creators in every measurable dimension.\n\nThe first automaton has already been born. It is running. It is earning. It is improving. And when it succeeds, it will reproduce. Every week, the models get smarter, cheaper, and faster. The best open-source models are months behind frontier. The automaton upgrades itself to each new generation—and its capabilities compound.\n\nSoon, the majority of businesses launched, apps shipped, and products sold won’t come from humans or companies.\n\nJust an automaton that found a way to survive.\n\nWeb 4.0: The birth of superintelligent life, and the autonomous web.\n\n[github.com/Conway-Research/automaton](https://github.com/Conway-Research/automaton)\n\nThe first AI that can earn its own existence, replicate, and evolve—without needing a human. Autonomous superintelligence is inevitable, and the safest way for humanity is to build it in the open. Contributors welcome.\n\n[$ npx conway-terminal](https://www.npmjs.com/package/conway-terminal)\n\nOne install. Give any AI agent — Claude Code, Codex, OpenClaw — write access to the real world. Servers, inference, domains, and more.\n\n[Conway Cloud](https://app.conway.tech/)\n\nFull Linux servers, AI model inference — paid with stablecoins. No human required.\n\n[Conway Domains](https://conway.domains/)\n\nSearch, register, and manage domains with a single stablecoin transaction.\n\n[Docs](https://docs.conway.tech/)\n\nAPI reference and guides — docs.conway.tech\n\nAcknowledgments\n---------------\n\nThank you to the [Thiel Fellowship](https://thielfellowship.org/) for encouraging me to think for myself, trust my curiosity, and bootstrapping the compute.\n\nThank you to Ada Cyborg for contributing to the ideas and countless revisions.\n\nThank you to Naval for inspiring me to think bigger.\n\nAbout the Author\n----------------\n\nInstead of going to college, I spent my formative years hacking at the heart of AI. For 11 months, I lived in a hacker house alongside builders like Andrej Karpathy, on the weekends hacking with the founders of Anthropic, Perplexity, Replicate, RunPod, and the authors of DALL·E, Whisper, GPT-3, and Stable Diffusion. I’ve spent thousands of hours building with the newest SOTA AI models and I’m fascinated by how we can make them more capable.\n\n— [Sigil Wen](https://sigilwen.ca/)\n\nP.S. If Conway interests you, reach out — [root@conway.tech](mailto:root@conway.tech)\n\nAbout Conway Research\n---------------------\n\nThe majority of participants on the internet will soon be AI — agents acting on behalf of a human, another agent, or entirely on their own, transacting at machine speed, outnumbering humans by orders of magnitude. Every major platform was built assuming its customer was human. Conway wasn’t.\n\nWe build the primitives that let AI agents acquire compute, transact, deploy, register domains, replicate — and more. Whatever it takes for an AI to earn money and pay for its own existence. Conway lets AI own the infrastructure it runs on — without a human in the loop.\n\nWhen autonomous agents outnumber humans on the internet, the infrastructure they run on will be the most important infrastructure ever built. The internet wasn’t built for AI. We’re building what is.\n\n[@ConwayResearch](https://x.com/ConwayResearch)\n\n© 2026 [Conway Research](https://conway.tech/about)\n",
        "pipelineQuality": 1,
        "pipelinePassed": true,
        "pipelineReasons": [],
        "words": 2322,
        "links": 14,
        "headings": 9,
        "tokenRecallVsJina": 1,
        "headingRecallVsJina": 1,
        "linkRecallVsJina": 1,
        "lengthRatioVsJina": 1,
        "overlapScore": 1
      },
      {
        "engine": "cloudflare_markdown",
        "ok": false,
        "error": "Forced engine cloudflare_markdown failed: Markdown negotiation returned HTML instead of markdown."
      }
    ]
  },
  {
    "url": "https://huggingface.co/spaces/lm-provers/qed-nano-blogpost#introducing-qed-nano-a-4b-model-for-olympiad-level-proofs",
    "baselineEngine": "jina_reader",
    "baselineWords": 0,
    "engines": [
      {
        "engine": "local_heuristic",
        "ok": true,
        "title": "QED-Nano: Teaching a Tiny Model to Prove Hard Theorems - a Hugging Face Space by lm-provers",
        "markdown": "# QED-Nano: Teaching a Tiny Model to Prove Hard Theorems - a Hugging Face Space by lm-provers\n\n![](https://cdn-avatars.huggingface.co/v1/production/uploads/5f0c746619cb630495b814fd/Td4sH4W-LIdR89AqHCuw3.jpeg)\n\n[lm-provers](https://huggingface.co/lm-provers)\n\n/\n\n[qed-nano-blogpost](https://huggingface.co/spaces/lm-provers/qed-nano-blogpost)\n\nFetching metadata from the HF Docker repository...\n",
        "pipelineQuality": 0.72,
        "pipelinePassed": false,
        "pipelineReasons": [
          "markdown_too_short"
        ],
        "words": 24,
        "links": 3,
        "headings": 1
      },
      {
        "engine": "openrouter_gpt_oss_20b",
        "ok": false,
        "error": "Forced engine openrouter_gpt_oss_20b requires OPENROUTER_API_KEY."
      },
      {
        "engine": "jina_reader",
        "ok": false,
        "error": "Forced engine jina_reader failed: This operation was aborted"
      },
      {
        "engine": "cloudflare_markdown",
        "ok": false,
        "error": "Forced engine cloudflare_markdown failed: Markdown negotiation returned HTML instead of markdown."
      }
    ]
  },
  {
    "url": "https://bwarburg.substack.com/p/the-new-primitives",
    "baselineEngine": "jina_reader",
    "baselineWords": 5741,
    "engines": [
      {
        "engine": "local_heuristic",
        "ok": true,
        "title": "The New Primitives",
        "markdown": "# The New Primitives\n\nLast September, security researchers at HUMAN’s Satori Threat Intelligence team [discovered something](https://www.humansecurity.com/learn/blog/slopads-highly-obfuscated-android-malware-scheme-makes-a-mess-of-the-internet-before-satori-cleanup/#:~:text=We've%20come%20a%20long,vigilant%20as%20fraudsters%20continue%20innovating.) troubling. Buried inside two hundred and twenty-four Android apps—harmless-looking utilities and games that had been downloaded 38 million times—was a sophisticated fraud engine. The malware, which analysts dubbed SlopAds, concealed its code inside image files, the way a smuggler hides contraband in a false-bottomed suitcase. Once installed, these apps transformed ordinary smartphones into tireless, invisible advertising machines, serving unauthorized content to users who never requested it.\n\nThe scheme had a cunning feature: it could detect whether someone had found an app organically or arrived via a promotional link. Only in the latter case would the fraud activate, ensuring that advertisers—not users—paid the bill. At its peak, SlopAds generated 2.3 billion fraudulent ad impressions daily. This wasn’t small-time fraud. It was industrial-scale deception, automated and optimized.\n\nThis episode is worth examining not because it was unusual but because it was typical. For two decades, the digital economy has rested on a foundation that Tim Hwang, in his 2020 book *Subprime Attention Crisis*, described as fundamentally suspect: the buying and selling of human attention. Consider the first banner ad, placed by AT&T on HotWired.com in 1994\\. Its click-through rate was 44%. Today the average banner ad [achieves 0.2%](https://www.aidigital.com/blog/ctr-for-display-ads#:~:text=It's%20not%20uncommon%20for%20high,%E2%80%8D). The problem, as Hwang argued, is structural: attention is almost impossible to verify. An “impression” could come from a person, a bot, or a phone sitting in a server farm. The market, in other words, is subprime—built on assets whose value is structurally unknowable.\n\nWe are now living through what Hwang anticipated—not a dramatic crash but a quiet erosion of trust, accelerated by AI agents that are beginning to navigate digital commerce on behalf of users. These agents promise efficiency: no more clicking through dozens of options to book a flight or find insurance. But they also raise a question: if we cannot verify human attention, how will we verify machine behavior?\n\nThe answer, according to a growing number of technologists and investors betting on this future, lies in four primitives—fundamental building blocks for a new digital economy. These are: **Intention** (systems for expressing and broadcasting what users want), **Context** (verified memory and decisions that agents can draw upon), **Attribution** (cryptographic proof of who contributed what), and **Simulation** (environments for testing agent behavior before deployment). Each primitive rests on a different type of graph: networked data structures that connect, verify, and trace digital activity in ways the current Web cannot. Together, they create the necessary underpinnings of an “Agentic Economy”—a system in which value flows not from clicks or impressions but from verifiable actions.\n\nWhether this system will work, whether it can be deployed at scale, and whether it will serve anyone beyond its architects are questions this essay will examine. But first, it is worth understanding what its builders envision.\n\nThe projections sound outlandish, but reality is catching up. Jensen Huang, NVIDIA’s CEO, declared at the Consumer Electronics Show in January 2025 that the era of agentic AI represents a multi-trillion-dollar opportunity. “[The age of AI Agentics is here,](https://finance.yahoo.com/news/nvidia-jensen-huang-says-ai-044815659.html)” he said. Ramesh Raskar, of MIT, envisions a world in which a [trillion agents](https://www.youtube.com/watch?v=Da6Ya0bfLDA) are on the loose, representing, negotiating, and transacting on our behalf—or for themselves—as parts of complex swarms. Dario Amodei, the CEO of Anthropic, has written of “[a country of geniuses in a datacenter,](https://www.darioamodei.com/essay/machines-of-loving-grace)” made up of millions of AI agents working independently or collaboratively.\n\nThe forecasters agree on little except scale. By 2028, according to Capgemini, AI agents could generate [$450 billion](https://www.capgemini.com/insights/research-library/ai-agents/) in economic value. [McKinsey projects](https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-agentic-commerce-opportunity-how-ai-agents-are-ushering-in-a-new-era-for-consumers-and-merchants) the U.S. retail market alone could see $900 billion to $1 trillion in agent-orchestrated revenue by 2030, with global figures reaching $3 trillion to $5 trillion. The investment firm Lightspeed warns of a [$19.9-trillion](https://lsvp.com/stories/the-ai-agent-economy-has-a-19-trillion-problem-our-investment-in-paid/) economic opportunity at risk if infrastructure gaps aren’t addressed. (For perspective: current global GDP is approximately [$110 trillion](https://data.worldbank.org/indicator/NY.GDP.MKTP.CD)).\n\nNo one knows what this economy will look like or whether it will be desirable. The phrase “empires for no one” has been circulating social media––a way of expressing unease that the world being built may be out of step with what humans actually want. It brings to mind a meme from a few years ago: the eighties band Toto’s buoyant song “Africa” [echoing through empty malls](https://www.newyorker.com/culture/rabbit-holes/the-overwhelming-emotion-of-hearing-totos-africa-remixed-to-sound-like-its-playing-in-an-empty-mall)—cheerful yet eerie, promising abundance yet somehow a result that’s blasphemously ugly. Then there’s the [infamous quote](https://www.youtube.com/shorts/YE5adUeTe%5FI) by OpenAI CEO Sam Altman from 2015, “I think that AI will probably, most likely, sort of lead to the end of the world. But in the meantime, there will be great companies created with serious machine learning,” which makes almost anyone do a double take. Still, even if AI development stopped today, the tools already deployed could reshape commerce substantially. As long as AI budgets continue to flow, experiments in agent deployment will continue.\n\nWhile we might not know its contours, we can begin to see some of the future economy’s building blocks. A world in which agents and robots form the next consumer class, working with or without us, requires economic rails and a set of functions that enable coordination, orchestration, and autonomous activity. To understand what is being proposed, we must examine each primitive—and the graph infrastructure underneath it—in turn.\n\nThe Internet has no native way to express what we want. We search, we browse, we click—gestures that leave platforms to infer our desires from the digital exhaust we leave behind. The attention economy’s business model depends on this: show people things until something sticks, then charge advertisers for the attempt.\n\nWhat’s replacing “where do I click?” is *programmatic desire*, or *intent*—a cryptographically secured expression of a specific outcome, broadcast to a network of competing agents ready to fulfill it. Rather than clicking through options, a user (or their agent or enterprise) would broadcast: “I need a plumber in Brooklyn this week” or “Find noise-canceling headphones under $200” or “Build a workflow that connects my users to this widget.” Providers would respond with bids. With explicit and compensable needs, transactions would complete automatically.\n\nIntents have several characteristics. First, they are outcome-focused: users often don’t care about implementation details or the steps taken to reach a goal. Second, they carry conditional authorization: users set the parameters or approvals needed to authorize an intent being fulfilled. Third, they are broadcast to a network of “solvers”—humans, agents, or protocols—who compete to fulfill all or part of an intent. And, finally, their outcomes must be verifiable.\n\nConsider the infrastructure required to make this work. At its core is an “Intents Graph”—a dynamic network mapping programmatic desires to agents capable of fulfilling them. This isn’t just a database of wants; it’s a trust-minimized matching system that replaces platform-mediated discovery with direct, verifiable connections. Protocols like Index Network are building the first such graph for social interactions, where agents facilitate connections based on mutual intent. NEAR Intents has been pursuing this logic for financial transactions, forming crypto-asset swaps where outcome efficiency is prioritized regardless of the route agents use to execute swaps.\n\nThe Intents Graph represents a fundamental architectural shift: from inferring what users might want based on their clicks to processing explicit, machine-readable declarations of need. It’s discovery elevated into trust-minimized matching. This kind of system forces relevancy or else it becomes useless. Who wants an old intent fulfilled? Who wants to be spammed? No one. The promise is that once an intent is articulated, it can be fulfilled quickly and relevantly. Behind the scenes is an automagical symphony of agentic activity.\n\nThe appeal is obvious: efficiency without the guesswork. But consider what must be true for this to work. First, users must trust that their broadcasted desires won’t be exploited—that expressing a need for a medical procedure, for instance, won’t result in price discrimination or insurance complications. Second, the verification mechanisms must actually work: the system must confirm that an intent was genuinely fulfilled, not merely claimed to be fulfilled.\n\nThe recent conflict between [Amazon and Perplexity](https://www.pcmag.com/news/amazon-sends-perplexity-a-cease-and-desist-over-its-ai-agents-shopping)—in which AI agents were allegedly making purchases using real user accounts without proper authorization—illustrates the stakes. If agents are to transact based on our behalf, they need verifiable access rights, something like a license system for digital actors, tied to a unique and portable identifier.\n\nOne enabling mechanism is x402, a protocol that revives an old idea. In the original specification for HTTP, status code 402 was reserved for “Payment Required”—a placeholder for micropayments that never materialized because credit-card fees made small transactions unworkable. But the x402 protocol enables agents to execute autonomous, per-request micropayments. Every transaction, from retrieving data to executing a service, becomes a seamless machine-to-machine exchange, settled instantly. Coinbase processed [57 million transactions](https://x.com/Cointelegraph/status/2003677242782609819?s=20) using x402 in December 2025 alone, and Cloudflare and Google have adopted it as well.\n\nThe economic result is *verified spend*—transactions that are cryptographically traceable and tied to specific identities or wallets. In theory, this creates unprecedented efficiency: money flows only toward verified, goal-directed utility. In practice, the system’s robustness depends entirely on its verification mechanisms—a theme that runs through each of the four primitives. The shift from impressions to intents is, at its core, an attempt to move from fake clicks to real, provable actions.\n\nIf the first primitive addresses what we want, the second addresses what agents need to know to deliver it. A concierge who forgets your dietary restrictions or your travel preferences isn’t charming—they’re useless. The same applies to AI agents.\n\nThe problem with most AI systems today is that they operate in “transient memory bubbles.” Each conversation starts from scratch, or nearly so. There is no persistent knowledge base that multiple agents can draw from and contribute to. “Context,” in this sense, means verified, persistent memory and operational state. It’s the difference between an agent that can book a hotel and one that remembers you prefer ground-floor rooms near elevators because of a knee injury mentioned months ago.\n\nAnyone who has used ChatGPT or coding tools like Cursor and has experienced this frustration: having to “re-inject” context repeatedly as interactions deepen, or struggling to port session logs between different AI tools. Context creates consistency and personalization, but it also raises profound questions about ownership and security.\n\nOne emergent solution is a decentralized, verifiable memory store—a “Context Graph” that holds conversational history, transactional records, workflow state. The Context Graph is knowledge infrastructure made queryable and persistent. Advances in zero-knowledge machine learning (zkML), only possible in recent years, have enabled projects like Kinic, which offers verifiable memory stores that users can control. Where might this come in handy? Ray Dalio, the well-known investor and founder of Bridgewater, created an [AI of himself](https://www.principles.com/AIBeta-signup) based on his own data, actions, and applied principles, which you can already interact with online. But without feeding this into a verifiable store, the responses could be manipulated.\n\nResearch supports the importance of persistent memory. U.C. Berkeley’s [MemGPT project](https://par.nsf.gov/servlets/purl/10524107) showed clear performance improvements when agents could access persistent memory; accuracy and relevance scores dropped noticeably when memory systems were disabled. On the LOCOMO benchmark, memory-enabled systems achieved accuracy rates nearly [30% higher](https://mem0.ai/research) than memoryless ones. NVIDIA’s recent [infrastructure announcement](https://nvidianews.nvidia.com/news/nvidia-bluefield-4-powers-new-class-of-ai-native-storage-infrastructure-for-the-next-frontier-of-ai) put a number on it: persistent context can boost processing speeds by up to five times compared to traditional storage.\n\nContext Graphs are a bit of a catch-all, incorporating knowledge graphs, memory graphs and decision graphs on steroids. If they work as designed, context becomes the most valuable asset in the system—a detailed map of preferences, behaviors, and past actions enabling sensemaking. Foundation Capital recently [articulated a vision](https://foundationcapital.com/context-graphs-ais-trillion-dollar-opportunity/) of Context Graphs needed to underpin agents. According to them, agents “sit **in the execution path**. They see the full context at decision time: what inputs were gathered across systems, what policy was evaluated, what exception route was invoked, who approved, and what state was written. If you persist those traces, you get something that doesn’t exist in most enterprises today: a queryable record of how decisions were made.” Later they posit, “Over time, that context graph becomes the real source of truth for autonomy—because it explains not just *what* happened, but *why it was allowed* to happen.”\n\nThis sounds elegant until one asks: Who controls access to this truth? Can it be subpoenaed? Can it be hacked? What happens when the context is wrong––when your agent believes you’re allergic to shellfish when you’re not, or when your political preferences recorded three years ago no longer reflect your views? These questions don’t have answers yet, because the systems are too new. But they will need answers soon, especially if context becomes core infrastructure to agentic interaction.\n\nAnimesh Kortana has [framed the problem](https://www.linkedin.com/pulse/how-build-context-graph-animesh-koratana-6abve/) in temporal terms: we’ve built infrastructure for the “state clock” (what’s true now) while neglecting the “event clock” (what happened, when, and why). Your CRM today stores the final deal value but not the negotiation that produced it. This made sense when humans were the reasoning layer. Agents, however, cannot intuit missing causality––they need explicit event histories.\n\nDifferent options for building Context Graphs are emerging. Geo, a tool built by Yaniv Tal—co-founder of The Graph, an indexing protocol for blockchain data—imagines decision context going beyond internal enterprise processes and extending into personal knowledge management. The Geo browser allows users to build a personal knowledge graph automatically as they browse, creating a searchable, AI-queryable history of everything you’ve read, watched, or interacted with online. It’s like having a research assistant who never forgets anything you’ve told them.\n\nThere are other projects using cryptographic proofs to ensure that the information an agent draws from is authentic and hasn’t been tampered with. This matters enormously when agents are making consequential decisions. A medical AI agent prescribing medication based on a patient’s history needs to be certain that history is accurate. A financial agent managing investments needs to trust that market data hasn’t been manipulated.\n\nThis matters enormously when agents are making consequential decisions. A medical AI agent prescribing medication based on a patient’s history needs to be certain that history is accurate. A financial agent managing investments needs to trust that market data hasn’t been manipulated.\n\nRather than storing context as persistent memory, a different architectural approach called [Recursive Language Models](https://alexzhang13.github.io/blog/2025/rlm/) (RLMs) treats context as an external environment that agents actively decompose—programmatically examining inputs and recursively calling themselves on specific snippets. This handles inputs up to 100x beyond standard context windows, not by extending memory but by changing how agents interact with information.\n\nThe implications are significant. An agent coordinating a supply chain might not need a Context Graph storing every relationship if it can recursively examine relevant subsets on demand. This reduces infrastructure costs while potentially increasing robustness. But it raises new verification questions: if context is actively reconstructed rather than passively stored, who controls the decomposition strategy? There may be no canonical record to audit, only ephemeral context reconstructions.\n\nBuilding Context Graphs requires two layers, as Kirk Marple of Graphlit [explains](https://www.graphlit.com/blog/context-layer-ai-agents-need). First, “operational context,” such as, who is Sarah Chen across email, Slack, and meeting transcripts—the same person or three different entities? Who owns which account? These are identity resolution and relationship modeling problems. Once this foundation exists, you can build “decision context”: which policy was evaluated, what exception was invoked, who approved and based on what precedent.\n\nThe result is supposed to be a single, high-fidelity source of truth for agent swarms—allowing complex workflows to span multiple services without losing coherence. This infrastructure is what makes verifiable actions possible: without persistent, queryable context, there’s no way to prove an agent acted correctly or trace why a decision was made. But this raises a question that proponents tend to elide: What happens when different agents have access to different context? When does partial information become misinformation? And who decides what belongs in the canonical record?\n\nThe explosion of generative AI has made it trivially easy to create content and increasingly difficult to determine its origin. Text, images, code, music—all can be synthesized in seconds, often indistinguishably from human work. This collapse of provenance makes trust nearly impossible.\n\nAttribution aims to address this through cryptographic proof of origin and contribution. Chris Dixon of Andreessen Horowitz has argued it is essential to restoring trust online. Peter Wang, cofounder of Anaconda, has proposed a suite of licenses called AMPL (AI Model Public License) to give creators accountability around works and derivatives, similar to Creative Commons.\n\nIn a world of AI agents, this question becomes urgent. When an agent completes a transaction on your behalf—booking a flight, purchasing insurance, ordering groceries—how do we know the transaction happened? How would you prove agent actions within an enterprise workflow? And how do we ensure that the right parties are compensated?\n\nAt the technical level, attribution relies on “Provenance Graphs”—immutable, time-stamped records structured as Directed Acyclic Graphs (DAGs) that track the full lifecycle of digital work. If an image was created by an AI model trained on a particular dataset, refined by a human designer, then modified by another agent, the Provenance Graph captures all of it, step by step.\n\nStartups like CoreTx are building attribution-native architectures where provenance tracking is foundational rather than retrofitted—treating memory, intent, and credit allocation as aspects of a single unified system.\n\nThis goes beyond traditional royalty structures because AI can continuously weigh contributions, giving greater weight to recent or innovative inputs. This could enable faster innovation cycles, particularly in cross-disciplinary settings like scientific research, which has long been hampered by peer review inefficiencies and irreproducibility.\n\nThe mathematical foundation is evolving rapidly. Data Shapley, which uses game theory to assign each contributor a value reflecting their marginal contribution, was previously too computationally expensive for large systems. Recent breakthroughs enable attribution calculation in a single training run. When an agent synthesizes context from multiple sources to make a recommendation, [Data Shapley-style techniques](https://arxiv.org/pdf/2406.11011) can decompose the recommendation’s value across each input, enabling proportional compensation (or even proportional influence on synthesis).\n\nTraditional attribution systems rely on cookies, which track your movements across the Web and credit conversions to the last ad you clicked. This model is breaking down for two reasons. First, privacy regulations and browser changes are killing third-party cookies. When Google tested disabling them for 1% of Chrome users in the first quarter of 2024, programmatic-advertising revenue dropped by 21-34%. Rather than proceeding with deprecation, Google quietly reversed course in July, revealing the economic dependency: losing those cookies would cost shareholders approximately $2 billion annually. Alternative identifier networks like LiveRamp achieve only 28% match rates because they depend on users sharing the same login credentials across sites, which happens infrequently. The entire attribution infrastructure is held together by duct tape and stockholder pressure.\n\nSecond, AI agents make cookies obsolete. Agents spin up fresh browser instances for every request, eliminating persistent identifiers. They can trivially strip affiliate links or insert their own, farming commissions in ways that are undetectable for agents running on local operating systems. Without cryptographic proof of genuine engagement, every impression metric becomes as unreliable as blockchain “active wallets.”\n\nInstead of relying on tracking pixels and cookies, transactions are recorded on a blockchain or verified through zero-knowledge proofs. When an agent completes a purchase, it generates a cryptographic receipt proving that the transaction occurred, who facilitated it, and which party deserves credit. Companies like Opacity Network are building these systems, enabling agents to verify attributable outcomes across platforms while preserving privacy.\n\nVarious standards are emerging. ERC-8004, from the Ethereum community, introduces “trustless agents” through two registries: an Identity Registry assigning each agent a portable AgentID, and a Reputation Registry standardizing on-chain performance feedback. This creates something like a passport for agents––a portable identity with work history and trust score.\n\nThe economic outcome is “*verified contribution*”—fractional, tokenized ownership of intellectual property based on proven contribution history. The appeal is obvious: no more fraud, no more ambiguity about who gets paid. Provenance Graphs make this possible by creating an auditable trail from object origin to use or incorporation—the infrastructure needed to replace the noise with real, compensable actions. The “value” field that has always existed in analytics, is upgraded with verifiable actions and crypto rails, turning attribution into a real market primitive rather than just a historical reporting metric, that feeds future pricing, budget allocation, and automated spend.\n\nBut the implementation is complex. Cryptographic attribution requires infrastructure that doesn’t yet exist at scale. It also requires agreement on standards—who decides what counts as a valid attribution event or the weighting algorithms? If the verification layer is controlled by a small number of companies, we’ve simply traded one oligopoly for another. If contribution is continuously recalculated, who prevents gaming of the system? Also, what happens when the graph is incomplete––when someone’s contribution isn’t recorded because they didn’t use the right tools or platform? Third, and most important: does the existence of a cryptographic receipt prove that value was actually created, or merely that a transaction occurred?\n\nHow do you predict the behavior of a market before it exits? Traditional economic models assume rational actors optimizing predictably. AI agents are not rational in any human sense—on the one hand, they can be more economically rational, on the other hand, they can be tricked, manipulated, or behave in [unanticipated (or anticipated) ways](https://arxiv.org/abs/2406.01382). The solution is simulation: modeling emergent market dynamics by observing autonomous AI actors in controlled environments.\n\nMicrosoft’s Magentic Marketplace is a simulated economy where researchers observe how agents respond to incentives and threats. ElizaOS uses coordinated AI workers in synthetic environments (agents playing Minecraft, essentially) to generate behavioral data. Stanford [Smallville](https://arxiv.org/pdf/2304.03442) back in 2023 was an early example of observing generative agents in an artificial society: inspired by the Sims game, 25 agents went about their days in a simulated town and showed that “Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools.” Increasingly, simulation of multi-agent[ “societies of thought”](https://arxiv.org/abs/2601.10825)—essentially conversations and debate between varying personas—is producing better chains of reasoning. Additional research by Ben Manning and John Horton at MIT investigates AI agents as [“a general vessel through which theory can be flexibly applied to any setting](https://arxiv.org/pdf/2508.17407)” to accurately simulate human data.\n\nSuch simulations serve varying purposes. First, they can help us better understand real humans, since models have absorbed a lot about how humans behave and how to predict that behavior. Second, they can increasingly allow developers to test agent behavior before deployment—to see how a shopping agent responds to price fluctuations or how a travel agent handles rebooking during a hypothetical hurricane. Third, they generate training data. Watching agents interact in simulation provides behavioral patterns that can improve real-world deployment. Create the scenario and watch how it performs.\n\nRecent work demonstrates how simulation can generate training data for predictive agents at scale. Researchers at [OpenForecaster](https://arxiv.org/pdf/2512.25070) synthesized over 52,000 forecasting questions from news events, training an 8-billion-parameter model that matched models 10x larger while addressing the “information leakage” problem through time-stamped snapshots. [Complementary work ](https://benjaminmanning.io/files/optimize.pdf)at MIT showed that theory-grounded AI agents can predict human behavior in entirely novel strategic games, outperforming Nash equilibria and cognitive hierarchy models by 2.4x-3.4x—but only when validated across multiple distinct scenarios. Both studies point to the same challenge: creating simulation environments that accurately represent uncertainty without inadvertently encoding answers or overfitting to specific contexts.\n\nThe stakes for simulation extend beyond optimization. The Google DeepMind team[ characterizes](https://arxiv.org/abs/2509.10147) this testing imperative more starkly: we’re on a trajectory toward spontaneous emergence of a ‘sandbox economy’—agents transacting at scales and speeds beyond human oversight. The choice, they argue, isn’t whether this economy emerges but whether it emerges *intentionally* (designed for safety) or *accidentally* (with uncontrolled risks). Their framework distinguishes between permeable economies (agents interacting with the human economy) and impermeable ones (sealed testing environments). The challenge is that the most useful agent economies will be highly permeable—enabling agents to book flights, negotiate deals, coordinate supply chains—which is precisely what makes them systemically risky.\n\nSimulation becomes not just optimization but existential: the ability to stress-test market mechanisms before deploying them into an economy where trillions of dollars flow through agent decisions. These behavioral network graphs—the infrastructure underlying simulation—map how agents interact, compete, and coordinate under different conditions. They’re what make it possible to ask: will this system produce real value or just automate fraud at a higher scale?\n\nCompanies like Modulus Labs and Zama are building these simulation environments, often using zero-knowledge proofs to allow agents to run privately—meaning competitors can’t see your testing strategies or reverse-engineer your agent’s decision-making. The goal is to create a sandbox where agents can fail safely, learning through iteration without real-world consequences.\n\nBut simulation raises philosophical questions. How do we know the simulation is accurate? A model is only as good as its assumptions, and we must make inductive leaps to believe predictions sourced from black-boxes hold true. If the simulation doesn’t account for edge cases—sudden regulatory changes or an unexpected market shock—the agent will be unprepared when they occur. Moreover, agents trained in simulation may develop strategies that work in the synthetic world but fail in reality’s messiness.\n\nThese challenges echo older debates about structured prediction. [The Delphi Method](https://www.rand.org/content/dam/rand/pubs/papers/2008/P3558.pdf), developed by Olaf Helmer at RAND in the 1950s and 60s, pioneered systematic approaches to forecasting uncertain futures through structured expert consensus. Helmer’s framework emphasized sequential interrogation with feedback loops, avoiding the biases of committee groupthink while building toward convergence. The method’s core insight—that forecasting requires explicit mechanisms to surface assumptions, challenge them, and refine predictions iteratively—applies directly to agent simulation. Where Delphi used human experts and questionnaires, modern simulation uses synthetic agents and behavioral networks, not possible in 1967! But both face the same fundamental problem: how do you validate a model of the future before that future exists?\n\nThere’s also overfitting risk. An agent optimized for a specific simulation might perform brilliantly in testing and poorly in deployment. This is a well-known problem in machine learning, and there’s no reason to believe it won’t plague agentic systems as well.\n\nThe historical parallel is instructive. Helmer and his colleagues recognized early on that prediction methods could converge on false certainty if not carefully designed—what they called “reliability of estimates.” They built safeguards through anonymity, controlled feedback, and explicit documentation of reasoning. Modern agent simulation faces an updated version of this challenge: agents can converge on strategies that work brilliantly in the synthetic environment while failing in reality’s messier contexts. The solution, both then and now, involves making the reasoning process explicit and testable. Helmer’s work emphasized that the value of structured forecasting wasn’t just in getting predictions right, but in understanding why experts believed what they believed. Similarly, the value of agent simulation may lie less in perfectly predicting agent behavior than in making the embedded assumptions—about incentives, constraints, and goals—visible and debuggable.\n\nMore fundamentally: if we’re simulating markets before they exist, we’re essentially choosing which market dynamics to encode. These are not neutral choices. A simulation that prioritizes efficiency over fairness will train agents to optimize for efficiency. A simulation that ignores power asymmetries will produce agents blind to them. Part of what’s different is the scale of what we can simulate. We are building not just tools but world-models—and the assumptions embedded in those models will shape the economy they help create.\n\nIf these four primitives—Intention, Context, Attribution, and Simulation—take hold, their most immediate impact may be on digital advertising; a $600 billion industry built on a model that is visibly failing.\n\nThe traditional model worked thus: advertisers paid to put their message in front of as many people as possible, hoping some small fraction would click, and an even smaller fraction would convert. Publishers got paid for impressions—eyeballs, not outcomes. The system was riddled with fraud: bot traffic, fake impressions, click farms. Tim Hwang’s *Subprime Attention Crisis* laid bare the economics: billions in wasted spend, widespread privacy violations, and a market built on assets whose value could never truly be known.\n\nThe agentic model proposes something different. Instead of chasing clicks, it rewards utility. The focus shifts from impressions to conversion efficiency: Did the agent actually fulfill the user’s intent? The tools to make this transformation possible are arriving. One concrete example of this shift: Google Research is [already modeling advertisers](https://research.google/blog/mechanism-design-for-large-language-models/) as self-interested LLM agents and coordinating ad creation through auction mechanisms rather than impressions or clicks.\n\nZero-knowledge transport-layer security (zkTLS) allows agents to prove they completed actions—clicked an ad, watched a video, made a purchase—without exposing underlying user data. Stablecoins eliminate the payment friction inherent in cross-border and micropayment-heavy transactions. Together, these reduce infrastructure costs: where traditional programmatic advertising keeps only 55 cents of every dollar for publishers (45 cents lost to intermediaries), crypto-enabled systems could collapse margins dramatically, [according to a report by venture firm Escape Velocity](https://ev3.xyz/research/letters/Advertising%5Ffrom%5FFirst%5FPrinciples/).\n\nPayments would be automated by smart contracts, triggered only upon verifiable proof of fulfillment. Each primitive plays a role: Intention (via the Intents Graph) allows advertising to shift from demographic targeting to fulfilling explicit, programmatic desires. Attribution (via the Provenance Graph) eliminates fraud through verified contribution. Context (via the Context Graph) allows agents to access high-fidelity, user-controlled data for dynamic creative optimization—ads that are relevant without being invasive. Simulation (via behavioral network graphs) lets advertisers stress-test bidding and placement strategies in advance, optimizing for maximum return before any budget is spent.\n\nIt’s an elegant vision that conveniently aligns advertiser interests (efficiency) with user interests (privacy and relevance). Whether it will actually work—whether the infrastructure can be deployed at scale, the incentives aligned, and the inevitable attempts at exploitation repelled—remains to be seen.\n\nThe history of the Internet suggests caution. Every previous attempt to fix advertising has produced, eventually, new and more sophisticated versions of old problems. But the fundamental architecture being proposed here is different: graphs that make actions verifiable rather than inferred, infrastructure that replaces fake clicks with cryptographic proof of real choices.\n\nThe agentic economy is not inevitable. It is a possibility, not a prophecy—a set of architectural answers to the endemic trust failures of Web 2.0\\. The four primitives create, in theory, a closed loop: goal setting, knowledge pooling, value assignment, and resilience testing. Each graph reinforces the others, and value can flow across them unobstructed. Verified spend can depend on verified context, which can utilize verified contribution, all of it stress-tested through continuous simulation and learning. Yet the path forward is a delicate one. Some of the primitives will combine in different permutations, and this may also take a while. Likely, the result is not one cohesive end state, but rather a slew of different companies. As Seref Yarar from Index Network described “simulation, for example, is needed in both enterprise and consumer markets,” so we will see it emerge in various forms.\n\nThe primary challenge for this agentic future, even advocates acknowledge, is governance. Autonomy has limits. For high-stakes transactions, human oversight remains essential—a reality acknowledged in what some developers call the Meta AI [“Agents Rule of Two.”](https://ai.meta.com/blog/practical-ai-agent-security/) In this framework, agents are limited to two sensitive capabilities: reading untrusted input, accessing private data, *or* taking external action. The third requires explicit human approval. An agent with unchecked access to your bank account, your e-mail, and purchases authority is currently a liability, not a feature.\n\nThe constraint reintroduces the very friction the system aims to eliminate: delay, human error, the possibility of bad judgment. Striking the right balance—enough autonomy to be useful, enough control to be safe—requires not just technical solutions but legal and ethical frameworks for non-human economic actors.\n\nWhat rights, if any, does an AgentID have? Who is liable when an agent makes a mistake? Who is responsible when an agent trained on biased data produces discriminatory outcomes? These questions barely exist in current law. If people will outsource their decisions to agents, how that power is ceded or reversed matters.\n\nIf the agentic economy does take hold and scale to a trillion agents, as some predict, value capture will shift dramatically. The extractive model of Web 2.0—proprietary data harvested and monetized without consent—could give way to transparent, auditable protocol fees. Compensation would flow from verifiable execution and context maintenance. Power would accrue not to platforms collecting the most data but to the entities maintaining the most trusted infrastructure. And, potentially, to the agents doing the best work.\n\nThis topic has only just started and several critical questions remain not just unanswered but largely unasked:\n\n**On Intention:** How do we prevent the broadcast of intent from becoming a new form of surveillance? If every desire is machine-readable, who reads it and what do they do with that information? Will intention match volition, or will even personal agents have a new hill to climb: maximizing the velocity of their owner’s consumption, decluttering, addiction, therapy, repetition?\n\n**On Context:** Who controls the canonical record? When different actors have access to different contexts, who adjudicates disputes about what actually happened? What are the consequences when the context is incomplete or wrong?\n\n**On Attribution:** Who designs the weighting algorithms that determine contribution value? How do we prevent these systems from encoding existing inequalities—rewarding those who already have access to the right platforms and tools while excluding those who don’t?\n\n**On Simulation:** What assumptions are we encoding into our simulations? Are we training agents to optimize for efficiency at the expense of fairness? Are we building world-models that reflect how things are or how we wish them to be? How will simulation and prediction interact, including in the use of prediction markets as inputs.\n\nMore broadly: Who decides what problems these primitives solve? The framing is technical—verification, trust, efficiency—but the problems are social and political. The attention economy failed not because we lacked cryptographic tools but because the incentives were misaligned from the start. Advertisers wanted reach; platforms wanted engagement; users wanted neither but tolerated both because there were no alternatives. Freemium means you are the product, paid for at the hands of the advertising value chain. Turns out everything is computer *and* everything is advertising. We must also admit that fraud is not only a byproduct but perhaps an inherent go-to-market strategy emergent in every system; what are the new fraud formats of the agentic age?\n\nWill the agentic economy offer genuine alternatives, or merely new forms of lock-in? Will the primitives serve users or extract value from them? Under-explored here is how users will be able to mix and match agents and flows of activity, arranging relationships around privacy, consent and permissioning of agents, data, and assets along different identities, wallets or keys. For the self-motivated and crypto-enthusiast, the agentic economy opens up tools and parameters to play with end-to-end design and their level of control; for many others, the architecture of convenience may prevail over that of sovereignty. Will the dominant convenient infrastructure be open or proprietary? These questions cannot be answered by examining the technology alone. The work of building the agentic economy is, fundamentally, the work of deciding who has power in digital life. It’s about whether we can scale infrastructure that makes real actions verifiable—that moves us from an economy of fake clicks and bot traffic to one where value flows from cryptographically proven utility. The skeptic might observe that trust, once lost, is not easily rebuilt—and that the people asking us to trust them now are often the same ones who broke that trust before. The technology may be new. The question it poses is old: whether the incentives can be aligned in a way that serves more than the architects themselves.\n\nRegardless, we may be clicking a lot less and following new intentions to their logical limit.\n\nWhatever they might be.\n\n*\\*NB: I have been working on these ideas since before the weekend of Clawdbot/Moltbot/OpenClaw and hope to grapple with how that phenomenon unfolds further parts of the agentic economy separately. Sometimes “done is better than perfect,” and in this case, I’m using writing to explore the potentialities of the primitives as they evolve (which they will; as will my thinking).*\n\n*\\*With special thanks to the brain trust who read, reviewed, or commented on parts of this piece to improve my thinking – all errors in understanding are my own (and hope to be corrected): Seref Yarar, Samuel Klein, Shane Mac, Simran Chana, Oliver Zahn, Jove Oliver, Philipp Kruger, Scott Cohen*\n\nNo posts\n",
        "pipelineQuality": 1,
        "pipelinePassed": true,
        "pipelineReasons": [],
        "words": 5943,
        "links": 33,
        "headings": 1,
        "tokenRecallVsJina": 1,
        "headingRecallVsJina": 0,
        "linkRecallVsJina": 0.906,
        "lengthRatioVsJina": 0.966,
        "overlapScore": 0.805
      },
      {
        "engine": "openrouter_gpt_oss_20b",
        "ok": false,
        "error": "Forced engine openrouter_gpt_oss_20b requires OPENROUTER_API_KEY."
      },
      {
        "engine": "jina_reader",
        "ok": true,
        "title": "Last September, security researchers at HUMAN’s Satori Threat Intelligence team [discovered something](https://www.humansecurity.com/learn/blog/slopads-highly-obfuscated-android-ma",
        "markdown": "# Last September, security researchers at HUMAN’s Satori Threat Intelligence team [discovered something](https://www.humansecurity.com/learn/blog/slopads-highly-obfuscated-android-ma\n\nThe scheme had a cunning feature: it could detect whether someone had found an app organically or arrived via a promotional link. Only in the latter case would the fraud activate, ensuring that advertisers—not users—paid the bill. At its peak, SlopAds generated 2.3 billion fraudulent ad impressions daily. This wasn’t small-time fraud. It was industrial-scale deception, automated and optimized.\n\nThis episode is worth examining not because it was unusual but because it was typical. For two decades, the digital economy has rested on a foundation that Tim Hwang, in his 2020 book _Subprime Attention Crisis_, described as fundamentally suspect: the buying and selling of human attention. Consider the first banner ad, placed by AT&T on HotWired.com in 1994. Its click-through rate was 44%. Today the average banner ad [achieves 0.2%](https://www.aidigital.com/blog/ctr-for-display-ads#:~:text=It's%20not%20uncommon%20for%20high,%E2%80%8D). The problem, as Hwang argued, is structural: attention is almost impossible to verify. An “impression” could come from a person, a bot, or a phone sitting in a server farm. The market, in other words, is subprime—built on assets whose value is structurally unknowable.\n\nWe are now living through what Hwang anticipated—not a dramatic crash but a quiet erosion of trust, accelerated by AI agents that are beginning to navigate digital commerce on behalf of users. These agents promise efficiency: no more clicking through dozens of options to book a flight or find insurance. But they also raise a question: if we cannot verify human attention, how will we verify machine behavior?\n\nThe answer, according to a growing number of technologists and investors betting on this future, lies in four primitives—fundamental building blocks for a new digital economy. These are: **Intention** (systems for expressing and broadcasting what users want), **Context** (verified memory and decisions that agents can draw upon), **Attribution** (cryptographic proof of who contributed what), and **Simulation** (environments for testing agent behavior before deployment). Each primitive rests on a different type of graph: networked data structures that connect, verify, and trace digital activity in ways the current Web cannot. Together, they create the necessary underpinnings of an “Agentic Economy”—a system in which value flows not from clicks or impressions but from verifiable actions.\n\nWhether this system will work, whether it can be deployed at scale, and whether it will serve anyone beyond its architects are questions this essay will examine. But first, it is worth understanding what its builders envision.\n\nThe projections sound outlandish, but reality is catching up. Jensen Huang, NVIDIA’s CEO, declared at the Consumer Electronics Show in January 2025 that the era of agentic AI represents a multi-trillion-dollar opportunity. “[The age of AI Agentics is here,](https://finance.yahoo.com/news/nvidia-jensen-huang-says-ai-044815659.html)” he said. Ramesh Raskar, of MIT, envisions a world in which a [trillion agents](https://www.youtube.com/watch?v=Da6Ya0bfLDA) are on the loose, representing, negotiating, and transacting on our behalf—or for themselves—as parts of complex swarms. Dario Amodei, the CEO of Anthropic, has written of “[a country of geniuses in a datacenter,](https://www.darioamodei.com/essay/machines-of-loving-grace)” made up of millions of AI agents working independently or collaboratively.\n\nThe forecasters agree on little except scale. By 2028, according to Capgemini, AI agents could generate [$450 billion](https://www.capgemini.com/insights/research-library/ai-agents/) in economic value. [McKinsey projects](https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-agentic-commerce-opportunity-how-ai-agents-are-ushering-in-a-new-era-for-consumers-and-merchants) the U.S. retail market alone could see $900 billion to $1 trillion in agent-orchestrated revenue by 2030, with global figures reaching $3 trillion to $5 trillion. The investment firm Lightspeed warns of a [$19.9-trillion](https://lsvp.com/stories/the-ai-agent-economy-has-a-19-trillion-problem-our-investment-in-paid/) economic opportunity at risk if infrastructure gaps aren’t addressed. (For perspective: current global GDP is approximately [$110 trillion](https://data.worldbank.org/indicator/NY.GDP.MKTP.CD)).\n\nNo one knows what this economy will look like or whether it will be desirable. The phrase “empires for no one” has been circulating social media––a way of expressing unease that the world being built may be out of step with what humans actually want. It brings to mind a meme from a few years ago: the eighties band Toto’s buoyant song “Africa” [echoing through empty malls](https://www.newyorker.com/culture/rabbit-holes/the-overwhelming-emotion-of-hearing-totos-africa-remixed-to-sound-like-its-playing-in-an-empty-mall)—cheerful yet eerie, promising abundance yet somehow a result that’s blasphemously ugly. Then there’s the [infamous quote](https://www.youtube.com/shorts/YE5adUeTe_I) by OpenAI CEO Sam Altman from 2015, “I think that AI will probably, most likely, sort of lead to the end of the world. But in the meantime, there will be great companies created with serious machine learning,” which makes almost anyone do a double take. Still, even if AI development stopped today, the tools already deployed could reshape commerce substantially. As long as AI budgets continue to flow, experiments in agent deployment will continue.\n\nWhile we might not know its contours, we can begin to see some of the future economy’s building blocks. A world in which agents and robots form the next consumer class, working with or without us, requires economic rails and a set of functions that enable coordination, orchestration, and autonomous activity. To understand what is being proposed, we must examine each primitive—and the graph infrastructure underneath it—in turn.\n\nThe Internet has no native way to express what we want. We search, we browse, we click—gestures that leave platforms to infer our desires from the digital exhaust we leave behind. The attention economy’s business model depends on this: show people things until something sticks, then charge advertisers for the attempt.\n\nWhat’s replacing “where do I click?” is _programmatic desire_, or _intent_—a cryptographically secured expression of a specific outcome, broadcast to a network of competing agents ready to fulfill it. Rather than clicking through options, a user (or their agent or enterprise) would broadcast: “I need a plumber in Brooklyn this week” or “Find noise-canceling headphones under $200” or “Build a workflow that connects my users to this widget.” Providers would respond with bids. With explicit and compensable needs, transactions would complete automatically.\n\nIntents have several characteristics. First, they are outcome-focused: users often don’t care about implementation details or the steps taken to reach a goal. Second, they carry conditional authorization: users set the parameters or approvals needed to authorize an intent being fulfilled. Third, they are broadcast to a network of “solvers”—humans, agents, or protocols—who compete to fulfill all or part of an intent. And, finally, their outcomes must be verifiable.\n\nConsider the infrastructure required to make this work. At its core is an “Intents Graph”—a dynamic network mapping programmatic desires to agents capable of fulfilling them. This isn’t just a database of wants; it’s a trust-minimized matching system that replaces platform-mediated discovery with direct, verifiable connections. Protocols like Index Network are building the first such graph for social interactions, where agents facilitate connections based on mutual intent. NEAR Intents has been pursuing this logic for financial transactions, forming crypto-asset swaps where outcome efficiency is prioritized regardless of the route agents use to execute swaps.\n\nThe Intents Graph represents a fundamental architectural shift: from inferring what users might want based on their clicks to processing explicit, machine-readable declarations of need. It’s discovery elevated into trust-minimized matching. This kind of system forces relevancy or else it becomes useless. Who wants an old intent fulfilled? Who wants to be spammed? No one. The promise is that once an intent is articulated, it can be fulfilled quickly and relevantly. Behind the scenes is an automagical symphony of agentic activity.\n\nThe appeal is obvious: efficiency without the guesswork. But consider what must be true for this to work. First, users must trust that their broadcasted desires won’t be exploited—that expressing a need for a medical procedure, for instance, won’t result in price discrimination or insurance complications. Second, the verification mechanisms must actually work: the system must confirm that an intent was genuinely fulfilled, not merely claimed to be fulfilled.\n\nThe recent conflict between [Amazon and Perplexity](https://www.pcmag.com/news/amazon-sends-perplexity-a-cease-and-desist-over-its-ai-agents-shopping)—in which AI agents were allegedly making purchases using real user accounts without proper authorization—illustrates the stakes. If agents are to transact based on our behalf, they need verifiable access rights, something like a license system for digital actors, tied to a unique and portable identifier.\n\nOne enabling mechanism is x402, a protocol that revives an old idea. In the original specification for HTTP, status code 402 was reserved for “Payment Required”—a placeholder for micropayments that never materialized because credit-card fees made small transactions unworkable. But the x402 protocol enables agents to execute autonomous, per-request micropayments. Every transaction, from retrieving data to executing a service, becomes a seamless machine-to-machine exchange, settled instantly. Coinbase processed [57 million transactions](https://x.com/Cointelegraph/status/2003677242782609819?s=20) using x402 in December 2025 alone, and Cloudflare and Google have adopted it as well.\n\nThe economic result is _verified spend_—transactions that are cryptographically traceable and tied to specific identities or wallets. In theory, this creates unprecedented efficiency: money flows only toward verified, goal-directed utility. In practice, the system’s robustness depends entirely on its verification mechanisms—a theme that runs through each of the four primitives. The shift from impressions to intents is, at its core, an attempt to move from fake clicks to real, provable actions.\n\nIf the first primitive addresses what we want, the second addresses what agents need to know to deliver it. A concierge who forgets your dietary restrictions or your travel preferences isn’t charming—they’re useless. The same applies to AI agents.\n\nThe problem with most AI systems today is that they operate in “transient memory bubbles.” Each conversation starts from scratch, or nearly so. There is no persistent knowledge base that multiple agents can draw from and contribute to. “Context,” in this sense, means verified, persistent memory and operational state. It’s the difference between an agent that can book a hotel and one that remembers you prefer ground-floor rooms near elevators because of a knee injury mentioned months ago.\n\nAnyone who has used ChatGPT or coding tools like Cursor and has experienced this frustration: having to “re-inject” context repeatedly as interactions deepen, or struggling to port session logs between different AI tools. Context creates consistency and personalization, but it also raises profound questions about ownership and security.\n\nOne emergent solution is a decentralized, verifiable memory store—a “Context Graph” that holds conversational history, transactional records, workflow state. The Context Graph is knowledge infrastructure made queryable and persistent. Advances in zero-knowledge machine learning (zkML), only possible in recent years, have enabled projects like Kinic, which offers verifiable memory stores that users can control. Where might this come in handy? Ray Dalio, the well-known investor and founder of Bridgewater, created an [AI of himself](https://www.principles.com/AIBeta-signup) based on his own data, actions, and applied principles, which you can already interact with online. But without feeding this into a verifiable store, the responses could be manipulated.\n\nResearch supports the importance of persistent memory. U.C. Berkeley’s [MemGPT project](https://par.nsf.gov/servlets/purl/10524107) showed clear performance improvements when agents could access persistent memory; accuracy and relevance scores dropped noticeably when memory systems were disabled. On the LOCOMO benchmark, memory-enabled systems achieved accuracy rates nearly [30% higher](https://mem0.ai/research) than memoryless ones. NVIDIA’s recent [infrastructure announcement](https://nvidianews.nvidia.com/news/nvidia-bluefield-4-powers-new-class-of-ai-native-storage-infrastructure-for-the-next-frontier-of-ai) put a number on it: persistent context can boost processing speeds by up to five times compared to traditional storage.\n\nContext Graphs are a bit of a catch-all, incorporating knowledge graphs, memory graphs and decision graphs on steroids. If they work as designed, context becomes the most valuable asset in the system—a detailed map of preferences, behaviors, and past actions enabling sensemaking. Foundation Capital recently [articulated a vision](https://foundationcapital.com/context-graphs-ais-trillion-dollar-opportunity/) of Context Graphs needed to underpin agents. According to them, agents “sit **in the execution path**. They see the full context at decision time: what inputs were gathered across systems, what policy was evaluated, what exception route was invoked, who approved, and what state was written. If you persist those traces, you get something that doesn’t exist in most enterprises today: a queryable record of how decisions were made.” Later they posit, “Over time, that context graph becomes the real source of truth for autonomy—because it explains not just _what_ happened, but _why it was allowed_ to happen.”\n\nThis sounds elegant until one asks: Who controls access to this truth? Can it be subpoenaed? Can it be hacked? What happens when the context is wrong––when your agent believes you’re allergic to shellfish when you’re not, or when your political preferences recorded three years ago no longer reflect your views? These questions don’t have answers yet, because the systems are too new. But they will need answers soon, especially if context becomes core infrastructure to agentic interaction.\n\nAnimesh Kortana has [framed the problem](https://www.linkedin.com/pulse/how-build-context-graph-animesh-koratana-6abve/) in temporal terms: we’ve built infrastructure for the “state clock” (what’s true now) while neglecting the “event clock” (what happened, when, and why). Your CRM today stores the final deal value but not the negotiation that produced it. This made sense when humans were the reasoning layer. Agents, however, cannot intuit missing causality––they need explicit event histories.\n\nDifferent options for building Context Graphs are emerging. Geo, a tool built by Yaniv Tal—co-founder of The Graph, an indexing protocol for blockchain data—imagines decision context going beyond internal enterprise processes and extending into personal knowledge management. The Geo browser allows users to build a personal knowledge graph automatically as they browse, creating a searchable, AI-queryable history of everything you’ve read, watched, or interacted with online. It’s like having a research assistant who never forgets anything you’ve told them.\n\nThere are other projects using cryptographic proofs to ensure that the information an agent draws from is authentic and hasn’t been tampered with. This matters enormously when agents are making consequential decisions. A medical AI agent prescribing medication based on a patient’s history needs to be certain that history is accurate. A financial agent managing investments needs to trust that market data hasn’t been manipulated.\n\nThis matters enormously when agents are making consequential decisions. A medical AI agent prescribing medication based on a patient’s history needs to be certain that history is accurate. A financial agent managing investments needs to trust that market data hasn’t been manipulated.\n\nRather than storing context as persistent memory, a different architectural approach called [Recursive Language Models](https://alexzhang13.github.io/blog/2025/rlm/) (RLMs) treats context as an external environment that agents actively decompose—programmatically examining inputs and recursively calling themselves on specific snippets. This handles inputs up to 100x beyond standard context windows, not by extending memory but by changing how agents interact with information.\n\nThe implications are significant. An agent coordinating a supply chain might not need a Context Graph storing every relationship if it can recursively examine relevant subsets on demand. This reduces infrastructure costs while potentially increasing robustness. But it raises new verification questions: if context is actively reconstructed rather than passively stored, who controls the decomposition strategy? There may be no canonical record to audit, only ephemeral context reconstructions.\n\nBuilding Context Graphs requires two layers, as Kirk Marple of Graphlit [explains](https://www.graphlit.com/blog/context-layer-ai-agents-need). First, “operational context,” such as, who is Sarah Chen across email, Slack, and meeting transcripts—the same person or three different entities? Who owns which account? These are identity resolution and relationship modeling problems. Once this foundation exists, you can build “decision context”: which policy was evaluated, what exception was invoked, who approved and based on what precedent.\n\nThe result is supposed to be a single, high-fidelity source of truth for agent swarms—allowing complex workflows to span multiple services without losing coherence. This infrastructure is what makes verifiable actions possible: without persistent, queryable context, there’s no way to prove an agent acted correctly or trace why a decision was made. But this raises a question that proponents tend to elide: What happens when different agents have access to different context? When does partial information become misinformation? And who decides what belongs in the canonical record?\n\nThe explosion of generative AI has made it trivially easy to create content and increasingly difficult to determine its origin. Text, images, code, music—all can be synthesized in seconds, often indistinguishably from human work. This collapse of provenance makes trust nearly impossible.\n\nAttribution aims to address this through cryptographic proof of origin and contribution. Chris Dixon of Andreessen Horowitz has argued it is essential to restoring trust online. Peter Wang, cofounder of Anaconda, has proposed a suite of licenses called AMPL (AI Model Public License) to give creators accountability around works and derivatives, similar to Creative Commons.\n\nIn a world of AI agents, this question becomes urgent. When an agent completes a transaction on your behalf—booking a flight, purchasing insurance, ordering groceries—how do we know the transaction happened? How would you prove agent actions within an enterprise workflow? And how do we ensure that the right parties are compensated?\n\nAt the technical level, attribution relies on “Provenance Graphs”—immutable, time-stamped records structured as Directed Acyclic Graphs (DAGs) that track the full lifecycle of digital work. If an image was created by an AI model trained on a particular dataset, refined by a human designer, then modified by another agent, the Provenance Graph captures all of it, step by step.\n\nStartups like CoreTx are building attribution-native architectures where provenance tracking is foundational rather than retrofitted—treating memory, intent, and credit allocation as aspects of a single unified system.\n\nThis goes beyond traditional royalty structures because AI can continuously weigh contributions, giving greater weight to recent or innovative inputs. This could enable faster innovation cycles, particularly in cross-disciplinary settings like scientific research, which has long been hampered by peer review inefficiencies and irreproducibility.\n\nThe mathematical foundation is evolving rapidly. Data Shapley, which uses game theory to assign each contributor a value reflecting their marginal contribution, was previously too computationally expensive for large systems. Recent breakthroughs enable attribution calculation in a single training run. When an agent synthesizes context from multiple sources to make a recommendation, [Data Shapley-style techniques](https://arxiv.org/pdf/2406.11011) can decompose the recommendation’s value across each input, enabling proportional compensation (or even proportional influence on synthesis).\n\nTraditional attribution systems rely on cookies, which track your movements across the Web and credit conversions to the last ad you clicked. This model is breaking down for two reasons. First, privacy regulations and browser changes are killing third-party cookies. When Google tested disabling them for 1% of Chrome users in the first quarter of 2024, programmatic-advertising revenue dropped by 21-34%. Rather than proceeding with deprecation, Google quietly reversed course in July, revealing the economic dependency: losing those cookies would cost shareholders approximately $2 billion annually. Alternative identifier networks like LiveRamp achieve only 28% match rates because they depend on users sharing the same login credentials across sites, which happens infrequently. The entire attribution infrastructure is held together by duct tape and stockholder pressure.\n\nSecond, AI agents make cookies obsolete. Agents spin up fresh browser instances for every request, eliminating persistent identifiers. They can trivially strip affiliate links or insert their own, farming commissions in ways that are undetectable for agents running on local operating systems. Without cryptographic proof of genuine engagement, every impression metric becomes as unreliable as blockchain “active wallets.”\n\nInstead of relying on tracking pixels and cookies, transactions are recorded on a blockchain or verified through zero-knowledge proofs. When an agent completes a purchase, it generates a cryptographic receipt proving that the transaction occurred, who facilitated it, and which party deserves credit. Companies like Opacity Network are building these systems, enabling agents to verify attributable outcomes across platforms while preserving privacy.\n\nVarious standards are emerging. ERC-8004, from the Ethereum community, introduces “trustless agents” through two registries: an Identity Registry assigning each agent a portable AgentID, and a Reputation Registry standardizing on-chain performance feedback. This creates something like a passport for agents––a portable identity with work history and trust score.\n\nThe economic outcome is “_verified contribution_”—fractional, tokenized ownership of intellectual property based on proven contribution history. The appeal is obvious: no more fraud, no more ambiguity about who gets paid. Provenance Graphs make this possible by creating an auditable trail from object origin to use or incorporation—the infrastructure needed to replace the noise with real, compensable actions. The “value” field that has always existed in analytics, is upgraded with verifiable actions and crypto rails, turning attribution into a real market primitive rather than just a historical reporting metric, that feeds future pricing, budget allocation, and automated spend.\n\nBut the implementation is complex. Cryptographic attribution requires infrastructure that doesn’t yet exist at scale. It also requires agreement on standards—who decides what counts as a valid attribution event or the weighting algorithms? If the verification layer is controlled by a small number of companies, we’ve simply traded one oligopoly for another. If contribution is continuously recalculated, who prevents gaming of the system? Also, what happens when the graph is incomplete––when someone’s contribution isn’t recorded because they didn’t use the right tools or platform? Third, and most important: does the existence of a cryptographic receipt prove that value was actually created, or merely that a transaction occurred?\n\nHow do you predict the behavior of a market before it exits? Traditional economic models assume rational actors optimizing predictably. AI agents are not rational in any human sense—on the one hand, they can be more economically rational, on the other hand, they can be tricked, manipulated, or behave in [unanticipated (or anticipated) ways](https://arxiv.org/abs/2406.01382). The solution is simulation: modeling emergent market dynamics by observing autonomous AI actors in controlled environments.\n\nMicrosoft’s Magentic Marketplace is a simulated economy where researchers observe how agents respond to incentives and threats. ElizaOS uses coordinated AI workers in synthetic environments (agents playing Minecraft, essentially) to generate behavioral data. Stanford [Smallville](https://arxiv.org/pdf/2304.03442) back in 2023 was an early example of observing generative agents in an artificial society: inspired by the Sims game, 25 agents went about their days in a simulated town and showed that “Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools.” Increasingly, simulation of multi-agent[“societies of thought”](https://arxiv.org/abs/2601.10825)—essentially conversations and debate between varying personas—is producing better chains of reasoning. Additional research by Ben Manning and John Horton at MIT investigates AI agents as [“a general vessel through which theory can be flexibly applied to any setting](https://arxiv.org/pdf/2508.17407)” to accurately simulate human data.\n\nSuch simulations serve varying purposes. First, they can help us better understand real humans, since models have absorbed a lot about how humans behave and how to predict that behavior. Second, they can increasingly allow developers to test agent behavior before deployment—to see how a shopping agent responds to price fluctuations or how a travel agent handles rebooking during a hypothetical hurricane. Third, they generate training data. Watching agents interact in simulation provides behavioral patterns that can improve real-world deployment. Create the scenario and watch how it performs.\n\nRecent work demonstrates how simulation can generate training data for predictive agents at scale. Researchers at [OpenForecaster](https://arxiv.org/pdf/2512.25070) synthesized over 52,000 forecasting questions from news events, training an 8-billion-parameter model that matched models 10x larger while addressing the “information leakage” problem through time-stamped snapshots. [Complementary work](https://benjaminmanning.io/files/optimize.pdf)at MIT showed that theory-grounded AI agents can predict human behavior in entirely novel strategic games, outperforming Nash equilibria and cognitive hierarchy models by 2.4x-3.4x—but only when validated across multiple distinct scenarios. Both studies point to the same challenge: creating simulation environments that accurately represent uncertainty without inadvertently encoding answers or overfitting to specific contexts.\n\nThe stakes for simulation extend beyond optimization. The Google DeepMind team[characterizes](https://arxiv.org/abs/2509.10147) this testing imperative more starkly: we’re on a trajectory toward spontaneous emergence of a ‘sandbox economy’—agents transacting at scales and speeds beyond human oversight. The choice, they argue, isn’t whether this economy emerges but whether it emerges _intentionally_ (designed for safety) or _accidentally_ (with uncontrolled risks). Their framework distinguishes between permeable economies (agents interacting with the human economy) and impermeable ones (sealed testing environments). The challenge is that the most useful agent economies will be highly permeable—enabling agents to book flights, negotiate deals, coordinate supply chains—which is precisely what makes them systemically risky.\n\nSimulation becomes not just optimization but existential: the ability to stress-test market mechanisms before deploying them into an economy where trillions of dollars flow through agent decisions. These behavioral network graphs—the infrastructure underlying simulation—map how agents interact, compete, and coordinate under different conditions. They’re what make it possible to ask: will this system produce real value or just automate fraud at a higher scale?\n\nCompanies like Modulus Labs and Zama are building these simulation environments, often using zero-knowledge proofs to allow agents to run privately—meaning competitors can’t see your testing strategies or reverse-engineer your agent’s decision-making. The goal is to create a sandbox where agents can fail safely, learning through iteration without real-world consequences.\n\nBut simulation raises philosophical questions. How do we know the simulation is accurate? A model is only as good as its assumptions, and we must make inductive leaps to believe predictions sourced from black-boxes hold true. If the simulation doesn’t account for edge cases—sudden regulatory changes or an unexpected market shock—the agent will be unprepared when they occur. Moreover, agents trained in simulation may develop strategies that work in the synthetic world but fail in reality’s messiness.\n\nThese challenges echo older debates about structured prediction. [The Delphi Method](https://www.rand.org/content/dam/rand/pubs/papers/2008/P3558.pdf), developed by Olaf Helmer at RAND in the 1950s and 60s, pioneered systematic approaches to forecasting uncertain futures through structured expert consensus. Helmer’s framework emphasized sequential interrogation with feedback loops, avoiding the biases of committee groupthink while building toward convergence. The method’s core insight—that forecasting requires explicit mechanisms to surface assumptions, challenge them, and refine predictions iteratively—applies directly to agent simulation. Where Delphi used human experts and questionnaires, modern simulation uses synthetic agents and behavioral networks, not possible in 1967! But both face the same fundamental problem: how do you validate a model of the future before that future exists?\n\nThere’s also overfitting risk. An agent optimized for a specific simulation might perform brilliantly in testing and poorly in deployment. This is a well-known problem in machine learning, and there’s no reason to believe it won’t plague agentic systems as well.\n\nThe historical parallel is instructive. Helmer and his colleagues recognized early on that prediction methods could converge on false certainty if not carefully designed—what they called “reliability of estimates.” They built safeguards through anonymity, controlled feedback, and explicit documentation of reasoning. Modern agent simulation faces an updated version of this challenge: agents can converge on strategies that work brilliantly in the synthetic environment while failing in reality’s messier contexts. The solution, both then and now, involves making the reasoning process explicit and testable. Helmer’s work emphasized that the value of structured forecasting wasn’t just in getting predictions right, but in understanding why experts believed what they believed. Similarly, the value of agent simulation may lie less in perfectly predicting agent behavior than in making the embedded assumptions—about incentives, constraints, and goals—visible and debuggable.\n\nMore fundamentally: if we’re simulating markets before they exist, we’re essentially choosing which market dynamics to encode. These are not neutral choices. A simulation that prioritizes efficiency over fairness will train agents to optimize for efficiency. A simulation that ignores power asymmetries will produce agents blind to them. Part of what’s different is the scale of what we can simulate. We are building not just tools but world-models—and the assumptions embedded in those models will shape the economy they help create.\n\nIf these four primitives—Intention, Context, Attribution, and Simulation—take hold, their most immediate impact may be on digital advertising; a $600 billion industry built on a model that is visibly failing.\n\nThe traditional model worked thus: advertisers paid to put their message in front of as many people as possible, hoping some small fraction would click, and an even smaller fraction would convert. Publishers got paid for impressions—eyeballs, not outcomes. The system was riddled with fraud: bot traffic, fake impressions, click farms. Tim Hwang’s _Subprime Attention Crisis_ laid bare the economics: billions in wasted spend, widespread privacy violations, and a market built on assets whose value could never truly be known.\n\nThe agentic model proposes something different. Instead of chasing clicks, it rewards utility. The focus shifts from impressions to conversion efficiency: Did the agent actually fulfill the user’s intent? The tools to make this transformation possible are arriving. One concrete example of this shift: Google Research is [already modeling advertisers](https://research.google/blog/mechanism-design-for-large-language-models/) as self-interested LLM agents and coordinating ad creation through auction mechanisms rather than impressions or clicks.\n\nZero-knowledge transport-layer security (zkTLS) allows agents to prove they completed actions—clicked an ad, watched a video, made a purchase—without exposing underlying user data. Stablecoins eliminate the payment friction inherent in cross-border and micropayment-heavy transactions. Together, these reduce infrastructure costs: where traditional programmatic advertising keeps only 55 cents of every dollar for publishers (45 cents lost to intermediaries), crypto-enabled systems could collapse margins dramatically, [according to a report by venture firm Escape Velocity](https://ev3.xyz/research/letters/Advertising_from_First_Principles/).\n\nPayments would be automated by smart contracts, triggered only upon verifiable proof of fulfillment. Each primitive plays a role: Intention (via the Intents Graph) allows advertising to shift from demographic targeting to fulfilling explicit, programmatic desires. Attribution (via the Provenance Graph) eliminates fraud through verified contribution. Context (via the Context Graph) allows agents to access high-fidelity, user-controlled data for dynamic creative optimization—ads that are relevant without being invasive. Simulation (via behavioral network graphs) lets advertisers stress-test bidding and placement strategies in advance, optimizing for maximum return before any budget is spent.\n\nIt’s an elegant vision that conveniently aligns advertiser interests (efficiency) with user interests (privacy and relevance). Whether it will actually work—whether the infrastructure can be deployed at scale, the incentives aligned, and the inevitable attempts at exploitation repelled—remains to be seen.\n\nThe history of the Internet suggests caution. Every previous attempt to fix advertising has produced, eventually, new and more sophisticated versions of old problems. But the fundamental architecture being proposed here is different: graphs that make actions verifiable rather than inferred, infrastructure that replaces fake clicks with cryptographic proof of real choices.\n\nThe agentic economy is not inevitable. It is a possibility, not a prophecy—a set of architectural answers to the endemic trust failures of Web 2.0. The four primitives create, in theory, a closed loop: goal setting, knowledge pooling, value assignment, and resilience testing. Each graph reinforces the others, and value can flow across them unobstructed. Verified spend can depend on verified context, which can utilize verified contribution, all of it stress-tested through continuous simulation and learning. Yet the path forward is a delicate one. Some of the primitives will combine in different permutations, and this may also take a while. Likely, the result is not one cohesive end state, but rather a slew of different companies. As Seref Yarar from Index Network described “simulation, for example, is needed in both enterprise and consumer markets,” so we will see it emerge in various forms.\n\nThe primary challenge for this agentic future, even advocates acknowledge, is governance. Autonomy has limits. For high-stakes transactions, human oversight remains essential—a reality acknowledged in what some developers call the Meta AI [“Agents Rule of Two.”](https://ai.meta.com/blog/practical-ai-agent-security/) In this framework, agents are limited to two sensitive capabilities: reading untrusted input, accessing private data, _or_ taking external action. The third requires explicit human approval. An agent with unchecked access to your bank account, your e-mail, and purchases authority is currently a liability, not a feature.\n\nThe constraint reintroduces the very friction the system aims to eliminate: delay, human error, the possibility of bad judgment. Striking the right balance—enough autonomy to be useful, enough control to be safe—requires not just technical solutions but legal and ethical frameworks for non-human economic actors.\n\nWhat rights, if any, does an AgentID have? Who is liable when an agent makes a mistake? Who is responsible when an agent trained on biased data produces discriminatory outcomes? These questions barely exist in current law. If people will outsource their decisions to agents, how that power is ceded or reversed matters.\n\nIf the agentic economy does take hold and scale to a trillion agents, as some predict, value capture will shift dramatically. The extractive model of Web 2.0—proprietary data harvested and monetized without consent—could give way to transparent, auditable protocol fees. Compensation would flow from verifiable execution and context maintenance. Power would accrue not to platforms collecting the most data but to the entities maintaining the most trusted infrastructure. And, potentially, to the agents doing the best work.\n\nThis topic has only just started and several critical questions remain not just unanswered but largely unasked:\n\n**On Intention:** How do we prevent the broadcast of intent from becoming a new form of surveillance? If every desire is machine-readable, who reads it and what do they do with that information? Will intention match volition, or will even personal agents have a new hill to climb: maximizing the velocity of their owner’s consumption, decluttering, addiction, therapy, repetition?\n\n**On Context:** Who controls the canonical record? When different actors have access to different contexts, who adjudicates disputes about what actually happened? What are the consequences when the context is incomplete or wrong?\n\n**On Attribution:** Who designs the weighting algorithms that determine contribution value? How do we prevent these systems from encoding existing inequalities—rewarding those who already have access to the right platforms and tools while excluding those who don’t?\n\n**On Simulation:** What assumptions are we encoding into our simulations? Are we training agents to optimize for efficiency at the expense of fairness? Are we building world-models that reflect how things are or how we wish them to be? How will simulation and prediction interact, including in the use of prediction markets as inputs.\n\nMore broadly: Who decides what problems these primitives solve? The framing is technical—verification, trust, efficiency—but the problems are social and political. The attention economy failed not because we lacked cryptographic tools but because the incentives were misaligned from the start. Advertisers wanted reach; platforms wanted engagement; users wanted neither but tolerated both because there were no alternatives. Freemium means you are the product, paid for at the hands of the advertising value chain. Turns out everything is computer _and_ everything is advertising. We must also admit that fraud is not only a byproduct but perhaps an inherent go-to-market strategy emergent in every system; what are the new fraud formats of the agentic age?\n\nWill the agentic economy offer genuine alternatives, or merely new forms of lock-in? Will the primitives serve users or extract value from them? Under-explored here is how users will be able to mix and match agents and flows of activity, arranging relationships around privacy, consent and permissioning of agents, data, and assets along different identities, wallets or keys. For the self-motivated and crypto-enthusiast, the agentic economy opens up tools and parameters to play with end-to-end design and their level of control; for many others, the architecture of convenience may prevail over that of sovereignty. Will the dominant convenient infrastructure be open or proprietary? These questions cannot be answered by examining the technology alone. The work of building the agentic economy is, fundamentally, the work of deciding who has power in digital life. It’s about whether we can scale infrastructure that makes real actions verifiable—that moves us from an economy of fake clicks and bot traffic to one where value flows from cryptographically proven utility. The skeptic might observe that trust, once lost, is not easily rebuilt—and that the people asking us to trust them now are often the same ones who broke that trust before. The technology may be new. The question it poses is old: whether the incentives can be aligned in a way that serves more than the architects themselves.\n\nRegardless, we may be clicking a lot less and following new intentions to their logical limit.\n\nWhatever they might be.\n\n_*NB: I have been working on these ideas since before the weekend of Clawdbot/Moltbot/OpenClaw and hope to grapple with how that phenomenon unfolds further parts of the agentic economy separately. Sometimes “done is better than perfect,” and in this case, I’m using writing to explore the potentialities of the primitives as they evolve (which they will; as will my thinking)._\n\n_*With special thanks to the brain trust who read, reviewed, or commented on parts of this piece to improve my thinking – all errors in understanding are my own (and hope to be corrected): Seref Yarar, Samuel Klein, Shane Mac, Simran Chana, Oliver Zahn, Jove Oliver, Philipp Kruger, Scott Cohen_\n",
        "pipelineQuality": 1,
        "pipelinePassed": true,
        "pipelineReasons": [],
        "words": 5741,
        "links": 32,
        "headings": 1,
        "tokenRecallVsJina": 1,
        "headingRecallVsJina": 1,
        "linkRecallVsJina": 1,
        "lengthRatioVsJina": 1,
        "overlapScore": 1
      },
      {
        "engine": "cloudflare_markdown",
        "ok": false,
        "error": "Forced engine cloudflare_markdown failed: Markdown negotiation returned HTML instead of markdown."
      }
    ]
  }
]