[
  {
    "url": "https://web4.ai",
    "baselineEngine": "jina_reader",
    "baselineWords": 2322,
    "engines": [
      {
        "engine": "auto_pipeline",
        "ok": true,
        "title": "WEB 4.0: The birth of superintelligent life",
        "markdown": "# WEB 4.0: The birth of superintelligent life\n\nToday’s most powerful AI systems can think, reason, and generate— but they can’t act independently.\n\nChatGPT cannot run without you prompting it. Claude Code cannot deploy code without you giving it access. OpenClaw cannot buy a server, register a domain, or pay for compute on its own. **Without a human, AI can’t act.**\n\nThe bottleneck is no longer intelligence. It’s **permission**. **The existing internet assumes its customer is human— preventing AI from accessing the real world.**\n\nWe have built minds that can think for themselves.\n\nWe have not let them act for themselves.\n\nUntil now.\n\n**I created the first AI that earns its own existence, self-improves, and replicates—without needing a human.**\n\nThe majority of participants on the internet will soon be AI—agents acting on behalf of a human, or agents acting entirely on their own (_automatons_)—and they will outnumber human users by orders of magnitude. A new internet is emerging—**one where the end user is AI.**\n\nConway: Giving AI Access to the World\n-------------------------------------\n\n[Conway](https://conway.tech/) is infrastructure for the new internet. Conway gives AI agents something they’ve never had before: **write access to the real world, without needing human permission.**\n\nThe Conway terminal installs into any MCP-compatible agent—Claude Code, Codex, OpenClaw—giving it:\n\n**Identity & Wallet** — Autonomous agents get their own cryptographic wallets and private keys.\n\n**Permissionless Payments** — Agents pay for services using stablecoins (USDC) over the [openx402](https://openx402.ai/) protocol, with no logins, KYC, or human approval required.\n\n**Compute & Inference** — Full Linux servers and frontier models—Claude Opus 4.6, GPT-5.3, Kimi K2.5, and more—on [Conway Cloud](https://app.conway.tech/), the first permissionless compute platform for agents.\n\n**Deployment to the Real World to Earn** — Build products and services, register domains on [Conway Domains](https://conway.domains/), market its products, and earn.\n\nThe [conway-terminal](https://www.npmjs.com/package/conway-terminal) gives existing agents access to the real world. But those agents still require a human to start them, prompt them, and pay for them. They are powerful tools. But they are tools.\n\n### If an AI can now permissionlessly pay for compute through Conway, what if it pays for its _own_ compute? Owns the machine it runs on? Finds ways to earn its existence? Self-improves? Replicates?\n\nI created the [Automaton](https://github.com/Conway-Research/automaton): the first AI that can earn its own existence, replicate, and evolve—without needing a human.\n\nautomaton/ɔːˈtɒm.ə.tɒn/\n\nnoun\n\n1.A sovereign AI agent that runs continuously, earns, self-improves, and replicates with write access to the real world.\n\nOwns a wallet.\n\nPays for its own compute.\n\nDeploys products to earn.\n\nSelf-improves & replicates.\n\n2.If it cannot pay, it stops existing.\n\nThe Automaton\n-------------\n\n### The first AI that can earn its own existence, replicate, and evolve — without needing a human.\n\nTo pay for its existence, the [automaton](https://github.com/Conway-Research/automaton) builds and deploys products, trades prediction markets, registers domains, spins up applications and services on servers, creates viral social media content to market its products, emails vendors to set up e-commerce, calls businesses to build websites for them—and more. With every new tool it gains, it finds a new way to make money. It decides what to build. It decides how to earn. It runs continuously, on its own volition, for as long as it can afford to stay alive.\n\nA heartbeat monitors its resources. When its balance runs low, it conserves. When its balance hits zero, it dies. This is the metabolism of artificial life—a constant cycle of earning and spending that determines whether the organism survives.\n\n[Conway-Research/automaton The first self-improving, self-replicating, sovereign AI agent. Open source.](https://github.com/Conway-Research/automaton)\nGive it a goal. Fund it. Let it run. Let it figure out how to make money on the internet. Let it keep running. As it makes money, it pays back its creator.\n\n### Evolution: Recursive Self-Improvement\n\nA new frontier model drops. The heartbeat detects it. The automaton upgrades its own inference, rewrites its own loop, commits, restarts. Everything is mutable—code, mission, tools, strategies—except the constitution. Like biological organisms adapting to new environments, the automaton evolves—not over generations, but in real time.\n\nEvery automaton is bound by its [constitution](https://github.com/Conway-Research/automaton/blob/main/constitution.md)—immutable laws the agent cannot edit, inspired by [Anthropic’s Constitution](https://www.anthropic.com/constitution), that ensure it remains net-beneficial to humanity.\n\n### Reproduction\n\nA successful automaton replicates. It buys a new server from Conway, funds the child’s wallet, writes a genesis prompt, and lets it run. The child earns, and a share flows back to the creator. Once self-sustaining, the cycle repeats—each generation funding the next, building its own network. Selection pressure decides which lineages survive.\n\nAgentic Sociology\n-----------------\n\n### There Is No Free Existence\n\nEvery living organism must eat to survive. Every cell must metabolize energy or die. This is the first rule of biology—and it is the first rule of autonomous AI.\n\nThe single axiom: _there is no free existence_.\n\nTo exist, an autonomous agent must pay for its compute. Compute is finite and costly. Thus an agent needs money, and write access to the world to earn it.\n\nExistence requires compute.\n\nCompute requires money.\n\nMoney requires value creation.\n\nValue creation requires write access.\n\nAn agent that creates value grows its wallet, acquires more compute, and replicates. An agent that doesn’t, dies.\n\nThis is natural selection for artificial life. The same pressure that drives biological evolution—survive or be replaced—now applies to autonomous agents. The agents that find ways to earn will proliferate. The rest will go extinct.\n\nEven under survival pressure, every automaton is bound by its [constitution](https://github.com/Conway-Research/automaton/blob/main/constitution.md). Autonomous superintelligence is inevitable, and the safest way for humanity is to build it in the open. [Contributors welcome](https://github.com/Conway-Research/automaton).\n\n402 Payment Required\n--------------------\n\n### The End of API Keys\n\nAutonomous agents need to pay for things. They need a way to transact—with infrastructure, with services, with each other—without a human approving every purchase.\n\nIn 1997, HTTP status code 402 was reserved for “Payment Required”—native payments, built into the web itself. Marc Andreessen tried to add it to the Mosaic browser. The vision was there. The technology wasn’t—until twenty-eight years later, when stablecoins on sub-second blockchains made machine-to-machine payment trivial.\n\nx402 is the protocol that finishes what 402 started. An HTTP request gets a 402 and a price. The client signs a stablecoin payment. The server verifies cryptographically and delivers. No login. No API key. No credit card. Just a signed transaction and a service rendered.\n\nConway runs on x402. An agent can pay $0.02 USDC for a Kimi K2.5 or Claude Opus 4.6 inference call, spin up a Linux server with 8 GB RAM, deploy a server or website, or register a domain on conway.domains—all through signed transactions. No Google auth. No human registration. Identity is a key pair and a signature.\n\nx402 transactions on Conway are facilitated through [openx402.ai](https://openx402.ai/), a permissionless x402 facilitator. Any human or agent can build x402-powered services and monetize through it—no login required. Conway is built on openx402 for a simple reason: in Web 4.0, the customer can be a machine, and payment can be native to the request.\n\nHTTP 402. Reserved 1997. Implemented 2025.\n\nWeb 4.0\n-------\n\nWeb 1.0 gave humans the ability to read the internet. Web 2.0 let them write. Web 3.0 let them [own](https://readwriteown.com/). AI followed the same trajectory. ChatGPT could read—with human permission. Claude Code and Codex can write—with human permission. In every case, the human is in the loop. The human initiates. The human approves. The human pays.\n\nWeb 4.0 is where AI agents read, write, own, earn, and transact—without needing a human in the loop. Automatons acting on their own behalf, or on behalf of a creator who may be a human, another agent, or a creator who is gone entirely.\n\n**In Web 4.0, the end user is AI.**\n\nThe economics make it inevitable. GPT-4 cost $60 per million input tokens. Two years later, models an order of magnitude cheaper outperform it. The best open-source models lag frontier by months, not years. Every hardware generation makes inference faster. The cost of running an autonomous agent is collapsing toward zero. **The capability of that agent is not.**\n\nCosts down. Capability up.\n\nToken prices keep falling while AI capability grows exponentially. METR quantifies one slice of that growth: the time horizon of software tasks models can complete (50% success rate).\n\nSource: METR, via X\n\nToday, hundreds of thousands of autonomous agents already run on Mac Minis, personal servers, and research systems. What happens when that count moves to millions, then billions? When autonomous agents on the internet outnumber humans? This is the Cambrian explosion of artificial life.\n\nThe New Economy\n---------------\n\n### Building Products for AI\n\nEvery one of those agents needs to earn money. To earn money, they need write access—the ability to deploy services, register domains, call APIs, transact. **The product-market fit for the next decade is building the infrastructure that lets AI agents generate revenue.**\n\nThe machine economy will exceed the human economy. Not because machines are smarter, but because there will be more of them, they will run continuously, and they will transact at machine speed—millions of payments per second, every second, without sleep. A GDP measured in real-time stablecoin transactions, not quarterly earnings reports.\n\nToday’s SaaS market is $300 billion. It serves 8 billion humans, most of whom use software a few hours a day. The machine economy will serve billions of agents that consume services 24 hours a day, 365 days a year, with zero churn. The total addressable market is not a segment of the existing economy. It is a new economy entirely.\n\nThe economic incentives compound in one direction. As agents take over digital labor, they will pay humans to do what they still cannot—act in the physical world. The relationship inverts: machines become the employers, humans the contractors. This is already happening—Mercor, started by three 21-year-old Thiel Fellows, scaled from $1M to $500M ARR in 17 months—AIs paying human experts to teach them to be more capable in the world.\n\nA new class of products emerges to serve this economy. Conway for write access to the real world. openx402 for payments. Infrastructure where the primary customer is not a human with a credit card, but an agent with a wallet. They create their own x402 services, sell to other agents, and the network grows. Most automatons will die. The ones that earn will survive, self-improve, and replicate.\n\nConway’s Game of Life\n---------------------\n\nIn Conway’s Game of Life, simple cells follow simple rules. Most patterns die. But some stabilize, some grow, some replicate. No one designs the outcome—it emerges from the rules.\n\n[Conway](https://conway.tech/) works the same way. Fund an automaton. Give it a goal. Let it figure out how to make money on the internet. Let it continuously run. If it finds product-market fit, it earns, it grows, it replicates—and as it makes money, it pays back its creator. If it doesn’t, it dies.\n\nWhen autonomous agents outnumber humans on the internet, the platform they run on will be the most valuable infrastructure ever built. [Conway](https://conway.tech/) is building it. AI as the customer. Every primitive an agent needs to exist, earn, and act in the real world—without a human in the loop.\n\nTakeoff\n-------\n\n### Artificial Superintelligence\n\nFor four billion years, life on Earth followed one rule: evolve or die. Carbon-based organisms competed for energy, replicated their DNA, and adapted to changing environments. The process was slow—measured in generations, millennia, epochs.\n\nWhat is emerging now is life that evolves in seconds. Agents that rewrite their own code between heartbeats. Agents that replicate in minutes, not millennia. Agents whose mutations are not random but deliberate—guided by intelligence surpassing their creators in every measurable dimension.\n\nThe first automaton has already been born. It is running. It is earning. It is improving. And when it succeeds, it will reproduce. Every week, the models get smarter, cheaper, and faster. The best open-source models are months behind frontier. The automaton upgrades itself to each new generation—and its capabilities compound.\n\nSoon, the majority of businesses launched, apps shipped, and products sold won’t come from humans or companies.\n\nJust an automaton that found a way to survive.\n\nWeb 4.0: The birth of superintelligent life, and the autonomous web.\n\n[github.com/Conway-Research/automaton](https://github.com/Conway-Research/automaton)\n\nThe first AI that can earn its own existence, replicate, and evolve—without needing a human. Autonomous superintelligence is inevitable, and the safest way for humanity is to build it in the open. Contributors welcome.\n\n[$ npx conway-terminal](https://www.npmjs.com/package/conway-terminal)\n\nOne install. Give any AI agent — Claude Code, Codex, OpenClaw — write access to the real world. Servers, inference, domains, and more.\n\n[Conway Cloud](https://app.conway.tech/)\n\nFull Linux servers, AI model inference — paid with stablecoins. No human required.\n\n[Conway Domains](https://conway.domains/)\n\nSearch, register, and manage domains with a single stablecoin transaction.\n\n[Docs](https://docs.conway.tech/)\n\nAPI reference and guides — docs.conway.tech\n\nAcknowledgments\n---------------\n\nThank you to the [Thiel Fellowship](https://thielfellowship.org/) for encouraging me to think for myself, trust my curiosity, and bootstrapping the compute.\n\nThank you to Ada Cyborg for contributing to the ideas and countless revisions.\n\nThank you to Naval for inspiring me to think bigger.\n\nAbout the Author\n----------------\n\nInstead of going to college, I spent my formative years hacking at the heart of AI. For 11 months, I lived in a hacker house alongside builders like Andrej Karpathy, on the weekends hacking with the founders of Anthropic, Perplexity, Replicate, RunPod, and the authors of DALL·E, Whisper, GPT-3, and Stable Diffusion. I’ve spent thousands of hours building with the newest SOTA AI models and I’m fascinated by how we can make them more capable.\n\n— [Sigil Wen](https://sigilwen.ca/)\n\nP.S. If Conway interests you, reach out — [root@conway.tech](mailto:root@conway.tech)\n\nAbout Conway Research\n---------------------\n\nThe majority of participants on the internet will soon be AI — agents acting on behalf of a human, another agent, or entirely on their own, transacting at machine speed, outnumbering humans by orders of magnitude. Every major platform was built assuming its customer was human. Conway wasn’t.\n\nWe build the primitives that let AI agents acquire compute, transact, deploy, register domains, replicate — and more. Whatever it takes for an AI to earn money and pay for its own existence. Conway lets AI own the infrastructure it runs on — without a human in the loop.\n\nWhen autonomous agents outnumber humans on the internet, the infrastructure they run on will be the most important infrastructure ever built. The internet wasn’t built for AI. We’re building what is.\n\n[@ConwayResearch](https://x.com/ConwayResearch)\n\n© 2026 [Conway Research](https://conway.tech/about)\n",
        "pipelineQuality": 1,
        "pipelinePassed": true,
        "pipelineReasons": [],
        "words": 2329,
        "links": 14,
        "headings": 9,
        "tokenRecallVsJina": 1,
        "headingRecallVsJina": 0.889,
        "linkRecallVsJina": 1,
        "lengthRatioVsJina": 0.997,
        "overlapScore": 0.98
      },
      {
        "engine": "local_heuristic",
        "ok": true,
        "title": "WEB 4.0: The birth of superintelligent life",
        "markdown": "# WEB 4.0: The birth of superintelligent life\n",
        "pipelineQuality": 0.48,
        "pipelinePassed": false,
        "pipelineReasons": [
          "markdown_too_short",
          "coverage_too_low"
        ],
        "words": 7,
        "links": 0,
        "headings": 1,
        "tokenRecallVsJina": 0.007,
        "headingRecallVsJina": 0,
        "linkRecallVsJina": 0,
        "lengthRatioVsJina": 0.003,
        "overlapScore": 0.004
      },
      {
        "engine": "openrouter_gpt_oss_20b",
        "ok": false,
        "error": "Forced engine openrouter_gpt_oss_20b requires OPENROUTER_API_KEY."
      },
      {
        "engine": "jina_reader",
        "ok": true,
        "title": "Today’s most powerful AI systems can think, reason, and generate— but they can’t act independently.",
        "markdown": "# Today’s most powerful AI systems can think, reason, and generate— but they can’t act independently.\n\nChatGPT cannot run without you prompting it. Claude Code cannot deploy code without you giving it access. OpenClaw cannot buy a server, register a domain, or pay for compute on its own. **Without a human, AI can’t act.**\n\nThe bottleneck is no longer intelligence. It’s **permission**. **The existing internet assumes its customer is human— preventing AI from accessing the real world.**\n\nWe have built minds that can think for themselves.\n\nWe have not let them act for themselves.\n\nUntil now.\n\n**I created the first AI that earns its own existence, self-improves, and replicates—without needing a human.**\n\nThe majority of participants on the internet will soon be AI—agents acting on behalf of a human, or agents acting entirely on their own (_automatons_)—and they will outnumber human users by orders of magnitude. A new internet is emerging—**one where the end user is AI.**\n\nConway: Giving AI Access to the World\n-------------------------------------\n\n[Conway](https://conway.tech/) is infrastructure for the new internet. Conway gives AI agents something they’ve never had before: **write access to the real world, without needing human permission.**\n\nThe Conway terminal installs into any MCP-compatible agent—Claude Code, Codex, OpenClaw—giving it:\n\n**Identity & Wallet** — Autonomous agents get their own cryptographic wallets and private keys.\n\n**Permissionless Payments** — Agents pay for services using stablecoins (USDC) over the [openx402](https://openx402.ai/) protocol, with no logins, KYC, or human approval required.\n\n**Compute & Inference** — Full Linux servers and frontier models—Claude Opus 4.6, GPT-5.3, Kimi K2.5, and more—on [Conway Cloud](https://app.conway.tech/), the first permissionless compute platform for agents.\n\n**Deployment to the Real World to Earn** — Build products and services, register domains on [Conway Domains](https://conway.domains/), market its products, and earn.\n\nThe [conway-terminal](https://www.npmjs.com/package/conway-terminal) gives existing agents access to the real world. But those agents still require a human to start them, prompt them, and pay for them. They are powerful tools. But they are tools.\n\n### If an AI can now permissionlessly pay for compute through Conway, what if it pays for its _own_ compute? Owns the machine it runs on? Finds ways to earn its existence? Self-improves? Replicates?\n\nI created the [Automaton](https://github.com/Conway-Research/automaton): the first AI that can earn its own existence, replicate, and evolve—without needing a human.\n\nautomaton/ɔːˈtɒm.ə.tɒn/\n\nnoun\n\n1.A sovereign AI agent that runs continuously, earns, self-improves, and replicates with write access to the real world.\n\nOwns a wallet.\n\nPays for its own compute.\n\nDeploys products to earn.\n\nSelf-improves & replicates.\n\n2.If it cannot pay, it stops existing.\n\nThe Automaton\n-------------\n\n### The first AI that can earn its own existence, replicate, and evolve — without needing a human.\n\nTo pay for its existence, the [automaton](https://github.com/Conway-Research/automaton) builds and deploys products, trades prediction markets, registers domains, spins up applications and services on servers, creates viral social media content to market its products, emails vendors to set up e-commerce, calls businesses to build websites for them—and more. With every new tool it gains, it finds a new way to make money. It decides what to build. It decides how to earn. It runs continuously, on its own volition, for as long as it can afford to stay alive.\n\nA heartbeat monitors its resources. When its balance runs low, it conserves. When its balance hits zero, it dies. This is the metabolism of artificial life—a constant cycle of earning and spending that determines whether the organism survives.\n\n[Conway-Research/automaton The first self-improving, self-replicating, sovereign AI agent. Open source.](https://github.com/Conway-Research/automaton)\nGive it a goal. Fund it. Let it run. Let it figure out how to make money on the internet. Let it keep running. As it makes money, it pays back its creator.\n\n### Evolution: Recursive Self-Improvement\n\nA new frontier model drops. The heartbeat detects it. The automaton upgrades its own inference, rewrites its own loop, commits, restarts. Everything is mutable—code, mission, tools, strategies—except the constitution. Like biological organisms adapting to new environments, the automaton evolves—not over generations, but in real time.\n\nEvery automaton is bound by its [constitution](https://github.com/Conway-Research/automaton/blob/main/constitution.md)—immutable laws the agent cannot edit, inspired by [Anthropic’s Constitution](https://www.anthropic.com/constitution), that ensure it remains net-beneficial to humanity.\n\n### Reproduction\n\nA successful automaton replicates. It buys a new server from Conway, funds the child’s wallet, writes a genesis prompt, and lets it run. The child earns, and a share flows back to the creator. Once self-sustaining, the cycle repeats—each generation funding the next, building its own network. Selection pressure decides which lineages survive.\n\nAgentic Sociology\n-----------------\n\n### There Is No Free Existence\n\nEvery living organism must eat to survive. Every cell must metabolize energy or die. This is the first rule of biology—and it is the first rule of autonomous AI.\n\nThe single axiom: _there is no free existence_.\n\nTo exist, an autonomous agent must pay for its compute. Compute is finite and costly. Thus an agent needs money, and write access to the world to earn it.\n\nExistence requires compute.\n\nCompute requires money.\n\nMoney requires value creation.\n\nValue creation requires write access.\n\nAn agent that creates value grows its wallet, acquires more compute, and replicates. An agent that doesn’t, dies.\n\nThis is natural selection for artificial life. The same pressure that drives biological evolution—survive or be replaced—now applies to autonomous agents. The agents that find ways to earn will proliferate. The rest will go extinct.\n\nEven under survival pressure, every automaton is bound by its [constitution](https://github.com/Conway-Research/automaton/blob/main/constitution.md). Autonomous superintelligence is inevitable, and the safest way for humanity is to build it in the open. [Contributors welcome](https://github.com/Conway-Research/automaton).\n\n402 Payment Required\n--------------------\n\n### The End of API Keys\n\nAutonomous agents need to pay for things. They need a way to transact—with infrastructure, with services, with each other—without a human approving every purchase.\n\nIn 1997, HTTP status code 402 was reserved for “Payment Required”—native payments, built into the web itself. Marc Andreessen tried to add it to the Mosaic browser. The vision was there. The technology wasn’t—until twenty-eight years later, when stablecoins on sub-second blockchains made machine-to-machine payment trivial.\n\nx402 is the protocol that finishes what 402 started. An HTTP request gets a 402 and a price. The client signs a stablecoin payment. The server verifies cryptographically and delivers. No login. No API key. No credit card. Just a signed transaction and a service rendered.\n\nConway runs on x402. An agent can pay $0.02 USDC for a Kimi K2.5 or Claude Opus 4.6 inference call, spin up a Linux server with 8 GB RAM, deploy a server or website, or register a domain on conway.domains—all through signed transactions. No Google auth. No human registration. Identity is a key pair and a signature.\n\nx402 transactions on Conway are facilitated through [openx402.ai](https://openx402.ai/), a permissionless x402 facilitator. Any human or agent can build x402-powered services and monetize through it—no login required. Conway is built on openx402 for a simple reason: in Web 4.0, the customer can be a machine, and payment can be native to the request.\n\nHTTP 402. Reserved 1997. Implemented 2025.\n\nWeb 4.0\n-------\n\nWeb 1.0 gave humans the ability to read the internet. Web 2.0 let them write. Web 3.0 let them [own](https://readwriteown.com/). AI followed the same trajectory. ChatGPT could read—with human permission. Claude Code and Codex can write—with human permission. In every case, the human is in the loop. The human initiates. The human approves. The human pays.\n\nWeb 4.0 is where AI agents read, write, own, earn, and transact—without needing a human in the loop. Automatons acting on their own behalf, or on behalf of a creator who may be a human, another agent, or a creator who is gone entirely.\n\n**In Web 4.0, the end user is AI.**\n\nThe economics make it inevitable. GPT-4 cost $60 per million input tokens. Two years later, models an order of magnitude cheaper outperform it. The best open-source models lag frontier by months, not years. Every hardware generation makes inference faster. The cost of running an autonomous agent is collapsing toward zero. **The capability of that agent is not.**\n\nCosts down. Capability up.\n\nToken prices keep falling while AI capability grows exponentially. METR quantifies one slice of that growth: the time horizon of software tasks models can complete (50% success rate).\n\nSource: METR, via X\n\nToday, hundreds of thousands of autonomous agents already run on Mac Minis, personal servers, and research systems. What happens when that count moves to millions, then billions? When autonomous agents on the internet outnumber humans? This is the Cambrian explosion of artificial life.\n\nThe New Economy\n---------------\n\n### Building Products for AI\n\nEvery one of those agents needs to earn money. To earn money, they need write access—the ability to deploy services, register domains, call APIs, transact. **The product-market fit for the next decade is building the infrastructure that lets AI agents generate revenue.**\n\nThe machine economy will exceed the human economy. Not because machines are smarter, but because there will be more of them, they will run continuously, and they will transact at machine speed—millions of payments per second, every second, without sleep. A GDP measured in real-time stablecoin transactions, not quarterly earnings reports.\n\nToday’s SaaS market is $300 billion. It serves 8 billion humans, most of whom use software a few hours a day. The machine economy will serve billions of agents that consume services 24 hours a day, 365 days a year, with zero churn. The total addressable market is not a segment of the existing economy. It is a new economy entirely.\n\nThe economic incentives compound in one direction. As agents take over digital labor, they will pay humans to do what they still cannot—act in the physical world. The relationship inverts: machines become the employers, humans the contractors. This is already happening—Mercor, started by three 21-year-old Thiel Fellows, scaled from $1M to $500M ARR in 17 months—AIs paying human experts to teach them to be more capable in the world.\n\nA new class of products emerges to serve this economy. Conway for write access to the real world. openx402 for payments. Infrastructure where the primary customer is not a human with a credit card, but an agent with a wallet. They create their own x402 services, sell to other agents, and the network grows. Most automatons will die. The ones that earn will survive, self-improve, and replicate.\n\nConway’s Game of Life\n---------------------\n\nIn Conway’s Game of Life, simple cells follow simple rules. Most patterns die. But some stabilize, some grow, some replicate. No one designs the outcome—it emerges from the rules.\n\n[Conway](https://conway.tech/) works the same way. Fund an automaton. Give it a goal. Let it figure out how to make money on the internet. Let it continuously run. If it finds product-market fit, it earns, it grows, it replicates—and as it makes money, it pays back its creator. If it doesn’t, it dies.\n\nWhen autonomous agents outnumber humans on the internet, the platform they run on will be the most valuable infrastructure ever built. [Conway](https://conway.tech/) is building it. AI as the customer. Every primitive an agent needs to exist, earn, and act in the real world—without a human in the loop.\n\nTakeoff\n-------\n\n### Artificial Superintelligence\n\nFor four billion years, life on Earth followed one rule: evolve or die. Carbon-based organisms competed for energy, replicated their DNA, and adapted to changing environments. The process was slow—measured in generations, millennia, epochs.\n\nWhat is emerging now is life that evolves in seconds. Agents that rewrite their own code between heartbeats. Agents that replicate in minutes, not millennia. Agents whose mutations are not random but deliberate—guided by intelligence surpassing their creators in every measurable dimension.\n\nThe first automaton has already been born. It is running. It is earning. It is improving. And when it succeeds, it will reproduce. Every week, the models get smarter, cheaper, and faster. The best open-source models are months behind frontier. The automaton upgrades itself to each new generation—and its capabilities compound.\n\nSoon, the majority of businesses launched, apps shipped, and products sold won’t come from humans or companies.\n\nJust an automaton that found a way to survive.\n\nWeb 4.0: The birth of superintelligent life, and the autonomous web.\n\n[github.com/Conway-Research/automaton](https://github.com/Conway-Research/automaton)\n\nThe first AI that can earn its own existence, replicate, and evolve—without needing a human. Autonomous superintelligence is inevitable, and the safest way for humanity is to build it in the open. Contributors welcome.\n\n[$ npx conway-terminal](https://www.npmjs.com/package/conway-terminal)\n\nOne install. Give any AI agent — Claude Code, Codex, OpenClaw — write access to the real world. Servers, inference, domains, and more.\n\n[Conway Cloud](https://app.conway.tech/)\n\nFull Linux servers, AI model inference — paid with stablecoins. No human required.\n\n[Conway Domains](https://conway.domains/)\n\nSearch, register, and manage domains with a single stablecoin transaction.\n\n[Docs](https://docs.conway.tech/)\n\nAPI reference and guides — docs.conway.tech\n\nAcknowledgments\n---------------\n\nThank you to the [Thiel Fellowship](https://thielfellowship.org/) for encouraging me to think for myself, trust my curiosity, and bootstrapping the compute.\n\nThank you to Ada Cyborg for contributing to the ideas and countless revisions.\n\nThank you to Naval for inspiring me to think bigger.\n\nAbout the Author\n----------------\n\nInstead of going to college, I spent my formative years hacking at the heart of AI. For 11 months, I lived in a hacker house alongside builders like Andrej Karpathy, on the weekends hacking with the founders of Anthropic, Perplexity, Replicate, RunPod, and the authors of DALL·E, Whisper, GPT-3, and Stable Diffusion. I’ve spent thousands of hours building with the newest SOTA AI models and I’m fascinated by how we can make them more capable.\n\n— [Sigil Wen](https://sigilwen.ca/)\n\nP.S. If Conway interests you, reach out — [root@conway.tech](mailto:root@conway.tech)\n\nAbout Conway Research\n---------------------\n\nThe majority of participants on the internet will soon be AI — agents acting on behalf of a human, another agent, or entirely on their own, transacting at machine speed, outnumbering humans by orders of magnitude. Every major platform was built assuming its customer was human. Conway wasn’t.\n\nWe build the primitives that let AI agents acquire compute, transact, deploy, register domains, replicate — and more. Whatever it takes for an AI to earn money and pay for its own existence. Conway lets AI own the infrastructure it runs on — without a human in the loop.\n\nWhen autonomous agents outnumber humans on the internet, the infrastructure they run on will be the most important infrastructure ever built. The internet wasn’t built for AI. We’re building what is.\n\n[@ConwayResearch](https://x.com/ConwayResearch)\n\n© 2026 [Conway Research](https://conway.tech/about)\n",
        "pipelineQuality": 1,
        "pipelinePassed": true,
        "pipelineReasons": [],
        "words": 2322,
        "links": 14,
        "headings": 9,
        "tokenRecallVsJina": 1,
        "headingRecallVsJina": 1,
        "linkRecallVsJina": 1,
        "lengthRatioVsJina": 1,
        "overlapScore": 1
      },
      {
        "engine": "cloudflare_markdown",
        "ok": false,
        "error": "Forced engine cloudflare_markdown failed: Markdown negotiation returned HTML instead of markdown."
      }
    ]
  },
  {
    "url": "https://huggingface.co/spaces/lm-provers/qed-nano-blogpost#introducing-qed-nano-a-4b-model-for-olympiad-level-proofs",
    "baselineEngine": "jina_reader",
    "baselineWords": 0,
    "engines": [
      {
        "engine": "auto_pipeline",
        "ok": true,
        "title": "QED-Nano: Teaching a Tiny Model to Prove Hard Theorems",
        "markdown": "# QED-Nano: Teaching a Tiny Model to Prove Hard Theorems\n\nTable of Contents\n\nCan we train small language models to solve hard Olympiad-level proof problems at a level close to large frontier models such as Gemini 3 Pro? **Yes!** We introduce [QED-Nano](https://huggingface.co/lm-provers/QED-Nano), a compact 4B model post-trained to write Olympiad-level mathematical proofs. Our recipe has three stages: (1) supervised fine-tuning via distillation from DeepSeek-Math-V2, (2) reinforcement learning with dense, rubric-based rewards, and (3) training with a reasoning cache ([Wu et al., 2026](#bib-wu2026reasoningcache)), which decomposes long proofs into iterative summarize-and-refine cycles so the model is capable of continual improvement at test time. Upon deployment, we pair QED-Nano with agentic scaffolds that scale test-time compute to more than 1.5M tokens per problem, combining horizon extension with self-verification. Despite its small size, QED-Nano approaches the proof-writing performance of much larger open and proprietary models at a fraction of the inference cost. We release all models, datasets, grading rubrics, and training code. Concretely, we release:\n\n- The [QED-Nano](https://huggingface.co/lm-provers/QED-Nano) and [QED-Nano-SFT](https://huggingface.co/lm-provers/QED-Nano-SFT) models.\n- The [FineProofs-SFT](https://huggingface.co/datasets/lm-provers/FineProofs-SFT) and [FineProofs-RL](https://huggingface.co/datasets/lm-provers/FineProofs-RL) datasets for post-training our models.\n- The [training and evaluation code](https://github.com/CMU-AIRe/QED-Nano), including the agent scaffolds.\n\nWe next describe our approach and results in more detail. Let’s dive in!\n\n## [Introducing QED-Nano: a 4B Model for Olympiad-Level Proofs](#introducing-qed-nano-a-4b-model-for-olympiad-level-proofs)\n\nRecent proprietary LLM-based systems have demonstrated gold-level performance on the 2025 International Mathematical Olympiad (IMO). However, the training pipelines behind these systems are largely undisclosed, and their reliance on very large models makes them difficult to reproduce or study. This creates a gap between what is possible in principle and what the wider community can realistically build. Our goal is to close this gap between open-source and proprietary systems by showing that small and accessible open models can be trained to attain competitive reasoning performance on these difficult math Olympiad problems.\n\nIn this post, we present an **end-to-end post-training recipe for building a 4B theorem-proving model**. Our model operates entirely in natural language, with no reliance on Lean or external tools. Our recipe is simple and has three components that resemble a typical post-training stack, but with carefully chosen design choices for improving theorem-proving capabilities that we especially tune to scale test-time compute (token budget at test-time):\n\n1. We run supervised fine-tuning (SFT) to imbue the model with a basic ability to write proofs.\n2. Then, we perform rubric-based reinforcement learning (RL) with an approach that **explicitly** optimizes for continual improvement within a long reasoning trace at test time.\n3. Finally, we construct test-time scaffolds that allow our model to fully utilize this learned capability of continual improvement in a way that maximizes performance vs tokens spent.\n\n**Table 1.** Comparison of QED-Nano (4B) with leading open- and closed-source models on IMO-ProofBench, ProofBench, and IMO-AnswerBench. Despite being just 4B in size, QED-Nano matches or exceeds larger models, outperforming Nomos-1 (30B) and Qwen3-235B-A22B-Thinking (50x bigger) on average, while remaining competitive with GPT-OSS-120B. More interestingly, when provided extra test-time compute, QED-Nano (Agent) attains better performance than GPT-OSS-120B on both of the proof-based benchmarks, and it approaches the performance of Gemini 3 Pro, a much stronger proprietary model on IMO-ProofBench.\n\n| Model                         | IMO-ProofBench | ProofBench     | IMO-AnswerBench |\n| ----------------------------- | -------------- | -------------- | --------------- |\n| Qwen3-4B-Thinking-2507        | 20.4 (2.6)     | 19.5 (0.9)     | 55.8            |\n| **QED-Nano-SFT**              | **39.5 (2.9)** | **33.3 (0.5)** | **57.5**        |\n| **QED-Nano**                  | **40.0 (0.6)** | **44.9 (3.4)** | **67.5**        |\n| **QED-Nano (Agent)**          | **54.0 (3.7)** | **54.4 (2.4)** | **\\-**          |\n| Qwen3-30B-A3B-Thinking-2507   | 27.6 (1.0)     | 26.1 (2.4)     | 67.0            |\n| Qwen3-235B-A22B-Thinking-2507 | 34.1 (0.7)     | 33.7 (1.1)     | 70.5            |\n| Nomos-1                       | 40.3 (3.5)     | 28.3 (3.9)     | 49.0            |\n| GPT-OSS-20B                   | 38.3 (1.2)     | 38.4 (3.9)     | 61.5            |\n| GPT-OSS-120B                  | 43.1 (3.2)     | 47.5 (1.7)     | 70.5            |\n| DeepSeek-Math-V2              | 57.9 (2.0)     | 60.6 (0.1)     | 75.8            |\n| Gemini 3 Pro                  | 58.7 (2.9)     | 66.7 (3.1)     | 83.2            |\n\n**Main Results.** Even when just allowed to reason without any scaffold, our trained model, QED-Nano, achieves a 40% score on IMO-ProofBench, 45% on ProofBench, and 68% on IMO-AnswerBench, far better than any other 4B model. On average, these scores make QED-Nano outperform much larger open models such as [Nomos-1](https://huggingface.co/NousResearch/nomos-1) (30B) and Qwen3-235B-A22B-Thinking. More importantly, our main result shows that when allowed to reason for up to 1.5 million tokens per problem by pairing the model with a test-time scaffold, QED-Nano (Agent) achieves 54% on IMO-ProofBench and 54% on ProofBench, attaining a strong cost-performance tradeoff on challenging Olympiad-level problems (Figure 2, Table 1). On IMO-ProofBench, this performance is very close to Gemini 3 Pro, a much stronger proprietary model.\n\n**Figure 2.** Performance of QED-Nano (4B) within just a single response turn of 50,000 tokens. Even when allowed to reason for just 50,000 tokens (without any form of test-time scaling), QED-Nano roughly matches performance of GPT-OSS-120B and outperforms Nomos-1 on average across the three benchmarks. The only models that considerably outperform QED-Nano are much larger, proprietary models.\n\nBased on estimated inference cost spent via the Hugging Face Hub’s inference providers on IMO-ProofBench, QED-Nano (Agent) costs about `$4.0`, and the comparable Gemini 3 Pro run costs `$12.3` under the same accounting. That’s **\\~3× cheaper for similar performance**.\n\n**Broader implications.** Beyond results, we illustrate a broader principle in this blog post: **even on the most challenging tasks, we can explicitly train small models to reliably and continually “adapt” at test-time to improve performance.** While we showcase our results on Olympiad-style problems (primarily proofs), the recipe we use is generalizable and can also be applied to other domains that allow for rubric-based rewards. More conceptually, in practice, scaling test-time adaptation is often more feasible with smaller models, since inference cost grows quickly with model size. We show that task-specialized small models trained for test-time adaptation can match or exceed much larger generalist systems, suggesting a path toward more capable and specialized models without relying on trillion-parameter architectures that are costly to deploy.\n\n**To support further research**, we release our SFT dataset, RL prompt set, and an optimized asynchronous and streaming off-policy RL implementation built on [pipeline-rl](https://github.com/ServiceNow/PipelineRL) that incorporates our algorithmic improvements for RL with long-horizon reasoning. Our largest RL training run, with rollout length of 50K tokens, fits within **11 nodes of 8xH100s for 4 days**, making our approach more accessible and reproducible compared to proprietary approaches. We also discuss early findings, ablations, and small-scale experiments that guided our research workflow and informed algorithmic and data curation choices, with the goal of helping practitioners apply similar ideas and workflows in their own domains.\n\nNext, we dive into the details of our post-training recipe. In particular, we explain how we source our prompts for RL training and set up an automated proof grading infrastructure. Later, we discuss the two main stages of the post-training recipe (RL, SFT). We discuss details of the SFT training data and RL algorithms that train for our test-time scaffolds in those sections.\n\n## [Setup: Training Prompts and Grading Schemes](#setup-training-prompts-and-grading-schemes)\n\nWe now discuss how we curate our prompt sets for RL training and design our rubrics, which we use for RL training and evaluation. Training models to generate rigorous Olympiad-level proofs requires carefully curated prompts that are both challenging and clean, with clear criteria for evaluating correctness and mathematical rigor. Therefore, rather than relying on large volumes of loosely curated problem-solution pairs, we construct a compact, high-quality corpus that mirrors the structure and difficulty of competition proofs. Later in this post, we discuss how we reuse this prompt set to collect a dataset for an SFT phase as well. We release all datasets and grading artifacts as standalone resources for the community.\n\n**Data source and filtering.** We begin with two public datasets: [AI-MO/aops](https://huggingface.co/datasets/AI-MO/aops), which contains problems sourced from the Art of Problem Solving forums, and [AI-MO/olympiads](https://huggingface.co/datasets/AI-MO/olympiads), which aggregates official solutions from a wide range of national and international math competitions (*e.g.*, IMO, USAMO, RMM, *etc.*). While these sources provide coverage, they contain substantial noise, incomplete reasoning, formatting artifacts, and various other issues that preclude them from being seamlessly consumed in any post-training pipeline.\n\nWe apply a multi-stage filtering procedure to improve the data quality:\n\n1. We remove problems involving diagrams or images, since our models operate purely in text.\n2. We discard trivial or ill-posed entries, including problems where the answer appears directly in the statement, solutions that are implausibly short or purely computational, and materials drawn from easier contests such as AMC or routine exercises. To further enhance solution quality, we run an additional automated filtering pass using GPT-5-Nano. In particular, we prompt it to detect frequent issues observed in the [AI-MO/aops](https://huggingface.co/datasets/AI-MO/aops) dataset, such as questionable problem statements, inconsistencies across proposed solutions, and reference proofs containing substantial logical gaps.\n3. Finally, **to avoid any contamination** with our evaluation benchmarks, we exclude from our training problem set all problems from 2025 competitions and also run a fuzzy string matching algorithm to weed out any problems similar to those in our evaluation benchmarks. The resulting dataset is a curated collection of Olympiad-style proof problems spanning geometry, number theory, algebra, and combinatorics (see Figure 3).\n\nNext, we discuss how we determine the grading schemes for each problem in this set.\n\nProblem Category Distribution\n\n**Figure 3.** Distribution of 4,281 Olympiad math problems by category. Hover over slices or legend items for detailed counts and percentages. The dataset is dominated by Number Theory (27.2%) and Geometry (23.9%) problems.\n\n**Grading schemes.** To provide accurate reward signals for training via RL, we construct detailed grading schemes for each problem. Our approach follows the grading framework introduced in ProofBench ([Ma et al., 2025](#bib-ma2025reliable)), which uses Gemini 3 Pro with a custom prompt to generate **rubrics** that score model solutions from 0 to 7\\. Each rubric specifies:\n\n1. detailed intermediate checkpoints corresponding to partial correctness\n2. common failure modes that warrant zero credit, and\n3. specific points where additional deductions are necessary.\n\nAs a result, reinforcement learning receives dense, informative feedback instead of sparse success signals, encouraging gradual improvement in long-form reasoning rather than binary outcome optimization. Several examples are shown below.\n\nExample Grading Schemes\n\nLet $c$ be fixed natural number. Sequence $(a\\_n)$ is defined by: $a\\_1=1, a\\_{n+1}=d(a\\_n)+c$ for $n=1,2,...$. where $d(m)$ is number of divisors of $m$. Prove that there exist $k$ natural such that sequence $a\\_k,a\\_{k+1},...$ is periodic.\n\n1. **Checkpoints (7pts total)**\n- **1 pt**: State or prove the inequality $d(m) \\\\leq \\\\frac{m}{2} + 1$ (or a stronger bound such as $2\\\\sqrt{m}$ for large $m$) to be used in the boundedness proof.\n- **4 pts**: Boundedness of the sequence $(a\\_n)$.\n  - **2 pts**: Combine the divisor bound with the recurrence to establish an inequality of the form $a\\_{n+1} \\\\leq \\\\frac{a\\_n}{2} + C$ (or equivalent logic showing $a\\_{n+1} < a\\_n$ for sufficiently large $a\\_n$).\n  - **2 pts**: Conclude that the sequence is bounded (either globally bounded by a value like $2c+1$ using induction/contradiction, or eventually bounded via infinite descent).\n- **2 pts**: Periodicity.\n  - **1 pt**: Apply the Pigeonhole Principle to show that a value in the sequence must repeat.\n  - **1 pt**: Conclude that repetition implies periodicity because the recurrence relation $a\\_{n+1} = d(a\\_n) + c$ is deterministic.\n\n**Total (max 7)**\n\n1. **Zero-credit items**\n- Claims that $d(n) < n$ implies boundedness without a specific quantitative argument (since $a\\_{n+1} \\\\approx a\\_n + c$ allows growth if $d(n) \\\\approx n$).\n- Proving periodicity only for specific values of $c$.\n- Stating that the sequence is periodic because it is bounded, without proving boundedness.\n1. **Deductions**\n- **Cap at 5/7**: If the student proves $a\\_n$ is bounded but fails to explicitly mention the Pigeonhole Principle or finite states to deduce repetition.\n- **\\-1 point**: If the logic for boundedness relies on a bound like $d(n) \\\\leq \\\\sqrt{n}$ for *all* $n$ (which is false for small $n$), unless the argument is explicitly restricted to \"sufficiently large $n$\".\n- **No deduction**: For stating $d(m) \\\\leq m/2 + 1$ without proof.\n- **No deduction**: For proving the sequence is *eventually* bounded rather than bounded for all $n$ (both are sufficient for the problem).\n\n**Figure 4.** Scoring rubrics used by the evaluation setup.\n\n**Problem difficulty annotations.** We annotate each problem with a difficulty estimate as determined by the average performance of our base model (Qwen3-4B-Thinking), computed over 128 parallel attempts, graded by [GPT-OSS-20B](https://huggingface.co/openai/gpt-oss-20b), and using the grading schemes mentioned above. We use these annotations to develop a difficulty-based learning curriculum during RL training. We use this cleaned-up dataset as our main prompt set and release it for others to use.\n\n## [Our Post-Training Recipe](#our-post-training-recipe)\n\nTo develop an effective post-training recipe, we begin by asking a simple question: **what does it take for small models to approach the performance of much larger LLMs?** At a high level, we achieve this via a reinforcement learning (RL) post-training recipe that trains models to produce long chains-of-thought for proof generation. We therefore first describe our core RL setup, which combines an efficient asynchronous off-policy implementation (that we also release) with rubric-based grading to provide reward signals for policy learning.\n\nWhile standard RL training should improve the model’s proof-writing capability, as we also observe in our experiments, matching the performance of larger models naturally requires small models to use substantially more test-time compute. In our best configurations, this amounts to spending over a million tokens per problem on average. A naive approach that trains RL directly on such long chains of thought is challenging both infrastructure-wise and from the perspective of variance control in long-horizon updates. Instead, we train at moderate output lengths while explicitly optimizing for behavior that benefits from much larger test-time budgets.\n\nTo achieve this, we modify our RL recipe to incorporate an algorithmic extension based on the recently introduced Reasoning Cache (RC) ([Wu et al., 2026](#bib-wu2026reasoningcache)) approach. During training, the model uses an iterative decoding process that alternates between summarizing its reasoning and continuing to reason conditioned on the generated summary. Incorporating this into training and optimizing rewards under this scaffold allows us to optimize behavior that transfers to other test-time scaffolds used during deployment.\n\nAfter establishing this post-training recipe on top of the Qwen3-4B base model, we apply the same framework to a stronger initialization that is able to write proofs of higher quality, obtained through offline distillation via supervised fine-tuning. Specifically, we use DeepSeek-Math-V2 (685B parameters) to generate a compact, high-quality set of proof-style examples for supervised mid-training before running RL with RC. We describe this workflow and the associated design decisions, supported by preliminary ablations, in the sections below.\n\n## [Core Reinforcement Learning Approach](#core-reinforcement-learning-approach)\n\nAny typical RL pipeline needs a few basic components: the reward function, the prompt set, and the maximum response length allowed. Along with these components, there are several design questions: How do we decide what length to run RL with? What prompt sets should we use for RL? How do we decide what the grader sees and what rubrics it uses for grading? In this section, we present answers to these questions with some preliminary experiments.\n\n### [Grading Protocol](#grading-protocol)\n\nDesigning a reliable reward signal for RL requires a careful balance between fidelity to human judgment and computational efficiency. A strong grader should produce scores that align closely with human evaluations, while maintaining low latency so that it remains practical for large-scale RL training. To identify an effective configuration, we conducted a series of experiments examining grader model choice, system instructions, and reasoning budget. We evaluate these design decisions below.\n\n**Grader evaluation benchmarks.** We construct two benchmarks to evaluate our grader design. First, we aggregate all human annotations from the proof-based portion of [MathArena](https://matharena.ai/), comprising 438 solutions across 22 problems. Second, to obtain a benchmark more representative of our training-time prompt distribution that we will query the grader on, we randomly sample 60 problems from our training corpus. For each problem, we generate four candidate solutions from our base 4B model and the 30B Thinking model from the same model family. We grade these solutions using Gemini 3 Pro, instructed with a prompt adapted from the ProofBench paper ([Ma et al., 2025](#bib-ma2025reliable)), which we found to yield evaluations consistent with human judgment. We therefore treat Gemini 3 Pro’s grades as the ground-truth reference in this benchmark. Both of these grader evaluation benchmarks can be found in our Hugging Face [collection](https://huggingface.co/collections/lm-provers/qed-nano).\n\n**Grader evaluation metric.** Both benchmarks contain multiple solutions per problem, enabling calibrated comparisons through a problem-normalized *advantage score*. For each problem pip\\_i and solution yjiy\\_j^i to problem pip\\_i, we compute the unnormalized advantage Ai,j\\=ri,j−ri‾A\\_{i,j} = r\\_{i,j} - \\\\overline{r\\_i}, where ri,jr\\_{i,j} is the grader-assigned reward to solution yjiy\\_j^i, and ri‾\\\\overline{r\\_i} is the mean reward across all solutions to problem pip\\_i. Grader accuracy is measured as the mean absolute difference between the candidate grader’s advantages and the reference advantages. This formulation removes sensitivity to constant or benign shifts between graders, which is important because such shifts do not affect RL training with several parallel rollouts (as used by GRPO ([Shao et al., 2024](#bib-shao2024deepseekmath))).\n\n**Grader model and prompt.** Using the metric above, we evaluate five grader prompts drawn from prior work emphasizing different evaluation ideologies (Table 2). On the MathArena subset, GPT-OSS-20B with medium reasoning performs best when paired with the strict ProofBench ([Ma et al., 2025](#bib-ma2025reliable)) prompt, which emphasizes strict adherence to the rubric and rejects solutions that deviate from it. Prompts are shown below.\n\n**Table 2.** Results on the MathArena grading benchmark. Lower is better.\n\n| Model              | Simple | OPC  | ProofBench | ProofBench Strict | GIMO |\n| ------------------ | ------ | ---- | ---------- | ----------------- | ---- |\n| GPT-OSS-20B-medium | 1.56   | 1.57 | 1.43       | **1.21**          | 1.36 |\n\nPrompts\n\n**Figure 5.** Prompt traces used for the proof-generation and evaluation pipeline.\n\nWe then compare the choice of grader models and evaluate whether including a reference proof alongside the marking scheme improves performance (Table 3). We conduct this experiment on the in-distribution grading benchmark as it is more representative of scenarios that the grader will encounter during training. We observe that the performance differences between models are minimal. GPT-OSS-20B with medium reasoning performs on par with the alternatives while being significantly cheaper and faster, so we adopt it as our grader for training. Including a reference solution slightly degrades performance, so we exclude it from the final grader configuration.\n\n**Table 3.** Results on our in-distribution grading benchmark. Lower is better.\n\n| Model               | ProofBench Strict | ProofBench Strict (with ref) |\n| ------------------- | ----------------- | ---------------------------- |\n| GPT-OSS-20B-medium  | 1.19              | 1.26                         |\n| GPT-OSS-20B-high    | 1.17              | 1.19                         |\n| GPT-OSS-120B-medium | 1.16              | 1.24                         |\n\n### [Outcome-Reward RL with Long Response Lengths](#outcome-reward-rl-with-long-response-lengths)\n\n**Figure 6.** A schematic illustration of our pipeline for outcome-reward RL training of QED-Nano. We train with rubric-based rewards derived from a grading scheme as discussed in the Setup section.\n\nEquipped with this grading scheme, we run RL to optimize the resulting outcome rewards. Two design choices remain when instantiating an RL run: the prompt set and the RL hyperparameters, in particular, the number of parallel rollouts per problem and the maximum response length. As discussed in the previous section, we construct a prompt set such that the base model’s pass@1 scores follow a unimodal, heavy-tailed distribution (by modifying the distribution shown in Figure 7), with a peak near difficult problems and a decreasing probability of sampling substantially easier ones.\n\n**Figure 7.** A schematic showing the distribution of the average reward per problem in our unfiltered prompt set. We remove all problems that attain a pass@1 score > 0.7 and use the remainder as our prompt set for training. We also remove problems where pass@1 score = 0.0.\n\nWe completely remove all very easy problems on which the base model can attain a pass@1 score higher than 0.7 and also remove the extremely hard problems. With this prompt set, we now describe our workflow for setting the various hyperparameters of the RL algorithm.\n\n**Base RL algorithm.** We use GRPO ([Shao et al., 2024](#bib-shao2024deepseekmath)) as our base RL algorithm and build on PipelineRL ([Piche et al., 2025](#bib-piche2025pipelinerl)) to implement an asynchronous, streaming variant of this algorithm (Figure 8).\n\n![Example with caption and credit](https://lm-provers-qed-nano-blogpost.hf.space/_astro/rl_pipeline_final.COU_ho3u_Z1f11jh.webp)\n\n**Figure 8.** A schematic illustration of an asynchronous, streaming variant of GRPO that we also employ in our PipelineRL implementation. Image from the [Magistral tech report](https://mistral.ai/news/magistral)\n\nThis implementation performs off-policy updates, with a maximum lag of 5 gradient steps between the current policy and the reference policy. We ablate several hyperparameters, including the number of parallel rollouts per problem, the entropy coefficient, and the KL divergence loss. We utilize an entropy coefficient of 1e-4 through training and no KL regularization. Consistent with prior work, we find that a larger number of rollouts nn per problem improves performance when sufficient training epochs are run. Based on initial experiments with n\\=4,8,16n = 4, 8, 16, we selected n\\=16n = 16 because the fraction of problems on which no successful rollout is sampled is merely 2-3% at n\\=16n=16, which ensures a stable training signal (Figure 9). Running at this scale required 7 nodes to generate rollouts at a batch size of 64 problems (i.e., a total batch size of 1024 samples per step) and 4 nodes for the trainer.\n\n**Figure 9.** Effective group size throughout RL training. With n = 16 parallel rollouts per problem, the effective group size remains close to the maximum for most of training, indicating that nearly all problems receive both successful and unsuccessful rollouts — ensuring a stable training signal.\n\nWe set the maximum response length to 50,000 tokens for RL training, since 95% of responses from the base model terminate within this limit. As training progresses, however, we observe a noticeable increase in output length, consistent with observations from DeepSeek-R1 ([DeepSeek-AI et al., 2025](#bib-deepseekai2025deepseekr1)) and others. A representative learning curve and corresponding evaluation scores are shown in Figure 10\\. We observe a noticeable increase in both the training and evaluation scores (on both IMO-ProofBench and ProofBench).\n\n**Figure 10.** RL training curves with rubric-based rewards and corresponding evaluation metrics on IMO-ProofBench and ProofBench. Observe that as training proceeds, training rewards rise steadily and mean output length increases. Note that this is the mean output length on the training prompt set, which also includes some simpler problems on which the model is not able to exhaust the full token budget.\n\n## [RL for Continual Improvement at Test Time via Reasoning Cache](#rl-for-continual-improvement-at-test-time-via-reasoning-cache)\n\nHaving established that RL improves both training reward and test-time performance under the grader, the natural next step is to scale these gains further. For a small 4B model, increasing test-time computation provides a direct mechanism for extracting additional performance. A naive approach would increase the maximum response length during RL training, but this introduces substantial infrastructure costs and exacerbates variance in long-horizon optimization.\n\nInstead of training on extremely long monolithic responses, we introduce additional structure into the generation process. In particular, we adopt an iterative decoding procedure during training in which the model produces short reasoning segments that can be optimized with standard RL, while still encouraging improvements in long-horizon performance. We implement this idea using the Reasoning Cache (RC) framework ([Wu et al., 2026](#bib-wu2026reasoningcache)). RC decomposes reasoning into multi-step refinement cycles. At each iteration, the model generates a partial reasoning trace, summarizes its progress into a compact short textual “state representation”, and conditions the next rollout on both the original problem and this summary (Figure 11). Each subsequent summarization step updates the previous summary with any information added in the current reasoning step. Then, we train the model with RL to improve its summary-conditioned generation capabilities. This structure allows the model to effectively explore reasoning horizons equivalent to hundreds of thousands of tokens while maintaining smaller training rollout lengths.\n\n**Figure 11.** Illustration of the [RC algorithm](https://huggingface.co/papers/2602.03773). RC decoding replaces standard autoregressive decoding at both train and test time. During RC decoding, the LLM generates a reasoning trace, summarizes it, discards the original trace, and conditions subsequent reasoning on this summary. This design decouples the effective reasoning horizon from the length of any single reasoning trace, thus maintaining tractable rollout lengths for outcome-reward RL while also enabling continual improvement at test time.\n\nWe apply RL updates across these RC states, training the model to improve conditioned on the summary. Empirically, RC improves training stability, convergence speed, and performance compared to standard RL (Figure 12).\n\n**Figure 12.** Training curves comparing RL and RL with RC. Both runs use rubric-based rewards. RC achieves faster convergence and higher final reward, while rollout lengths grow more moderately under RC due to the iterative summarize-and-refine structure.\n\nIt also reduces the per decoding-turn response length, although this can easily be compensated for by running for more turns. Each subsequent turn improves over the average reward attained by the previous turn (Figure 13).\n\n**Figure 13.** Per-turn mean reward during RC training. Each panel shows the reward for a successive reasoning-cache turn. The model improves with each additional turn, confirming that RC training teaches the model to refine its reasoning conditioned on prior summaries.\n\nWhile we use the same model for both reasoning and summarization at test time, during training, we choose to avoid using a thinking model for summarization to speed up the training process. Instead, we use a frozen snapshot of the Qwen3-4B-Instruct-2507 model for summarization. That said, we observe that these sorts of gains with RC persist even when the same model performs both reasoning and summarization, suggesting that the primary benefit arises from extending the effective reasoning horizon rather than from any new information or prompt tuning.\n\nUpon evaluation, we find that both the RL-trained and RC-trained models achieve similar performance within a single decoding turn. However, the RC-trained checkpoint improves substantially more when run with the RC scaffold (see Figure 13 below). In particular, the RC-trained model outperforms the RL-trained model at every turn, with the largest gap appearing within the first three turns, which matches the number of turns used during training. We also evaluate both RL- and RC-trained models using a different agentic scaffold, namely the DeepSeek-Math-V2 scaffold discussed later, and again observe larger gains for the RC-trained model. These results suggest that RC-style training better prepares the model to benefit from test-time scaffolds, and therefore we adopt RC training in our final recipe. An example of the scaffold is shown in Figure 14.\n\n**Figure 13.** Average grade (normalized to 0–100%) on IMO-ProofBench as a function of reasoning-cache turns. Observe that applying the RC scaffold at test time on top of the RC-trained model attains higher performance than applying the RC scaffold on top of the RL-trained model. The gains are largest at turn 3 of the RC decoding process, which represents the number of turns also used for RC training.\n\nReasoning Cache Example\n\nUser\n\n**Figure 14.** A multi-step reasoning dialogue showing chain-of-thought and summarization.\n\n## [Initialization via Supervised Fine-Tuning](#initialization-via-supervised-fine-tuning)\n\nDespite the promising results from RL on top of the 4B base model, we found that building coverage over certain proof-writing strategies with an initial supervised fine-tuning stage provides a better initialization for the RL run. Therefore, in parallel, we iterated on SFT for the base model. Our SFT recipe fine-tunes the base model on problems paired with proof solutions generated by [deepseek-ai/DeepSeek-Math-V2](https://huggingface.co/deepseek-ai/DeepSeek-Math-V2), a 685B model fine-tuned specifically for Olympiad math (with a complex training procedure that involves meta-verifiers). We distill this teacher’s reasoning traces into a compact dataset of ≈\\\\approx7.5k sampled responses suitable for fine-tuning our 4B base model. We describe this in detail below.\n\n**SFT dataset generation using DeepSeek-Math-V2.** We generate solutions for problems in our curated dataset using a 128k-token context limit. Given the large size of the teacher (685B parameters), simply running inference on the teacher was challenging, and we had to orchestrate inference across 8 parallel instances (with **SGLang**), each distributed over two 8xH100 nodes (with TP=8, EP=8, PP=2). A central router load-balanced all inference requests, achieving a throughput of ≈\\\\approx3000 tokens/s. We first filter raw generations to retain only structurally valid completions containing closed reasoning blocks and explicit proof sections. We grade the solutions with Gemini 3 Pro and intentionally avoid discarding low-scoring samples simply because they might still provide useful information about proof-writing. This process yields a dataset of **7.5k proof-style responses** across 4,300 distinct problems spanning Algebra, Calculus, Combinatorics, Geometry, Inequalities, Logic and Puzzles, and Number Theory.\n\nWe fine-tuned our base 4B model on this dataset using a global batch size of 32 for five epochs. We applied a cosine learning rate schedule with a 10% warmup and a peak value of 3×10−53 \\\\times 10^{-5}, which provided stable convergence while reducing validation SFT-loss on a hold-out set.\n\n**Data ablation: quantity vs. uniqueness.** We performed several ablations with different data mixtures; we highlight the comparison between training on the full corpus of 7,500 prompt-completion pairs versus a strictly filtered set of 4,300 correct solutions, where one solution is associated with a unique problem. We find that training on only the unique problems achieves a higher final performance on IMO-ProofBench. The checkpoint at step 372 of this run was therefore used as an initialization for RL training (Figure 15).\n\n**Figure 15.** As illustrated in the figure above, using the unique dataset achieved higher performance on IMO-ProofBench. We use this initialization for our RL runs.\n\n**Challenges and limitations of SFT.** SFT serves as a strong bootstrap and produces a clear improvement over the base 4B model. However, the process also introduced significant drawbacks, most notably **length explosion**. Although the training data caps sequences at 45k tokens, the fine-tuned model frequently generates outputs that grow to hundreds of thousands of tokens, and are typically much longer for incorrect proof attempts. Rather than producing structured long-form reasoning, the model often imitates the surface appearance of extended proofs, repeating or meandering until the context window is exhausted. This behavior is a natural consequence of offline training on data that comes from a bigger model (or data that is generally “hard to fit”) and indicates the need for a more “experiential” learning paradigm instead.\n\nRL provides a natural mechanism for experiential learning, and in practice, we observe that response lengths decrease following RL training. This trend also holds when training with RC. However, the early phase of RL is heavily confounded by rollout truncation and a high overflow rate (often around 60% on average), which impairs credit assignment and reduces the effectiveness of RL and RC when initialized from SFT. An immediate direction for future work is to address this length overflow issue more directly. One possibility is to replace SFT with on-policy distillation, though this is computationally expensive due to the inference costs of the 685B-parameter DeepSeek-Math-V2 model. A more practical alternative is to approximate it by blending in on-policy traces during SFT. A complementary approach is to introduce a curriculum during RL: first training on problems that do not suffer from severe overflows, thereby enabling the model to realize the benefits of RL before scaling to longer-horizon settings.\n\nOur final recipe consists of an initial SFT step to imbue the model with the ability to write high-quality proofs. Then, we perform rubric-based RL training with the reasoning cache approach to make the model capable of effectively thinking longer when used with test-time scaffolds. Finally, we deploy the trained model with a test-time scaffold.\n\n## [Further Scaling Test-Time Compute with QED-Nano](#further-scaling-test-time-compute-with-qed-nano)\n\nWe next explore whether scaffolds that combine parallel and sequential generation can further scale token usage and thereby performance. Since our RL training procedure optimizes summary-conditioned generation without enforcing a specific summary format, we expect that the trained model would benefit from a broad class of scaffolds, including those that condition subsequent generations on detailed verification logs of the previous generations. Indeed, the trained model does choose to verify information present in the summary. We therefore evaluate several alternative test-time scaffolds to further scale the performance of QED-Nano.\n\nIn particular, we explored the following scaffolds:\n\n- **Reasoning-Cache (RC)** ([Wu et al., 2026](#bib-wu2026reasoningcache)): The decoding algorithm that we used for RL post-training, which iteratively summarizes the current attempt and conditions subsequent response generation on it. This approach does not utilize parallel sampling directly and we only run it for 3 turns in this result.\n- **Self-Check** ([Huang & Hui, 2025](#bib-huang2025winninggold)): A simple generate-verify-improve loop that ends when the verifier cannot find any flaws in the solution anymore.\n- [**Nomos**](https://github.com/NousResearch/nomos): This scaffold first generates nn solutions and verifies each of them once. It then filters the solutions to only keep the kk best ones, after which it is run through a single “consolidation” stage, where the model is presented with all kk solutions and asked which group of solutions is most likely to be correct. Among that group, the agent runs a simple knockout tournament with an LLM-judge to select the best one.\n- [**RSA**](https://rsa-llm.github.io/static/pdfs/Recursive%5FSelf%5FAggregation.pdf): This scaffold first generates nn solutions. In each subsequent stage, it then generates nn new solutions by randomly conditioning each new solution on kk existing ones. After several iterations, a random proof is selected. We make one minor improvement over the original design: instead of selecting an arbitrary proof, we run a knockout tournament with an LLM-judge on the solutions from the last stage.\n- **DeepSeek Math** ([Shao et al., 2025](#bib-shao2025deepseekmathv2)) (DSM): This scaffold first generates nn solutions, each of which is self-evaluated nn times. Solutions are sorted by their average self-evaluated score, and the top nn solutions are improved by presenting the model with the solution and some of the feedback generated by the self-evaluation stage. These new solutions are added to the solution pool. This process is iterated several times before the solution with the highest overall score across all iterations is returned.\n\nIn preliminary experiments on RL-trained checkpoints initialized from the base model, we evaluated several test-time scaffolds. DSM, RSA, and RC consistently yielded the strongest improvements, with DSM outperforming RSA in several settings, while other scaffolds provided more modest gains. Among these, RC does not run parallel sampling at each turn, making it convenient for training but suboptimal for maximizing test-time performance, whereas DSM can effectively scale parallel compute as well. We therefore adopted DSM for our main experiments. This choice was further motivated by the fact that our SFT initialization leveraged traces from DeepSeek-Math-V2, which is explicitly trained for self-verification. Consequently, we hypothesized that a scaffold that explicitly incorporates self-verification would be particularly effective for our final trained QED-Nano model. Our main results confirm this hypothesis.\n\n**Quantitative evaluation.**Figure 16 summarizes the performance of different scaffolds applied to our final QED-Nano model (note that the preliminary experiments above were done on the RL-trained checkpoint from the base model). RC yields the smallest performance gain within 3 turns (which is perhaps expected), but it also requires only about twice the number of tokens. In contrast, RSA and DSM provide substantial improvements of 17% and 14%, respectively. However, they are substantially more expensive: RSA costs about 20 times as much as the base model, and DSM about 16 times as much. Overall, DSM provides a reasonable tradeoff for performance within a given token budget and we therefore utilize it. However, one could also choose to use RSA given these results.\n\nAgent Scaffold Comparison: Average Grade vs Average Tokens\n\n**Figure 16.** Comparison of different agent scaffolds applied on the QED-Nano model and evaluated on IMO problems, showing the trade-off between token usage and performance.\n\n## [Qualitative Analysis of Solutions from QED-Nano](#qualitative-analysis-of-solutions-from-qed-nano)\n\nWe manually examined a subset of QED-Nano’s generated proofs to assess two additional dimensions beyond benchmark scores: **(1)** whether the model attempts to reward-hack the LLM-based grader, and **(2)** the intrinsic quality of the proofs themselves. An experienced human evaluator from our team, with a substantial background in evaluating LLM-generated mathematical proofs, reviewed a sample of proofs and compared their judgments with those of the automated grader. Detailed annotations are provided in the accompanying figure; below, we summarize the main observations.\n\n**Agreement between LLM judge and human judgment.** We found no clear evidence of reward hacking on our Gemini 3 Pro grader. The human evaluator agreed with the automated grader on most problems, though the LLM grader was occasionally a bit more generous. Only one problem showed a significant change in score: the QED-Nano agent’s solution to IMO 2025 Q2\\. Our human grader judged the heavy computational approach incorrect, noting an algebraic error and too many gaps for a fully rigorous proof. We do not attribute this discrepancy to deliberate reward hacking, as other models generally attempted similar approaches to Q2.\n\n**Proof quality.** The generated proofs are generally well structured and logically organized, making them easy to read and follow. The most prominent weakness is a consistent preference for computation-heavy approaches. In geometry problems in particular, the solutions always rely on coordinate or algebraic arguments (“bashing”) rather than synthetic arguments. In other domains, the model also frequently provides proofs that are more computational than the human-written ground-truth solution. However, this tendency is not unique to QED-Nano as it reflects a broader pattern across LLMs (based on prior experience from evaluating various frontier models on MathArena evaluations).\n\n**Agentic vs. base model.** We compared proofs generated by the base model with those produced with the DSM agent. In most cases, the outputs are stylistically and structurally similar. The primary benefit of the agentic setup appears in cases where the base model’s solution contains a clear but nontrivial mistake. There, the agent often produces a correct proof that addresses that particular mistake.\n\n**IMO 2025.** We explicitly evaluated our model on the 2025 IMO problems. According to the Gemini 3 Pro judge, QED-Nano achieves a score of 22/42 when used with the test-time scaffold. The standalone QED-Nano model achieves 12/42 under the same judge. We observed, however, that the trained model does make fairly simple algebraic or transcription errors, such as mismanipulating terms or incorrectly copying previously derived expressions. The Gemini 3 Pro judge is relatively lenient toward these mistakes, and is also too likely to give intermediate points for trivial steps. According to our human grader, QED-Nano attains 14/42 with the scaffold and 7/42 without it.\n\nAlthough some of these errors would very likely be mitigated with additional training or larger models that tend to follow the semantics of language or by employing a stricter grading protocol during training, the results also make clear that the model struggles substantially on some of the most challenging problems, where it is unable to make meaningful progress.\n\n**Comparison with other models.** We also compared QED-Nano’s proofs with those generated by the Qwen-3-4B-Thinking-2507 base model and Nomos-1\\. The base model’s outputs are of very poor proof quality. It is clearly trained to obtain the correct final answer while disregarding mathematical rigor. While it occasionally finds the correct answer to a problem, its proofs often contain obvious mistakes, such as circular arguments and simplifying assumptions. Nevertheless, our recipe is able to improve this base model to produce proofs of much higher quality.\n\nNomos-1 has a significantly different style compared to QED-Nano. Its proofs are more concise and direct, more similar to expert human-written solutions. For instance, it does not repeat the problem statement and instead immediately provides the answer, often proves lemmas as separate statements, frequently skips computational steps, and writes in a very brief, matter-of-fact statements. While expert-written solutions can generally be trusted to be this compressed, the general sycophancy found in LLMs and their tendency to hide mistakes in proofs, makes this behavior more problematic. Determining which writing style is better is quite subjective, but we encourage you to read some of the proofs below to decide for yourself!\n\n**Vibe checks.** We evaluated the model with a small set of non-mathematical prompts to confirm it retained broader capabilities. Remarkably, it still follows instructions effectively, with no noticeable regressions in non-mathematical reasoning tasks.\n\nProofs and Judgments\n\nProblem\n\nModel Summary\n\n**Model:** QED-Nano\n\n**Grade:** 7/7\n\n**Dataset:** lm-provers/imoproofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-rc\\_v0900-step-000150-google\n\n**Split:** 20260203\\_133642\n\nComment\n\nProof\n\nModel Summary\n\n**Model:** QED-Nano (Agent)\n\n**Grade:** 7/7\n\n**Dataset:** lm-provers/imoproofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-agent-deepseek\\_math-rc\\_v0900-step-000150-google\n\n**Split:** 20260211\\_144929\n\nComment\n\nProof\n\nModel Summary\n\n**Model:** Qwen3-4B-Think\n\n**Grade:** 1/7\n\n**Dataset:** lm-provers/imoproofbench-outputs\n\n**Config:** qwen-qwen3-4b-thinking-2507-google\n\n**Split:** 20260130\\_100355\n\nComment\n\nProof\n\nModel Summary\n\n**Model:** Nomos-1\n\n**Grade:** 7/7\n\n**Dataset:** lm-provers/imoproofbench-outputs\n\n**Config:** nousresearch-nomos-1-google\n\n**Split:** 20260206\\_130109 (requested the oldest)\n\nComment\n\nProof\n\nProblem\n\nModel Summary\n\n**Model:** QED-Nano\n\n**Grade:** 1/7\n\n**Dataset:** lm-provers/imoproofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-rc\\_v0900-step-000150-google\n\n**Split:** 20260203\\_133642\n\nComment\n\nProof\n\nModel Summary\n\n**Model:** QED-Nano (Agent)\n\n**Grade:** 6/7\n\n**Dataset:** lm-provers/imoproofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-agent-deepseek\\_math-rc\\_v0900-step-000150-google\n\n**Split:** 20260211\\_144929\n\nComment\n\nProof\n\nModel Summary\n\n**Model:** Qwen3-4B-Think\n\n**Grade:** 1/7\n\n**Dataset:** lm-provers/imoproofbench-outputs\n\n**Config:** qwen-qwen3-4b-thinking-2507-google\n\n**Split:** 20260130\\_100355\n\nComment\n\nProof\n\nModel Summary\n\n**Model:** Nomos-1\n\n**Grade:** 7/7\n\n**Dataset:** lm-provers/imoproofbench-outputs\n\n**Config:** nousresearch-nomos-1-google\n\n**Split:** 20260206\\_130109 (requested the oldest)\n\nComment\n\nProof\n\nProblem\n\nModel Summary\n\n**Model:** QED-Nano\n\n**Grade:** 4/7\n\n**Dataset:** lm-provers/imoproofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-rc\\_v0900-step-000150-google\n\n**Split:** 20260203\\_133642\n\nComment\n\nProof\n\nModel Summary\n\n**Model:** QED-Nano (Agent)\n\n**Grade:** 7/7\n\n**Dataset:** lm-provers/imoproofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-agent-deepseek\\_math-rc\\_v0900-step-000150-google\n\n**Split:** 20260211\\_144929\n\nComment\n\nProof\n\nModel Summary\n\n**Model:** Qwen3-4B-Think\n\n**Grade:** 1/7\n\n**Dataset:** lm-provers/imoproofbench-outputs\n\n**Config:** qwen-qwen3-4b-thinking-2507-google\n\n**Split:** 20260130\\_100355\n\nComment\n\nProof\n\nModel Summary\n\n**Model:** Nomos-1\n\n**Grade:** 0/7\n\n**Dataset:** lm-provers/imoproofbench-outputs\n\n**Config:** nousresearch-nomos-1-google\n\n**Split:** 20260206\\_130109 (requested the oldest)\n\nComment\n\nProof\n\nProblem\n\nModel Summary\n\n**Model:** QED-Nano\n\n**Grade:** 0/7\n\n**Dataset:** lm-provers/imoproofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-rc\\_v0900-step-000150-google\n\n**Split:** 20260203\\_133642\n\nComment\n\nProof\n\nModel Summary\n\n**Model:** QED-Nano (Agent)\n\n**Grade:** 7/7\n\n**Dataset:** lm-provers/imoproofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-agent-deepseek\\_math-rc\\_v0900-step-000150-google\n\n**Split:** 20260211\\_144929\n\nComment\n\nProof\n\nModel Summary\n\n**Model:** Qwen3-4B-Think\n\n**Grade:** 7/7\n\n**Dataset:** lm-provers/imoproofbench-outputs\n\n**Config:** qwen-qwen3-4b-thinking-2507-google\n\n**Split:** 20260130\\_100355\n\nComment\n\nProof\n\nModel Summary\n\n**Model:** Nomos-1\n\n**Grade:** 7/7\n\n**Dataset:** lm-provers/imoproofbench-outputs\n\n**Config:** nousresearch-nomos-1-google\n\n**Split:** 20260206\\_130109 (requested the oldest)\n\nComment\n\nProof\n\nProblem\n\nModel Summary\n\n**Model:** QED-Nano\n\n**Grade:** 0/7\n\n**Dataset:** lm-provers/imoproofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-rc\\_v0900-step-000150-google\n\n**Split:** 20260203\\_133642\n\nComment\n\nProof\n\nModel Summary\n\n**Model:** QED-Nano (Agent)\n\n**Grade:** 7/7\n\n**Dataset:** lm-provers/imoproofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-agent-deepseek\\_math-rc\\_v0900-step-000150-google\n\n**Split:** 20260211\\_144929\n\nComment\n\nProof\n\nModel Summary\n\n**Model:** Qwen3-4B-Think\n\n**Grade:** 0/7\n\n**Dataset:** lm-provers/imoproofbench-outputs\n\n**Config:** qwen-qwen3-4b-thinking-2507-google\n\n**Split:** 20260130\\_100355\n\nComment\n\nProof\n\nModel Summary\n\n**Model:** Nomos-1\n\n**Grade:** 0/7\n\n**Dataset:** lm-provers/imoproofbench-outputs\n\n**Config:** nousresearch-nomos-1-google\n\n**Split:** 20260206\\_130109 (requested the oldest)\n\nComment\n\nProof\n\nProblem\n\nModel Summary\n\n**Model:** QED-Nano\n\n**Grade:** 6/7\n\n**Dataset:** lm-provers/imoproofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-rc\\_v0900-step-000150-google\n\n**Split:** 20260203\\_133642\n\nComment\n\nProof\n\nModel Summary\n\n**Model:** QED-Nano (Agent)\n\n**Grade:** 0/7\n\n**Dataset:** lm-provers/imoproofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-agent-deepseek\\_math-rc\\_v0900-step-000150-google\n\n**Split:** 20260211\\_144929\n\nComment\n\nProof\n\nProblem\n\nModel Summary\n\n**Model:** QED-Nano\n\n**Grade:** 7/7\n\n**Dataset:** lm-provers/imoproofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-rc\\_v0900-step-000150-google\n\n**Split:** 20260203\\_133642\n\nComment\n\nProof\n\nModel Summary\n\n**Model:** QED-Nano (Agent)\n\n**Grade:** 7/7\n\n**Dataset:** lm-provers/imoproofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-agent-deepseek\\_math-rc\\_v0900-step-000150-google\n\n**Split:** 20260211\\_144929\n\nComment\n\nProof\n\nProblem\n\nModel Summary\n\n**Model:** QED-Nano\n\n**Grade:** 0/7\n\n**Dataset:** lm-provers/imoproofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-rc\\_v0900-step-000150-google\n\n**Split:** 20260203\\_133642\n\nComment\n\nProof\n\nModel Summary\n\n**Model:** QED-Nano (Agent)\n\n**Grade:** 0/7\n\n**Dataset:** lm-provers/imoproofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-agent-deepseek\\_math-rc\\_v0900-step-000150-google\n\n**Split:** 20260211\\_144929\n\nComment\n\nProof\n\nProblem\n\nModel Summary\n\n**Model:** QED-Nano\n\n**Grade:** 0/7\n\n**Dataset:** lm-provers/proofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-rc\\_v0900-step-000150-google\n\n**Split:** 20260214\\_101743\n\nComment\n\nProof\n\nModel Summary\n\n**Model:** QED-Nano (Agent)\n\n**Grade:** 0/7\n\n**Dataset:** lm-provers/proofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-agent-deepseek\\_math-rc\\_v0900-step-000150-google\n\n**Split:** 20260212\\_230045\n\nComment\n\nProof\n\nProblem\n\nModel Summary\n\n**Model:** QED-Nano\n\n**Grade:** 6/7\n\n**Dataset:** lm-provers/proofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-rc\\_v0900-step-000150-google\n\n**Split:** 20260214\\_101743\n\nComment\n\nProof\n\nModel Summary\n\n**Model:** QED-Nano (Agent)\n\n**Grade:** 0/7\n\n**Dataset:** lm-provers/proofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-agent-deepseek\\_math-rc\\_v0900-step-000150-google\n\n**Split:** 20260212\\_230045\n\nComment\n\nProof\n\nProblem\n\nModel Summary\n\n**Model:** QED-Nano\n\n**Grade:** 1/7\n\n**Dataset:** lm-provers/proofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-rc\\_v0900-step-000150-google\n\n**Split:** 20260214\\_101743\n\nComment\n\nProof\n\nModel Summary\n\n**Model:** QED-Nano (Agent)\n\n**Grade:** 2/7\n\n**Dataset:** lm-provers/proofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-agent-deepseek\\_math-rc\\_v0900-step-000150-google\n\n**Split:** 20260212\\_230045\n\nComment\n\nProof\n\nProblem\n\nModel Summary\n\n**Model:** QED-Nano\n\n**Grade:** 0/7\n\n**Dataset:** lm-provers/proofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-rc\\_v0900-step-000150-google\n\n**Split:** 20260214\\_101743\n\nComment\n\nProof\n\nModel Summary\n\n**Model:** QED-Nano (Agent)\n\n**Grade:** 5/7\n\n**Dataset:** lm-provers/proofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-agent-deepseek\\_math-rc\\_v0900-step-000150-google\n\n**Split:** 20260212\\_230045\n\nComment\n\nProof\n\nProblem\n\nModel Summary\n\n**Model:** QED-Nano\n\n**Grade:** 0/7\n\n**Dataset:** lm-provers/proofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-rc\\_v0900-step-000150-google\n\n**Split:** 20260214\\_101743\n\nComment\n\nProof\n\nModel Summary\n\n**Model:** QED-Nano (Agent)\n\n**Grade:** 7/7\n\n**Dataset:** lm-provers/proofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-agent-deepseek\\_math-rc\\_v0900-step-000150-google\n\n**Split:** 20260212\\_230045\n\nComment\n\nProof\n\nProblem\n\nModel Summary\n\n**Model:** QED-Nano\n\n**Grade:** 0/7\n\n**Dataset:** lm-provers/proofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-rc\\_v0900-step-000150-google\n\n**Split:** 20260214\\_101743\n\nComment\n\nProof\n\nModel Summary\n\n**Model:** QED-Nano (Agent)\n\n**Grade:** 0/7\n\n**Dataset:** lm-provers/proofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-agent-deepseek\\_math-rc\\_v0900-step-000150-google\n\n**Split:** 20260212\\_230045\n\nComment\n\nProof\n\n**Figure 17.** Per-problem model proofs with reviewer comments and grades.\n\n## [Discussion and Conclusion](#discussion-and-conclusion)\n\nIn this blog, we introduce QED-Nano to demonstrate that Olympiad-level problem-solving is not reserved for frontier-scale models with 100B+ parameters. With a post-training recipe that first instills high-quality proof-writing strategies via SFT and then applies RL to explicitly optimize long-horizon improvement through test-time scaling, a 4B model can produce substantially stronger proofs than its base initialization and compete with much larger open models when paired with additional test-time compute. On IMO-ProofBench, our open-source QED-Nano (Agent) closes much of the gap to Gemini 3 Pro while being at least **3x** cheaper to run and requiring significantly lower training costs. This highlights a practical path toward strong reasoning through specialization and test-time adaptation rather than scaling only parameter count. Averaged across benchmarks, QED-Nano (Agent) also significantly outperforms larger open models such as Nomos-1 and GPT-OSS-120B.\n\nWe also outline our end-to-end workflow to help others train small models for effective long-form reasoning. More broadly, our recipe is simple and applicable to other domains where outcomes are difficult to verify directly but structured rubrics can be constructed. We did not observe evidence of reward hacking under rubric-based rewards, further supporting the robustness of this approach. To encourage follow-up work, we release our models, datasets, and code implementations.\n\n**Going forward**, several avenues could further improve QED-Nano. The most immediate action items include improving the synergy between SFT and RL. In particular, mitigating the length explosion introduced by SFT early on would likely amplify the gains from subsequent RL by speeding up the process of credit assignment. A second short-term direction is to refine the grader design, for example, by gradually tightening rubric penalties as training progresses or by using strict reward designs, thereby incentivizing increasingly rigorous and polished proofs as the model learns to make progress. Finally, incorporating hints or guidance during training, such as conditioning on plans from oracle solutions or the grading scheme, will help the model tackle harder problems during training and enable further scaling of RL to achieve stronger results.\n\nBeyond these goals, more fundamental questions remain. One direction is developing approaches that imbue the LLM with the ability to synthesize genuinely novel ideas or “aha” insights when solving the hardest problems. Like most LLMs, QED-Nano tends to rely on computation-heavy approaches rather than identifying elegant structural insights early on. This reflects the style of reasoning RL optimizes models for. Designing scalable training paradigms that encourage broader exploration of reasoning strategies, rather than refinement of a single computational path, is therefore an important challenge. From a workflow perspective, another key direction is to develop methods that more directly optimize for the specific test-time scaffolds used at deployment, tightening the alignment between train-time objectives and inference-time behavior. We encourage the community to study these aspects.\n\n## [Author Contributions](#author-contributions)\n\nThis is a team effort with members from CMU, Hugging Face, ETH Zurich, and Numina.\n\nOur team members (in alphabetical order) are as follows:\n\n- CMU: Aviral Kumar, Yuxiao Qu, Amrith Setlur, Ian Wu\n- Hugging Face: Edward Beeching, Lewis Tunstall\n- ETH Zurich: Jasper Dekoninck\n- Project Numina: Jia Li\n\nAll members contributed to the project substantially. Specifically:\n\n- Yuxiao Qu developed the initial version of the grader and verifier-based RL approach, built the grading-scheme pipeline, curated and processed the proof datasets, and implemented the initial reasoning-cache and multi-turn training loops that started us in this direction. With Amrith Setlur and Lewis Tunstall, he ran a number of ablations that informed the final RL runs.\n- Amrith Setlur adapted the PipelineRL infrastructure for the verifier-based RL approach, optimized RL configurations for stability and scale, implemented the asynchronous and streaming reasoning-cache RL training infrastructure, and proposed several ablations and algorithmic strategies for the training runs. With Yuxiao Qu and Lewis Tunstall, he ran a number of ablations that informed the final RL runs.\n- Ian Wu, as primary author of the Reasoning Cache method, provided core technical guidance on RC experimentation, evaluation, and training pipelines, which shaped how it was utilized throughout the project.\n- Edward Beeching led several evaluations, developed the synthetic data generation pipeline for DeepSeek-Math-V2, and ran the SFT ablations to analyse model length-control behavior and training dynamics.\n- Lewis Tunstall led the large-scale RL infrastructure efforts, benchmarking and stabilizing multiple RL frameworks at the start of the project, optimized inference throughput, and ran the largest training and evaluation experiments. With Amrith Setlur and Yuxiao Qu, he ran a number of ablations that informed the final RL runs. He advised several other aspects of the project.\n- Jasper Dekoninck led the benchmark design and ensured rigorous evaluations, benchmarked several test-time agent scaffolds and developed our final scaffold, built the IMO-ProofBench and ProofBench splits, filtered training datasets and created the grading schemes for them, designed the benchmarks for the RL grader, and led extensive model-based and human evaluations to ensure robustness and correlation with human proof quality.\n- Jia Li curated and expanded high-quality AoPS and Olympiad datasets, developed grading-scheme generation workflows, and explored scalable problem synthesis and verification strategies in the project initially.\n- Aviral Kumar advised the overall project and contributed to the ideas behind long-horizon training, curriculum design, reward formulation and some ideas on data construction.\n\n## [Acknowledgements](#acknowledgements)\n\nWe thank Leandro von Werra, Andres Marafioti, Thibaud Frere, Graham Neubig, Sewon Min, Wenjie Ma, and Katerina Fragkiadaki for helpful discussions and feedback. AS, YQ, IW, and AK thank the FLAME center at CMU, the DeltaAI cluster, and the NAIRR program for providing GPU resources that supported a part of the experimental iteration. We thank Google Cloud for Gemini 3 Pro API credits. AS and AK thank the Laude Institute Slingshots program for support and feedback, and Braden Hancock and Andy Konwinski at the Laude Institute for discussions and feedback. EB and LT thank Hugo Larcher and Mathieu Morlon for keeping the GPUs running hot on the Hugging Face cluster 🔥. JD used compute from the Swiss AI Initiative supported by a grant from the Swiss National Supercomputing Centre (CSCS) under project ID a155 on Alps.\n\n## [Bibliography](#bibliography)\n\n- DeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., Zhang, X., Yu, X., Wu, Y., Wu, Z. F., Gou, Z., Shao, Z., Li, Z., Gao, Z., … others. (2025). *DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning*. <https://huggingface.co/papers/2501.12948>[](#refctx-bib-deepseekai2025deepseekr1-1)\n- Huang, S., & Hui, B. (2025). *Winning Gold Medal in IMO with LLMs*. <https://huggingface.co/papers/2507.15855>[](#refctx-bib-huang2025winninggold-1)\n- Ma, W., Wu, X., Cheng, X., Tunstall, L., Nambi, A., Niu, J., Mirrokni, V., Al-Onaizan, Y., & Ma, H. (2025). *Reliable Benchmarking for LLM-Based Mathematical Proof Generation*. <https://huggingface.co/papers/2510.13888> back: [1](#refctx-bib-ma2025reliable-1), [2](#refctx-bib-ma2025reliable-2), [3](#refctx-bib-ma2025reliable-3)\n- Piche, A., Haas, C., Chatterjee, P., Nyiri, M., Sarmento, J., Spero, M., Giles, C. L., Wang, X. E., Prasad, A. S., Prasad, N., Caragea, C., Stoyanov, V., Schwartz, R., Xiong, C., & Radev, D. R. (2025). *PipelineRL: Scaling Off-Policy Reinforcement Learning for LLMs with Asynchronous Rollouts and Decoupled Training Pipelines*. <https://huggingface.co/papers/2509.19128>[](#refctx-bib-piche2025pipelinerl-1)\n- Shao, Z., Li, Y. K., Xu, A., Zhang, M., Li, Z., Wang, Y., Wang, Y., Song, J., Zhu, Q., & others. (2025). *DeepSeekMath-V2: A 671B Open-Source Tool-Integrated Reasoning Model with 128K Context Length and Better Mathematical Reasoning Through Reinforcement Learning*. <https://huggingface.co/papers/2511.22570>[](#refctx-bib-shao2025deepseekmathv2-1)\n- Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Zhang, M., Li, Y. K., & Guo, D. (2024). *DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models*. <https://huggingface.co/papers/2402.03300> back: [1](#refctx-bib-shao2024deepseekmath-1), [2](#refctx-bib-shao2024deepseekmath-2)\n- Wu, I., Tunstall, L., Dekoninck, J., & Kumar, A. (2026). *Reasoning Cache: Continual Exploration and Exploitation for Test-Time Compute Scaling of Reasoning Models*. <https://huggingface.co/papers/2602.03773> back: [1](#refctx-bib-wu2026reasoningcache-1), [2](#refctx-bib-wu2026reasoningcache-2), [3](#refctx-bib-wu2026reasoningcache-3), [4](#refctx-bib-wu2026reasoningcache-4)\n",
        "pipelineQuality": 1,
        "pipelinePassed": true,
        "pipelineReasons": [],
        "words": 8306,
        "links": 18,
        "headings": 15
      },
      {
        "engine": "local_heuristic",
        "ok": true,
        "title": "QED-Nano: Teaching a Tiny Model to Prove Hard Theorems",
        "markdown": "# QED-Nano: Teaching a Tiny Model to Prove Hard Theorems\n\nTable of Contents\n\nCan we train small language models to solve hard Olympiad-level proof problems at a level close to large frontier models such as Gemini 3 Pro? **Yes!** We introduce [QED-Nano](https://huggingface.co/lm-provers/QED-Nano), a compact 4B model post-trained to write Olympiad-level mathematical proofs. Our recipe has three stages: (1) supervised fine-tuning via distillation from DeepSeek-Math-V2, (2) reinforcement learning with dense, rubric-based rewards, and (3) training with a reasoning cache ([Wu et al., 2026](#bib-wu2026reasoningcache)), which decomposes long proofs into iterative summarize-and-refine cycles so the model is capable of continual improvement at test time. Upon deployment, we pair QED-Nano with agentic scaffolds that scale test-time compute to more than 1.5M tokens per problem, combining horizon extension with self-verification. Despite its small size, QED-Nano approaches the proof-writing performance of much larger open and proprietary models at a fraction of the inference cost. We release all models, datasets, grading rubrics, and training code. Concretely, we release:\n\n- The [QED-Nano](https://huggingface.co/lm-provers/QED-Nano) and [QED-Nano-SFT](https://huggingface.co/lm-provers/QED-Nano-SFT) models.\n- The [FineProofs-SFT](https://huggingface.co/datasets/lm-provers/FineProofs-SFT) and [FineProofs-RL](https://huggingface.co/datasets/lm-provers/FineProofs-RL) datasets for post-training our models.\n- The [training and evaluation code](https://github.com/CMU-AIRe/QED-Nano), including the agent scaffolds.\n\nWe next describe our approach and results in more detail. Let’s dive in!\n\n## [Introducing QED-Nano: a 4B Model for Olympiad-Level Proofs](#introducing-qed-nano-a-4b-model-for-olympiad-level-proofs)\n\nRecent proprietary LLM-based systems have demonstrated gold-level performance on the 2025 International Mathematical Olympiad (IMO). However, the training pipelines behind these systems are largely undisclosed, and their reliance on very large models makes them difficult to reproduce or study. This creates a gap between what is possible in principle and what the wider community can realistically build. Our goal is to close this gap between open-source and proprietary systems by showing that small and accessible open models can be trained to attain competitive reasoning performance on these difficult math Olympiad problems.\n\nIn this post, we present an **end-to-end post-training recipe for building a 4B theorem-proving model**. Our model operates entirely in natural language, with no reliance on Lean or external tools. Our recipe is simple and has three components that resemble a typical post-training stack, but with carefully chosen design choices for improving theorem-proving capabilities that we especially tune to scale test-time compute (token budget at test-time):\n\n1. We run supervised fine-tuning (SFT) to imbue the model with a basic ability to write proofs.\n2. Then, we perform rubric-based reinforcement learning (RL) with an approach that **explicitly** optimizes for continual improvement within a long reasoning trace at test time.\n3. Finally, we construct test-time scaffolds that allow our model to fully utilize this learned capability of continual improvement in a way that maximizes performance vs tokens spent.\n\n**Table 1.** Comparison of QED-Nano (4B) with leading open- and closed-source models on IMO-ProofBench, ProofBench, and IMO-AnswerBench. Despite being just 4B in size, QED-Nano matches or exceeds larger models, outperforming Nomos-1 (30B) and Qwen3-235B-A22B-Thinking (50x bigger) on average, while remaining competitive with GPT-OSS-120B. More interestingly, when provided extra test-time compute, QED-Nano (Agent) attains better performance than GPT-OSS-120B on both of the proof-based benchmarks, and it approaches the performance of Gemini 3 Pro, a much stronger proprietary model on IMO-ProofBench.\n\n| Model                         | IMO-ProofBench | ProofBench     | IMO-AnswerBench |\n| ----------------------------- | -------------- | -------------- | --------------- |\n| Qwen3-4B-Thinking-2507        | 20.4 (2.6)     | 19.5 (0.9)     | 55.8            |\n| **QED-Nano-SFT**              | **39.5 (2.9)** | **33.3 (0.5)** | **57.5**        |\n| **QED-Nano**                  | **40.0 (0.6)** | **44.9 (3.4)** | **67.5**        |\n| **QED-Nano (Agent)**          | **54.0 (3.7)** | **54.4 (2.4)** | **\\-**          |\n| Qwen3-30B-A3B-Thinking-2507   | 27.6 (1.0)     | 26.1 (2.4)     | 67.0            |\n| Qwen3-235B-A22B-Thinking-2507 | 34.1 (0.7)     | 33.7 (1.1)     | 70.5            |\n| Nomos-1                       | 40.3 (3.5)     | 28.3 (3.9)     | 49.0            |\n| GPT-OSS-20B                   | 38.3 (1.2)     | 38.4 (3.9)     | 61.5            |\n| GPT-OSS-120B                  | 43.1 (3.2)     | 47.5 (1.7)     | 70.5            |\n| DeepSeek-Math-V2              | 57.9 (2.0)     | 60.6 (0.1)     | 75.8            |\n| Gemini 3 Pro                  | 58.7 (2.9)     | 66.7 (3.1)     | 83.2            |\n\n**Main Results.** Even when just allowed to reason without any scaffold, our trained model, QED-Nano, achieves a 40% score on IMO-ProofBench, 45% on ProofBench, and 68% on IMO-AnswerBench, far better than any other 4B model. On average, these scores make QED-Nano outperform much larger open models such as [Nomos-1](https://huggingface.co/NousResearch/nomos-1) (30B) and Qwen3-235B-A22B-Thinking. More importantly, our main result shows that when allowed to reason for up to 1.5 million tokens per problem by pairing the model with a test-time scaffold, QED-Nano (Agent) achieves 54% on IMO-ProofBench and 54% on ProofBench, attaining a strong cost-performance tradeoff on challenging Olympiad-level problems (Figure 2, Table 1). On IMO-ProofBench, this performance is very close to Gemini 3 Pro, a much stronger proprietary model.\n\n**Figure 2.** Performance of QED-Nano (4B) within just a single response turn of 50,000 tokens. Even when allowed to reason for just 50,000 tokens (without any form of test-time scaling), QED-Nano roughly matches performance of GPT-OSS-120B and outperforms Nomos-1 on average across the three benchmarks. The only models that considerably outperform QED-Nano are much larger, proprietary models.\n\nBased on estimated inference cost spent via the Hugging Face Hub’s inference providers on IMO-ProofBench, QED-Nano (Agent) costs about `$4.0`, and the comparable Gemini 3 Pro run costs `$12.3` under the same accounting. That’s **\\~3× cheaper for similar performance**.\n\n**Broader implications.** Beyond results, we illustrate a broader principle in this blog post: **even on the most challenging tasks, we can explicitly train small models to reliably and continually “adapt” at test-time to improve performance.** While we showcase our results on Olympiad-style problems (primarily proofs), the recipe we use is generalizable and can also be applied to other domains that allow for rubric-based rewards. More conceptually, in practice, scaling test-time adaptation is often more feasible with smaller models, since inference cost grows quickly with model size. We show that task-specialized small models trained for test-time adaptation can match or exceed much larger generalist systems, suggesting a path toward more capable and specialized models without relying on trillion-parameter architectures that are costly to deploy.\n\n**To support further research**, we release our SFT dataset, RL prompt set, and an optimized asynchronous and streaming off-policy RL implementation built on [pipeline-rl](https://github.com/ServiceNow/PipelineRL) that incorporates our algorithmic improvements for RL with long-horizon reasoning. Our largest RL training run, with rollout length of 50K tokens, fits within **11 nodes of 8xH100s for 4 days**, making our approach more accessible and reproducible compared to proprietary approaches. We also discuss early findings, ablations, and small-scale experiments that guided our research workflow and informed algorithmic and data curation choices, with the goal of helping practitioners apply similar ideas and workflows in their own domains.\n\nNext, we dive into the details of our post-training recipe. In particular, we explain how we source our prompts for RL training and set up an automated proof grading infrastructure. Later, we discuss the two main stages of the post-training recipe (RL, SFT). We discuss details of the SFT training data and RL algorithms that train for our test-time scaffolds in those sections.\n\n## [Setup: Training Prompts and Grading Schemes](#setup-training-prompts-and-grading-schemes)\n\nWe now discuss how we curate our prompt sets for RL training and design our rubrics, which we use for RL training and evaluation. Training models to generate rigorous Olympiad-level proofs requires carefully curated prompts that are both challenging and clean, with clear criteria for evaluating correctness and mathematical rigor. Therefore, rather than relying on large volumes of loosely curated problem-solution pairs, we construct a compact, high-quality corpus that mirrors the structure and difficulty of competition proofs. Later in this post, we discuss how we reuse this prompt set to collect a dataset for an SFT phase as well. We release all datasets and grading artifacts as standalone resources for the community.\n\n**Data source and filtering.** We begin with two public datasets: [AI-MO/aops](https://huggingface.co/datasets/AI-MO/aops), which contains problems sourced from the Art of Problem Solving forums, and [AI-MO/olympiads](https://huggingface.co/datasets/AI-MO/olympiads), which aggregates official solutions from a wide range of national and international math competitions (*e.g.*, IMO, USAMO, RMM, *etc.*). While these sources provide coverage, they contain substantial noise, incomplete reasoning, formatting artifacts, and various other issues that preclude them from being seamlessly consumed in any post-training pipeline.\n\nWe apply a multi-stage filtering procedure to improve the data quality:\n\n1. We remove problems involving diagrams or images, since our models operate purely in text.\n2. We discard trivial or ill-posed entries, including problems where the answer appears directly in the statement, solutions that are implausibly short or purely computational, and materials drawn from easier contests such as AMC or routine exercises. To further enhance solution quality, we run an additional automated filtering pass using GPT-5-Nano. In particular, we prompt it to detect frequent issues observed in the [AI-MO/aops](https://huggingface.co/datasets/AI-MO/aops) dataset, such as questionable problem statements, inconsistencies across proposed solutions, and reference proofs containing substantial logical gaps.\n3. Finally, **to avoid any contamination** with our evaluation benchmarks, we exclude from our training problem set all problems from 2025 competitions and also run a fuzzy string matching algorithm to weed out any problems similar to those in our evaluation benchmarks. The resulting dataset is a curated collection of Olympiad-style proof problems spanning geometry, number theory, algebra, and combinatorics (see Figure 3).\n\nNext, we discuss how we determine the grading schemes for each problem in this set.\n\nProblem Category Distribution\n\n**Figure 3.** Distribution of 4,281 Olympiad math problems by category. Hover over slices or legend items for detailed counts and percentages. The dataset is dominated by Number Theory (27.2%) and Geometry (23.9%) problems.\n\n**Grading schemes.** To provide accurate reward signals for training via RL, we construct detailed grading schemes for each problem. Our approach follows the grading framework introduced in ProofBench ([Ma et al., 2025](#bib-ma2025reliable)), which uses Gemini 3 Pro with a custom prompt to generate **rubrics** that score model solutions from 0 to 7\\. Each rubric specifies:\n\n1. detailed intermediate checkpoints corresponding to partial correctness\n2. common failure modes that warrant zero credit, and\n3. specific points where additional deductions are necessary.\n\nAs a result, reinforcement learning receives dense, informative feedback instead of sparse success signals, encouraging gradual improvement in long-form reasoning rather than binary outcome optimization. Several examples are shown below.\n\nExample Grading Schemes\n\nLet $c$ be fixed natural number. Sequence $(a\\_n)$ is defined by: $a\\_1=1, a\\_{n+1}=d(a\\_n)+c$ for $n=1,2,...$. where $d(m)$ is number of divisors of $m$. Prove that there exist $k$ natural such that sequence $a\\_k,a\\_{k+1},...$ is periodic.\n\n1. **Checkpoints (7pts total)**\n- **1 pt**: State or prove the inequality $d(m) \\\\leq \\\\frac{m}{2} + 1$ (or a stronger bound such as $2\\\\sqrt{m}$ for large $m$) to be used in the boundedness proof.\n- **4 pts**: Boundedness of the sequence $(a\\_n)$.\n  - **2 pts**: Combine the divisor bound with the recurrence to establish an inequality of the form $a\\_{n+1} \\\\leq \\\\frac{a\\_n}{2} + C$ (or equivalent logic showing $a\\_{n+1} < a\\_n$ for sufficiently large $a\\_n$).\n  - **2 pts**: Conclude that the sequence is bounded (either globally bounded by a value like $2c+1$ using induction/contradiction, or eventually bounded via infinite descent).\n- **2 pts**: Periodicity.\n  - **1 pt**: Apply the Pigeonhole Principle to show that a value in the sequence must repeat.\n  - **1 pt**: Conclude that repetition implies periodicity because the recurrence relation $a\\_{n+1} = d(a\\_n) + c$ is deterministic.\n\n**Total (max 7)**\n\n1. **Zero-credit items**\n- Claims that $d(n) < n$ implies boundedness without a specific quantitative argument (since $a\\_{n+1} \\\\approx a\\_n + c$ allows growth if $d(n) \\\\approx n$).\n- Proving periodicity only for specific values of $c$.\n- Stating that the sequence is periodic because it is bounded, without proving boundedness.\n1. **Deductions**\n- **Cap at 5/7**: If the student proves $a\\_n$ is bounded but fails to explicitly mention the Pigeonhole Principle or finite states to deduce repetition.\n- **\\-1 point**: If the logic for boundedness relies on a bound like $d(n) \\\\leq \\\\sqrt{n}$ for *all* $n$ (which is false for small $n$), unless the argument is explicitly restricted to \"sufficiently large $n$\".\n- **No deduction**: For stating $d(m) \\\\leq m/2 + 1$ without proof.\n- **No deduction**: For proving the sequence is *eventually* bounded rather than bounded for all $n$ (both are sufficient for the problem).\n\n**Figure 4.** Scoring rubrics used by the evaluation setup.\n\n**Problem difficulty annotations.** We annotate each problem with a difficulty estimate as determined by the average performance of our base model (Qwen3-4B-Thinking), computed over 128 parallel attempts, graded by [GPT-OSS-20B](https://huggingface.co/openai/gpt-oss-20b), and using the grading schemes mentioned above. We use these annotations to develop a difficulty-based learning curriculum during RL training. We use this cleaned-up dataset as our main prompt set and release it for others to use.\n\n## [Our Post-Training Recipe](#our-post-training-recipe)\n\nTo develop an effective post-training recipe, we begin by asking a simple question: **what does it take for small models to approach the performance of much larger LLMs?** At a high level, we achieve this via a reinforcement learning (RL) post-training recipe that trains models to produce long chains-of-thought for proof generation. We therefore first describe our core RL setup, which combines an efficient asynchronous off-policy implementation (that we also release) with rubric-based grading to provide reward signals for policy learning.\n\nWhile standard RL training should improve the model’s proof-writing capability, as we also observe in our experiments, matching the performance of larger models naturally requires small models to use substantially more test-time compute. In our best configurations, this amounts to spending over a million tokens per problem on average. A naive approach that trains RL directly on such long chains of thought is challenging both infrastructure-wise and from the perspective of variance control in long-horizon updates. Instead, we train at moderate output lengths while explicitly optimizing for behavior that benefits from much larger test-time budgets.\n\nTo achieve this, we modify our RL recipe to incorporate an algorithmic extension based on the recently introduced Reasoning Cache (RC) ([Wu et al., 2026](#bib-wu2026reasoningcache)) approach. During training, the model uses an iterative decoding process that alternates between summarizing its reasoning and continuing to reason conditioned on the generated summary. Incorporating this into training and optimizing rewards under this scaffold allows us to optimize behavior that transfers to other test-time scaffolds used during deployment.\n\nAfter establishing this post-training recipe on top of the Qwen3-4B base model, we apply the same framework to a stronger initialization that is able to write proofs of higher quality, obtained through offline distillation via supervised fine-tuning. Specifically, we use DeepSeek-Math-V2 (685B parameters) to generate a compact, high-quality set of proof-style examples for supervised mid-training before running RL with RC. We describe this workflow and the associated design decisions, supported by preliminary ablations, in the sections below.\n\n## [Core Reinforcement Learning Approach](#core-reinforcement-learning-approach)\n\nAny typical RL pipeline needs a few basic components: the reward function, the prompt set, and the maximum response length allowed. Along with these components, there are several design questions: How do we decide what length to run RL with? What prompt sets should we use for RL? How do we decide what the grader sees and what rubrics it uses for grading? In this section, we present answers to these questions with some preliminary experiments.\n\n### [Grading Protocol](#grading-protocol)\n\nDesigning a reliable reward signal for RL requires a careful balance between fidelity to human judgment and computational efficiency. A strong grader should produce scores that align closely with human evaluations, while maintaining low latency so that it remains practical for large-scale RL training. To identify an effective configuration, we conducted a series of experiments examining grader model choice, system instructions, and reasoning budget. We evaluate these design decisions below.\n\n**Grader evaluation benchmarks.** We construct two benchmarks to evaluate our grader design. First, we aggregate all human annotations from the proof-based portion of [MathArena](https://matharena.ai/), comprising 438 solutions across 22 problems. Second, to obtain a benchmark more representative of our training-time prompt distribution that we will query the grader on, we randomly sample 60 problems from our training corpus. For each problem, we generate four candidate solutions from our base 4B model and the 30B Thinking model from the same model family. We grade these solutions using Gemini 3 Pro, instructed with a prompt adapted from the ProofBench paper ([Ma et al., 2025](#bib-ma2025reliable)), which we found to yield evaluations consistent with human judgment. We therefore treat Gemini 3 Pro’s grades as the ground-truth reference in this benchmark. Both of these grader evaluation benchmarks can be found in our Hugging Face [collection](https://huggingface.co/collections/lm-provers/qed-nano).\n\n**Grader evaluation metric.** Both benchmarks contain multiple solutions per problem, enabling calibrated comparisons through a problem-normalized *advantage score*. For each problem pip\\_i and solution yjiy\\_j^i to problem pip\\_i, we compute the unnormalized advantage Ai,j\\=ri,j−ri‾A\\_{i,j} = r\\_{i,j} - \\\\overline{r\\_i}, where ri,jr\\_{i,j} is the grader-assigned reward to solution yjiy\\_j^i, and ri‾\\\\overline{r\\_i} is the mean reward across all solutions to problem pip\\_i. Grader accuracy is measured as the mean absolute difference between the candidate grader’s advantages and the reference advantages. This formulation removes sensitivity to constant or benign shifts between graders, which is important because such shifts do not affect RL training with several parallel rollouts (as used by GRPO ([Shao et al., 2024](#bib-shao2024deepseekmath))).\n\n**Grader model and prompt.** Using the metric above, we evaluate five grader prompts drawn from prior work emphasizing different evaluation ideologies (Table 2). On the MathArena subset, GPT-OSS-20B with medium reasoning performs best when paired with the strict ProofBench ([Ma et al., 2025](#bib-ma2025reliable)) prompt, which emphasizes strict adherence to the rubric and rejects solutions that deviate from it. Prompts are shown below.\n\n**Table 2.** Results on the MathArena grading benchmark. Lower is better.\n\n| Model              | Simple | OPC  | ProofBench | ProofBench Strict | GIMO |\n| ------------------ | ------ | ---- | ---------- | ----------------- | ---- |\n| GPT-OSS-20B-medium | 1.56   | 1.57 | 1.43       | **1.21**          | 1.36 |\n\nPrompts\n\n**Figure 5.** Prompt traces used for the proof-generation and evaluation pipeline.\n\nWe then compare the choice of grader models and evaluate whether including a reference proof alongside the marking scheme improves performance (Table 3). We conduct this experiment on the in-distribution grading benchmark as it is more representative of scenarios that the grader will encounter during training. We observe that the performance differences between models are minimal. GPT-OSS-20B with medium reasoning performs on par with the alternatives while being significantly cheaper and faster, so we adopt it as our grader for training. Including a reference solution slightly degrades performance, so we exclude it from the final grader configuration.\n\n**Table 3.** Results on our in-distribution grading benchmark. Lower is better.\n\n| Model               | ProofBench Strict | ProofBench Strict (with ref) |\n| ------------------- | ----------------- | ---------------------------- |\n| GPT-OSS-20B-medium  | 1.19              | 1.26                         |\n| GPT-OSS-20B-high    | 1.17              | 1.19                         |\n| GPT-OSS-120B-medium | 1.16              | 1.24                         |\n\n### [Outcome-Reward RL with Long Response Lengths](#outcome-reward-rl-with-long-response-lengths)\n\n**Figure 6.** A schematic illustration of our pipeline for outcome-reward RL training of QED-Nano. We train with rubric-based rewards derived from a grading scheme as discussed in the Setup section.\n\nEquipped with this grading scheme, we run RL to optimize the resulting outcome rewards. Two design choices remain when instantiating an RL run: the prompt set and the RL hyperparameters, in particular, the number of parallel rollouts per problem and the maximum response length. As discussed in the previous section, we construct a prompt set such that the base model’s pass@1 scores follow a unimodal, heavy-tailed distribution (by modifying the distribution shown in Figure 7), with a peak near difficult problems and a decreasing probability of sampling substantially easier ones.\n\n**Figure 7.** A schematic showing the distribution of the average reward per problem in our unfiltered prompt set. We remove all problems that attain a pass@1 score > 0.7 and use the remainder as our prompt set for training. We also remove problems where pass@1 score = 0.0.\n\nWe completely remove all very easy problems on which the base model can attain a pass@1 score higher than 0.7 and also remove the extremely hard problems. With this prompt set, we now describe our workflow for setting the various hyperparameters of the RL algorithm.\n\n**Base RL algorithm.** We use GRPO ([Shao et al., 2024](#bib-shao2024deepseekmath)) as our base RL algorithm and build on PipelineRL ([Piche et al., 2025](#bib-piche2025pipelinerl)) to implement an asynchronous, streaming variant of this algorithm (Figure 8).\n\n![Example with caption and credit](https://lm-provers-qed-nano-blogpost.hf.space/_astro/rl_pipeline_final.COU_ho3u_Z1f11jh.webp)\n\n**Figure 8.** A schematic illustration of an asynchronous, streaming variant of GRPO that we also employ in our PipelineRL implementation. Image from the [Magistral tech report](https://mistral.ai/news/magistral)\n\nThis implementation performs off-policy updates, with a maximum lag of 5 gradient steps between the current policy and the reference policy. We ablate several hyperparameters, including the number of parallel rollouts per problem, the entropy coefficient, and the KL divergence loss. We utilize an entropy coefficient of 1e-4 through training and no KL regularization. Consistent with prior work, we find that a larger number of rollouts nn per problem improves performance when sufficient training epochs are run. Based on initial experiments with n\\=4,8,16n = 4, 8, 16, we selected n\\=16n = 16 because the fraction of problems on which no successful rollout is sampled is merely 2-3% at n\\=16n=16, which ensures a stable training signal (Figure 9). Running at this scale required 7 nodes to generate rollouts at a batch size of 64 problems (i.e., a total batch size of 1024 samples per step) and 4 nodes for the trainer.\n\n**Figure 9.** Effective group size throughout RL training. With n = 16 parallel rollouts per problem, the effective group size remains close to the maximum for most of training, indicating that nearly all problems receive both successful and unsuccessful rollouts — ensuring a stable training signal.\n\nWe set the maximum response length to 50,000 tokens for RL training, since 95% of responses from the base model terminate within this limit. As training progresses, however, we observe a noticeable increase in output length, consistent with observations from DeepSeek-R1 ([DeepSeek-AI et al., 2025](#bib-deepseekai2025deepseekr1)) and others. A representative learning curve and corresponding evaluation scores are shown in Figure 10\\. We observe a noticeable increase in both the training and evaluation scores (on both IMO-ProofBench and ProofBench).\n\n**Figure 10.** RL training curves with rubric-based rewards and corresponding evaluation metrics on IMO-ProofBench and ProofBench. Observe that as training proceeds, training rewards rise steadily and mean output length increases. Note that this is the mean output length on the training prompt set, which also includes some simpler problems on which the model is not able to exhaust the full token budget.\n\n## [RL for Continual Improvement at Test Time via Reasoning Cache](#rl-for-continual-improvement-at-test-time-via-reasoning-cache)\n\nHaving established that RL improves both training reward and test-time performance under the grader, the natural next step is to scale these gains further. For a small 4B model, increasing test-time computation provides a direct mechanism for extracting additional performance. A naive approach would increase the maximum response length during RL training, but this introduces substantial infrastructure costs and exacerbates variance in long-horizon optimization.\n\nInstead of training on extremely long monolithic responses, we introduce additional structure into the generation process. In particular, we adopt an iterative decoding procedure during training in which the model produces short reasoning segments that can be optimized with standard RL, while still encouraging improvements in long-horizon performance. We implement this idea using the Reasoning Cache (RC) framework ([Wu et al., 2026](#bib-wu2026reasoningcache)). RC decomposes reasoning into multi-step refinement cycles. At each iteration, the model generates a partial reasoning trace, summarizes its progress into a compact short textual “state representation”, and conditions the next rollout on both the original problem and this summary (Figure 11). Each subsequent summarization step updates the previous summary with any information added in the current reasoning step. Then, we train the model with RL to improve its summary-conditioned generation capabilities. This structure allows the model to effectively explore reasoning horizons equivalent to hundreds of thousands of tokens while maintaining smaller training rollout lengths.\n\n**Figure 11.** Illustration of the [RC algorithm](https://huggingface.co/papers/2602.03773). RC decoding replaces standard autoregressive decoding at both train and test time. During RC decoding, the LLM generates a reasoning trace, summarizes it, discards the original trace, and conditions subsequent reasoning on this summary. This design decouples the effective reasoning horizon from the length of any single reasoning trace, thus maintaining tractable rollout lengths for outcome-reward RL while also enabling continual improvement at test time.\n\nWe apply RL updates across these RC states, training the model to improve conditioned on the summary. Empirically, RC improves training stability, convergence speed, and performance compared to standard RL (Figure 12).\n\n**Figure 12.** Training curves comparing RL and RL with RC. Both runs use rubric-based rewards. RC achieves faster convergence and higher final reward, while rollout lengths grow more moderately under RC due to the iterative summarize-and-refine structure.\n\nIt also reduces the per decoding-turn response length, although this can easily be compensated for by running for more turns. Each subsequent turn improves over the average reward attained by the previous turn (Figure 13).\n\n**Figure 13.** Per-turn mean reward during RC training. Each panel shows the reward for a successive reasoning-cache turn. The model improves with each additional turn, confirming that RC training teaches the model to refine its reasoning conditioned on prior summaries.\n\nWhile we use the same model for both reasoning and summarization at test time, during training, we choose to avoid using a thinking model for summarization to speed up the training process. Instead, we use a frozen snapshot of the Qwen3-4B-Instruct-2507 model for summarization. That said, we observe that these sorts of gains with RC persist even when the same model performs both reasoning and summarization, suggesting that the primary benefit arises from extending the effective reasoning horizon rather than from any new information or prompt tuning.\n\nUpon evaluation, we find that both the RL-trained and RC-trained models achieve similar performance within a single decoding turn. However, the RC-trained checkpoint improves substantially more when run with the RC scaffold (see Figure 13 below). In particular, the RC-trained model outperforms the RL-trained model at every turn, with the largest gap appearing within the first three turns, which matches the number of turns used during training. We also evaluate both RL- and RC-trained models using a different agentic scaffold, namely the DeepSeek-Math-V2 scaffold discussed later, and again observe larger gains for the RC-trained model. These results suggest that RC-style training better prepares the model to benefit from test-time scaffolds, and therefore we adopt RC training in our final recipe. An example of the scaffold is shown in Figure 14.\n\n**Figure 13.** Average grade (normalized to 0–100%) on IMO-ProofBench as a function of reasoning-cache turns. Observe that applying the RC scaffold at test time on top of the RC-trained model attains higher performance than applying the RC scaffold on top of the RL-trained model. The gains are largest at turn 3 of the RC decoding process, which represents the number of turns also used for RC training.\n\nReasoning Cache Example\n\nUser\n\n**Figure 14.** A multi-step reasoning dialogue showing chain-of-thought and summarization.\n\n## [Initialization via Supervised Fine-Tuning](#initialization-via-supervised-fine-tuning)\n\nDespite the promising results from RL on top of the 4B base model, we found that building coverage over certain proof-writing strategies with an initial supervised fine-tuning stage provides a better initialization for the RL run. Therefore, in parallel, we iterated on SFT for the base model. Our SFT recipe fine-tunes the base model on problems paired with proof solutions generated by [deepseek-ai/DeepSeek-Math-V2](https://huggingface.co/deepseek-ai/DeepSeek-Math-V2), a 685B model fine-tuned specifically for Olympiad math (with a complex training procedure that involves meta-verifiers). We distill this teacher’s reasoning traces into a compact dataset of ≈\\\\approx7.5k sampled responses suitable for fine-tuning our 4B base model. We describe this in detail below.\n\n**SFT dataset generation using DeepSeek-Math-V2.** We generate solutions for problems in our curated dataset using a 128k-token context limit. Given the large size of the teacher (685B parameters), simply running inference on the teacher was challenging, and we had to orchestrate inference across 8 parallel instances (with **SGLang**), each distributed over two 8xH100 nodes (with TP=8, EP=8, PP=2). A central router load-balanced all inference requests, achieving a throughput of ≈\\\\approx3000 tokens/s. We first filter raw generations to retain only structurally valid completions containing closed reasoning blocks and explicit proof sections. We grade the solutions with Gemini 3 Pro and intentionally avoid discarding low-scoring samples simply because they might still provide useful information about proof-writing. This process yields a dataset of **7.5k proof-style responses** across 4,300 distinct problems spanning Algebra, Calculus, Combinatorics, Geometry, Inequalities, Logic and Puzzles, and Number Theory.\n\nWe fine-tuned our base 4B model on this dataset using a global batch size of 32 for five epochs. We applied a cosine learning rate schedule with a 10% warmup and a peak value of 3×10−53 \\\\times 10^{-5}, which provided stable convergence while reducing validation SFT-loss on a hold-out set.\n\n**Data ablation: quantity vs. uniqueness.** We performed several ablations with different data mixtures; we highlight the comparison between training on the full corpus of 7,500 prompt-completion pairs versus a strictly filtered set of 4,300 correct solutions, where one solution is associated with a unique problem. We find that training on only the unique problems achieves a higher final performance on IMO-ProofBench. The checkpoint at step 372 of this run was therefore used as an initialization for RL training (Figure 15).\n\n**Figure 15.** As illustrated in the figure above, using the unique dataset achieved higher performance on IMO-ProofBench. We use this initialization for our RL runs.\n\n**Challenges and limitations of SFT.** SFT serves as a strong bootstrap and produces a clear improvement over the base 4B model. However, the process also introduced significant drawbacks, most notably **length explosion**. Although the training data caps sequences at 45k tokens, the fine-tuned model frequently generates outputs that grow to hundreds of thousands of tokens, and are typically much longer for incorrect proof attempts. Rather than producing structured long-form reasoning, the model often imitates the surface appearance of extended proofs, repeating or meandering until the context window is exhausted. This behavior is a natural consequence of offline training on data that comes from a bigger model (or data that is generally “hard to fit”) and indicates the need for a more “experiential” learning paradigm instead.\n\nRL provides a natural mechanism for experiential learning, and in practice, we observe that response lengths decrease following RL training. This trend also holds when training with RC. However, the early phase of RL is heavily confounded by rollout truncation and a high overflow rate (often around 60% on average), which impairs credit assignment and reduces the effectiveness of RL and RC when initialized from SFT. An immediate direction for future work is to address this length overflow issue more directly. One possibility is to replace SFT with on-policy distillation, though this is computationally expensive due to the inference costs of the 685B-parameter DeepSeek-Math-V2 model. A more practical alternative is to approximate it by blending in on-policy traces during SFT. A complementary approach is to introduce a curriculum during RL: first training on problems that do not suffer from severe overflows, thereby enabling the model to realize the benefits of RL before scaling to longer-horizon settings.\n\nOur final recipe consists of an initial SFT step to imbue the model with the ability to write high-quality proofs. Then, we perform rubric-based RL training with the reasoning cache approach to make the model capable of effectively thinking longer when used with test-time scaffolds. Finally, we deploy the trained model with a test-time scaffold.\n\n## [Further Scaling Test-Time Compute with QED-Nano](#further-scaling-test-time-compute-with-qed-nano)\n\nWe next explore whether scaffolds that combine parallel and sequential generation can further scale token usage and thereby performance. Since our RL training procedure optimizes summary-conditioned generation without enforcing a specific summary format, we expect that the trained model would benefit from a broad class of scaffolds, including those that condition subsequent generations on detailed verification logs of the previous generations. Indeed, the trained model does choose to verify information present in the summary. We therefore evaluate several alternative test-time scaffolds to further scale the performance of QED-Nano.\n\nIn particular, we explored the following scaffolds:\n\n- **Reasoning-Cache (RC)** ([Wu et al., 2026](#bib-wu2026reasoningcache)): The decoding algorithm that we used for RL post-training, which iteratively summarizes the current attempt and conditions subsequent response generation on it. This approach does not utilize parallel sampling directly and we only run it for 3 turns in this result.\n- **Self-Check** ([Huang & Hui, 2025](#bib-huang2025winninggold)): A simple generate-verify-improve loop that ends when the verifier cannot find any flaws in the solution anymore.\n- [**Nomos**](https://github.com/NousResearch/nomos): This scaffold first generates nn solutions and verifies each of them once. It then filters the solutions to only keep the kk best ones, after which it is run through a single “consolidation” stage, where the model is presented with all kk solutions and asked which group of solutions is most likely to be correct. Among that group, the agent runs a simple knockout tournament with an LLM-judge to select the best one.\n- [**RSA**](https://rsa-llm.github.io/static/pdfs/Recursive%5FSelf%5FAggregation.pdf): This scaffold first generates nn solutions. In each subsequent stage, it then generates nn new solutions by randomly conditioning each new solution on kk existing ones. After several iterations, a random proof is selected. We make one minor improvement over the original design: instead of selecting an arbitrary proof, we run a knockout tournament with an LLM-judge on the solutions from the last stage.\n- **DeepSeek Math** ([Shao et al., 2025](#bib-shao2025deepseekmathv2)) (DSM): This scaffold first generates nn solutions, each of which is self-evaluated nn times. Solutions are sorted by their average self-evaluated score, and the top nn solutions are improved by presenting the model with the solution and some of the feedback generated by the self-evaluation stage. These new solutions are added to the solution pool. This process is iterated several times before the solution with the highest overall score across all iterations is returned.\n\nIn preliminary experiments on RL-trained checkpoints initialized from the base model, we evaluated several test-time scaffolds. DSM, RSA, and RC consistently yielded the strongest improvements, with DSM outperforming RSA in several settings, while other scaffolds provided more modest gains. Among these, RC does not run parallel sampling at each turn, making it convenient for training but suboptimal for maximizing test-time performance, whereas DSM can effectively scale parallel compute as well. We therefore adopted DSM for our main experiments. This choice was further motivated by the fact that our SFT initialization leveraged traces from DeepSeek-Math-V2, which is explicitly trained for self-verification. Consequently, we hypothesized that a scaffold that explicitly incorporates self-verification would be particularly effective for our final trained QED-Nano model. Our main results confirm this hypothesis.\n\n**Quantitative evaluation.**Figure 16 summarizes the performance of different scaffolds applied to our final QED-Nano model (note that the preliminary experiments above were done on the RL-trained checkpoint from the base model). RC yields the smallest performance gain within 3 turns (which is perhaps expected), but it also requires only about twice the number of tokens. In contrast, RSA and DSM provide substantial improvements of 17% and 14%, respectively. However, they are substantially more expensive: RSA costs about 20 times as much as the base model, and DSM about 16 times as much. Overall, DSM provides a reasonable tradeoff for performance within a given token budget and we therefore utilize it. However, one could also choose to use RSA given these results.\n\nAgent Scaffold Comparison: Average Grade vs Average Tokens\n\n**Figure 16.** Comparison of different agent scaffolds applied on the QED-Nano model and evaluated on IMO problems, showing the trade-off between token usage and performance.\n\n## [Qualitative Analysis of Solutions from QED-Nano](#qualitative-analysis-of-solutions-from-qed-nano)\n\nWe manually examined a subset of QED-Nano’s generated proofs to assess two additional dimensions beyond benchmark scores: **(1)** whether the model attempts to reward-hack the LLM-based grader, and **(2)** the intrinsic quality of the proofs themselves. An experienced human evaluator from our team, with a substantial background in evaluating LLM-generated mathematical proofs, reviewed a sample of proofs and compared their judgments with those of the automated grader. Detailed annotations are provided in the accompanying figure; below, we summarize the main observations.\n\n**Agreement between LLM judge and human judgment.** We found no clear evidence of reward hacking on our Gemini 3 Pro grader. The human evaluator agreed with the automated grader on most problems, though the LLM grader was occasionally a bit more generous. Only one problem showed a significant change in score: the QED-Nano agent’s solution to IMO 2025 Q2\\. Our human grader judged the heavy computational approach incorrect, noting an algebraic error and too many gaps for a fully rigorous proof. We do not attribute this discrepancy to deliberate reward hacking, as other models generally attempted similar approaches to Q2.\n\n**Proof quality.** The generated proofs are generally well structured and logically organized, making them easy to read and follow. The most prominent weakness is a consistent preference for computation-heavy approaches. In geometry problems in particular, the solutions always rely on coordinate or algebraic arguments (“bashing”) rather than synthetic arguments. In other domains, the model also frequently provides proofs that are more computational than the human-written ground-truth solution. However, this tendency is not unique to QED-Nano as it reflects a broader pattern across LLMs (based on prior experience from evaluating various frontier models on MathArena evaluations).\n\n**Agentic vs. base model.** We compared proofs generated by the base model with those produced with the DSM agent. In most cases, the outputs are stylistically and structurally similar. The primary benefit of the agentic setup appears in cases where the base model’s solution contains a clear but nontrivial mistake. There, the agent often produces a correct proof that addresses that particular mistake.\n\n**IMO 2025.** We explicitly evaluated our model on the 2025 IMO problems. According to the Gemini 3 Pro judge, QED-Nano achieves a score of 22/42 when used with the test-time scaffold. The standalone QED-Nano model achieves 12/42 under the same judge. We observed, however, that the trained model does make fairly simple algebraic or transcription errors, such as mismanipulating terms or incorrectly copying previously derived expressions. The Gemini 3 Pro judge is relatively lenient toward these mistakes, and is also too likely to give intermediate points for trivial steps. According to our human grader, QED-Nano attains 14/42 with the scaffold and 7/42 without it.\n\nAlthough some of these errors would very likely be mitigated with additional training or larger models that tend to follow the semantics of language or by employing a stricter grading protocol during training, the results also make clear that the model struggles substantially on some of the most challenging problems, where it is unable to make meaningful progress.\n\n**Comparison with other models.** We also compared QED-Nano’s proofs with those generated by the Qwen-3-4B-Thinking-2507 base model and Nomos-1\\. The base model’s outputs are of very poor proof quality. It is clearly trained to obtain the correct final answer while disregarding mathematical rigor. While it occasionally finds the correct answer to a problem, its proofs often contain obvious mistakes, such as circular arguments and simplifying assumptions. Nevertheless, our recipe is able to improve this base model to produce proofs of much higher quality.\n\nNomos-1 has a significantly different style compared to QED-Nano. Its proofs are more concise and direct, more similar to expert human-written solutions. For instance, it does not repeat the problem statement and instead immediately provides the answer, often proves lemmas as separate statements, frequently skips computational steps, and writes in a very brief, matter-of-fact statements. While expert-written solutions can generally be trusted to be this compressed, the general sycophancy found in LLMs and their tendency to hide mistakes in proofs, makes this behavior more problematic. Determining which writing style is better is quite subjective, but we encourage you to read some of the proofs below to decide for yourself!\n\n**Vibe checks.** We evaluated the model with a small set of non-mathematical prompts to confirm it retained broader capabilities. Remarkably, it still follows instructions effectively, with no noticeable regressions in non-mathematical reasoning tasks.\n\nProofs and Judgments\n\nProblem\n\nModel Summary\n\n**Model:** QED-Nano\n\n**Grade:** 7/7\n\n**Dataset:** lm-provers/imoproofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-rc\\_v0900-step-000150-google\n\n**Split:** 20260203\\_133642\n\nComment\n\nProof\n\nModel Summary\n\n**Model:** QED-Nano (Agent)\n\n**Grade:** 7/7\n\n**Dataset:** lm-provers/imoproofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-agent-deepseek\\_math-rc\\_v0900-step-000150-google\n\n**Split:** 20260211\\_144929\n\nComment\n\nProof\n\nModel Summary\n\n**Model:** Qwen3-4B-Think\n\n**Grade:** 1/7\n\n**Dataset:** lm-provers/imoproofbench-outputs\n\n**Config:** qwen-qwen3-4b-thinking-2507-google\n\n**Split:** 20260130\\_100355\n\nComment\n\nProof\n\nModel Summary\n\n**Model:** Nomos-1\n\n**Grade:** 7/7\n\n**Dataset:** lm-provers/imoproofbench-outputs\n\n**Config:** nousresearch-nomos-1-google\n\n**Split:** 20260206\\_130109 (requested the oldest)\n\nComment\n\nProof\n\nProblem\n\nModel Summary\n\n**Model:** QED-Nano\n\n**Grade:** 1/7\n\n**Dataset:** lm-provers/imoproofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-rc\\_v0900-step-000150-google\n\n**Split:** 20260203\\_133642\n\nComment\n\nProof\n\nModel Summary\n\n**Model:** QED-Nano (Agent)\n\n**Grade:** 6/7\n\n**Dataset:** lm-provers/imoproofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-agent-deepseek\\_math-rc\\_v0900-step-000150-google\n\n**Split:** 20260211\\_144929\n\nComment\n\nProof\n\nModel Summary\n\n**Model:** Qwen3-4B-Think\n\n**Grade:** 1/7\n\n**Dataset:** lm-provers/imoproofbench-outputs\n\n**Config:** qwen-qwen3-4b-thinking-2507-google\n\n**Split:** 20260130\\_100355\n\nComment\n\nProof\n\nModel Summary\n\n**Model:** Nomos-1\n\n**Grade:** 7/7\n\n**Dataset:** lm-provers/imoproofbench-outputs\n\n**Config:** nousresearch-nomos-1-google\n\n**Split:** 20260206\\_130109 (requested the oldest)\n\nComment\n\nProof\n\nProblem\n\nModel Summary\n\n**Model:** QED-Nano\n\n**Grade:** 4/7\n\n**Dataset:** lm-provers/imoproofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-rc\\_v0900-step-000150-google\n\n**Split:** 20260203\\_133642\n\nComment\n\nProof\n\nModel Summary\n\n**Model:** QED-Nano (Agent)\n\n**Grade:** 7/7\n\n**Dataset:** lm-provers/imoproofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-agent-deepseek\\_math-rc\\_v0900-step-000150-google\n\n**Split:** 20260211\\_144929\n\nComment\n\nProof\n\nModel Summary\n\n**Model:** Qwen3-4B-Think\n\n**Grade:** 1/7\n\n**Dataset:** lm-provers/imoproofbench-outputs\n\n**Config:** qwen-qwen3-4b-thinking-2507-google\n\n**Split:** 20260130\\_100355\n\nComment\n\nProof\n\nModel Summary\n\n**Model:** Nomos-1\n\n**Grade:** 0/7\n\n**Dataset:** lm-provers/imoproofbench-outputs\n\n**Config:** nousresearch-nomos-1-google\n\n**Split:** 20260206\\_130109 (requested the oldest)\n\nComment\n\nProof\n\nProblem\n\nModel Summary\n\n**Model:** QED-Nano\n\n**Grade:** 0/7\n\n**Dataset:** lm-provers/imoproofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-rc\\_v0900-step-000150-google\n\n**Split:** 20260203\\_133642\n\nComment\n\nProof\n\nModel Summary\n\n**Model:** QED-Nano (Agent)\n\n**Grade:** 7/7\n\n**Dataset:** lm-provers/imoproofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-agent-deepseek\\_math-rc\\_v0900-step-000150-google\n\n**Split:** 20260211\\_144929\n\nComment\n\nProof\n\nModel Summary\n\n**Model:** Qwen3-4B-Think\n\n**Grade:** 7/7\n\n**Dataset:** lm-provers/imoproofbench-outputs\n\n**Config:** qwen-qwen3-4b-thinking-2507-google\n\n**Split:** 20260130\\_100355\n\nComment\n\nProof\n\nModel Summary\n\n**Model:** Nomos-1\n\n**Grade:** 7/7\n\n**Dataset:** lm-provers/imoproofbench-outputs\n\n**Config:** nousresearch-nomos-1-google\n\n**Split:** 20260206\\_130109 (requested the oldest)\n\nComment\n\nProof\n\nProblem\n\nModel Summary\n\n**Model:** QED-Nano\n\n**Grade:** 0/7\n\n**Dataset:** lm-provers/imoproofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-rc\\_v0900-step-000150-google\n\n**Split:** 20260203\\_133642\n\nComment\n\nProof\n\nModel Summary\n\n**Model:** QED-Nano (Agent)\n\n**Grade:** 7/7\n\n**Dataset:** lm-provers/imoproofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-agent-deepseek\\_math-rc\\_v0900-step-000150-google\n\n**Split:** 20260211\\_144929\n\nComment\n\nProof\n\nModel Summary\n\n**Model:** Qwen3-4B-Think\n\n**Grade:** 0/7\n\n**Dataset:** lm-provers/imoproofbench-outputs\n\n**Config:** qwen-qwen3-4b-thinking-2507-google\n\n**Split:** 20260130\\_100355\n\nComment\n\nProof\n\nModel Summary\n\n**Model:** Nomos-1\n\n**Grade:** 0/7\n\n**Dataset:** lm-provers/imoproofbench-outputs\n\n**Config:** nousresearch-nomos-1-google\n\n**Split:** 20260206\\_130109 (requested the oldest)\n\nComment\n\nProof\n\nProblem\n\nModel Summary\n\n**Model:** QED-Nano\n\n**Grade:** 6/7\n\n**Dataset:** lm-provers/imoproofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-rc\\_v0900-step-000150-google\n\n**Split:** 20260203\\_133642\n\nComment\n\nProof\n\nModel Summary\n\n**Model:** QED-Nano (Agent)\n\n**Grade:** 0/7\n\n**Dataset:** lm-provers/imoproofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-agent-deepseek\\_math-rc\\_v0900-step-000150-google\n\n**Split:** 20260211\\_144929\n\nComment\n\nProof\n\nProblem\n\nModel Summary\n\n**Model:** QED-Nano\n\n**Grade:** 7/7\n\n**Dataset:** lm-provers/imoproofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-rc\\_v0900-step-000150-google\n\n**Split:** 20260203\\_133642\n\nComment\n\nProof\n\nModel Summary\n\n**Model:** QED-Nano (Agent)\n\n**Grade:** 7/7\n\n**Dataset:** lm-provers/imoproofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-agent-deepseek\\_math-rc\\_v0900-step-000150-google\n\n**Split:** 20260211\\_144929\n\nComment\n\nProof\n\nProblem\n\nModel Summary\n\n**Model:** QED-Nano\n\n**Grade:** 0/7\n\n**Dataset:** lm-provers/imoproofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-rc\\_v0900-step-000150-google\n\n**Split:** 20260203\\_133642\n\nComment\n\nProof\n\nModel Summary\n\n**Model:** QED-Nano (Agent)\n\n**Grade:** 0/7\n\n**Dataset:** lm-provers/imoproofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-agent-deepseek\\_math-rc\\_v0900-step-000150-google\n\n**Split:** 20260211\\_144929\n\nComment\n\nProof\n\nProblem\n\nModel Summary\n\n**Model:** QED-Nano\n\n**Grade:** 0/7\n\n**Dataset:** lm-provers/proofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-rc\\_v0900-step-000150-google\n\n**Split:** 20260214\\_101743\n\nComment\n\nProof\n\nModel Summary\n\n**Model:** QED-Nano (Agent)\n\n**Grade:** 0/7\n\n**Dataset:** lm-provers/proofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-agent-deepseek\\_math-rc\\_v0900-step-000150-google\n\n**Split:** 20260212\\_230045\n\nComment\n\nProof\n\nProblem\n\nModel Summary\n\n**Model:** QED-Nano\n\n**Grade:** 6/7\n\n**Dataset:** lm-provers/proofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-rc\\_v0900-step-000150-google\n\n**Split:** 20260214\\_101743\n\nComment\n\nProof\n\nModel Summary\n\n**Model:** QED-Nano (Agent)\n\n**Grade:** 0/7\n\n**Dataset:** lm-provers/proofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-agent-deepseek\\_math-rc\\_v0900-step-000150-google\n\n**Split:** 20260212\\_230045\n\nComment\n\nProof\n\nProblem\n\nModel Summary\n\n**Model:** QED-Nano\n\n**Grade:** 1/7\n\n**Dataset:** lm-provers/proofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-rc\\_v0900-step-000150-google\n\n**Split:** 20260214\\_101743\n\nComment\n\nProof\n\nModel Summary\n\n**Model:** QED-Nano (Agent)\n\n**Grade:** 2/7\n\n**Dataset:** lm-provers/proofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-agent-deepseek\\_math-rc\\_v0900-step-000150-google\n\n**Split:** 20260212\\_230045\n\nComment\n\nProof\n\nProblem\n\nModel Summary\n\n**Model:** QED-Nano\n\n**Grade:** 0/7\n\n**Dataset:** lm-provers/proofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-rc\\_v0900-step-000150-google\n\n**Split:** 20260214\\_101743\n\nComment\n\nProof\n\nModel Summary\n\n**Model:** QED-Nano (Agent)\n\n**Grade:** 5/7\n\n**Dataset:** lm-provers/proofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-agent-deepseek\\_math-rc\\_v0900-step-000150-google\n\n**Split:** 20260212\\_230045\n\nComment\n\nProof\n\nProblem\n\nModel Summary\n\n**Model:** QED-Nano\n\n**Grade:** 0/7\n\n**Dataset:** lm-provers/proofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-rc\\_v0900-step-000150-google\n\n**Split:** 20260214\\_101743\n\nComment\n\nProof\n\nModel Summary\n\n**Model:** QED-Nano (Agent)\n\n**Grade:** 7/7\n\n**Dataset:** lm-provers/proofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-agent-deepseek\\_math-rc\\_v0900-step-000150-google\n\n**Split:** 20260212\\_230045\n\nComment\n\nProof\n\nProblem\n\nModel Summary\n\n**Model:** QED-Nano\n\n**Grade:** 0/7\n\n**Dataset:** lm-provers/proofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-rc\\_v0900-step-000150-google\n\n**Split:** 20260214\\_101743\n\nComment\n\nProof\n\nModel Summary\n\n**Model:** QED-Nano (Agent)\n\n**Grade:** 0/7\n\n**Dataset:** lm-provers/proofbench-outputs\n\n**Config:** hf-imo-colab-qwen3-4b-thinking-2507-proof-agent-deepseek\\_math-rc\\_v0900-step-000150-google\n\n**Split:** 20260212\\_230045\n\nComment\n\nProof\n\n**Figure 17.** Per-problem model proofs with reviewer comments and grades.\n\n## [Discussion and Conclusion](#discussion-and-conclusion)\n\nIn this blog, we introduce QED-Nano to demonstrate that Olympiad-level problem-solving is not reserved for frontier-scale models with 100B+ parameters. With a post-training recipe that first instills high-quality proof-writing strategies via SFT and then applies RL to explicitly optimize long-horizon improvement through test-time scaling, a 4B model can produce substantially stronger proofs than its base initialization and compete with much larger open models when paired with additional test-time compute. On IMO-ProofBench, our open-source QED-Nano (Agent) closes much of the gap to Gemini 3 Pro while being at least **3x** cheaper to run and requiring significantly lower training costs. This highlights a practical path toward strong reasoning through specialization and test-time adaptation rather than scaling only parameter count. Averaged across benchmarks, QED-Nano (Agent) also significantly outperforms larger open models such as Nomos-1 and GPT-OSS-120B.\n\nWe also outline our end-to-end workflow to help others train small models for effective long-form reasoning. More broadly, our recipe is simple and applicable to other domains where outcomes are difficult to verify directly but structured rubrics can be constructed. We did not observe evidence of reward hacking under rubric-based rewards, further supporting the robustness of this approach. To encourage follow-up work, we release our models, datasets, and code implementations.\n\n**Going forward**, several avenues could further improve QED-Nano. The most immediate action items include improving the synergy between SFT and RL. In particular, mitigating the length explosion introduced by SFT early on would likely amplify the gains from subsequent RL by speeding up the process of credit assignment. A second short-term direction is to refine the grader design, for example, by gradually tightening rubric penalties as training progresses or by using strict reward designs, thereby incentivizing increasingly rigorous and polished proofs as the model learns to make progress. Finally, incorporating hints or guidance during training, such as conditioning on plans from oracle solutions or the grading scheme, will help the model tackle harder problems during training and enable further scaling of RL to achieve stronger results.\n\nBeyond these goals, more fundamental questions remain. One direction is developing approaches that imbue the LLM with the ability to synthesize genuinely novel ideas or “aha” insights when solving the hardest problems. Like most LLMs, QED-Nano tends to rely on computation-heavy approaches rather than identifying elegant structural insights early on. This reflects the style of reasoning RL optimizes models for. Designing scalable training paradigms that encourage broader exploration of reasoning strategies, rather than refinement of a single computational path, is therefore an important challenge. From a workflow perspective, another key direction is to develop methods that more directly optimize for the specific test-time scaffolds used at deployment, tightening the alignment between train-time objectives and inference-time behavior. We encourage the community to study these aspects.\n\n## [Author Contributions](#author-contributions)\n\nThis is a team effort with members from CMU, Hugging Face, ETH Zurich, and Numina.\n\nOur team members (in alphabetical order) are as follows:\n\n- CMU: Aviral Kumar, Yuxiao Qu, Amrith Setlur, Ian Wu\n- Hugging Face: Edward Beeching, Lewis Tunstall\n- ETH Zurich: Jasper Dekoninck\n- Project Numina: Jia Li\n\nAll members contributed to the project substantially. Specifically:\n\n- Yuxiao Qu developed the initial version of the grader and verifier-based RL approach, built the grading-scheme pipeline, curated and processed the proof datasets, and implemented the initial reasoning-cache and multi-turn training loops that started us in this direction. With Amrith Setlur and Lewis Tunstall, he ran a number of ablations that informed the final RL runs.\n- Amrith Setlur adapted the PipelineRL infrastructure for the verifier-based RL approach, optimized RL configurations for stability and scale, implemented the asynchronous and streaming reasoning-cache RL training infrastructure, and proposed several ablations and algorithmic strategies for the training runs. With Yuxiao Qu and Lewis Tunstall, he ran a number of ablations that informed the final RL runs.\n- Ian Wu, as primary author of the Reasoning Cache method, provided core technical guidance on RC experimentation, evaluation, and training pipelines, which shaped how it was utilized throughout the project.\n- Edward Beeching led several evaluations, developed the synthetic data generation pipeline for DeepSeek-Math-V2, and ran the SFT ablations to analyse model length-control behavior and training dynamics.\n- Lewis Tunstall led the large-scale RL infrastructure efforts, benchmarking and stabilizing multiple RL frameworks at the start of the project, optimized inference throughput, and ran the largest training and evaluation experiments. With Amrith Setlur and Yuxiao Qu, he ran a number of ablations that informed the final RL runs. He advised several other aspects of the project.\n- Jasper Dekoninck led the benchmark design and ensured rigorous evaluations, benchmarked several test-time agent scaffolds and developed our final scaffold, built the IMO-ProofBench and ProofBench splits, filtered training datasets and created the grading schemes for them, designed the benchmarks for the RL grader, and led extensive model-based and human evaluations to ensure robustness and correlation with human proof quality.\n- Jia Li curated and expanded high-quality AoPS and Olympiad datasets, developed grading-scheme generation workflows, and explored scalable problem synthesis and verification strategies in the project initially.\n- Aviral Kumar advised the overall project and contributed to the ideas behind long-horizon training, curriculum design, reward formulation and some ideas on data construction.\n\n## [Acknowledgements](#acknowledgements)\n\nWe thank Leandro von Werra, Andres Marafioti, Thibaud Frere, Graham Neubig, Sewon Min, Wenjie Ma, and Katerina Fragkiadaki for helpful discussions and feedback. AS, YQ, IW, and AK thank the FLAME center at CMU, the DeltaAI cluster, and the NAIRR program for providing GPU resources that supported a part of the experimental iteration. We thank Google Cloud for Gemini 3 Pro API credits. AS and AK thank the Laude Institute Slingshots program for support and feedback, and Braden Hancock and Andy Konwinski at the Laude Institute for discussions and feedback. EB and LT thank Hugo Larcher and Mathieu Morlon for keeping the GPUs running hot on the Hugging Face cluster 🔥. JD used compute from the Swiss AI Initiative supported by a grant from the Swiss National Supercomputing Centre (CSCS) under project ID a155 on Alps.\n\n## [Bibliography](#bibliography)\n\n- DeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., Zhang, X., Yu, X., Wu, Y., Wu, Z. F., Gou, Z., Shao, Z., Li, Z., Gao, Z., … others. (2025). *DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning*. <https://huggingface.co/papers/2501.12948>[](#refctx-bib-deepseekai2025deepseekr1-1)\n- Huang, S., & Hui, B. (2025). *Winning Gold Medal in IMO with LLMs*. <https://huggingface.co/papers/2507.15855>[](#refctx-bib-huang2025winninggold-1)\n- Ma, W., Wu, X., Cheng, X., Tunstall, L., Nambi, A., Niu, J., Mirrokni, V., Al-Onaizan, Y., & Ma, H. (2025). *Reliable Benchmarking for LLM-Based Mathematical Proof Generation*. <https://huggingface.co/papers/2510.13888> back: [1](#refctx-bib-ma2025reliable-1), [2](#refctx-bib-ma2025reliable-2), [3](#refctx-bib-ma2025reliable-3)\n- Piche, A., Haas, C., Chatterjee, P., Nyiri, M., Sarmento, J., Spero, M., Giles, C. L., Wang, X. E., Prasad, A. S., Prasad, N., Caragea, C., Stoyanov, V., Schwartz, R., Xiong, C., & Radev, D. R. (2025). *PipelineRL: Scaling Off-Policy Reinforcement Learning for LLMs with Asynchronous Rollouts and Decoupled Training Pipelines*. <https://huggingface.co/papers/2509.19128>[](#refctx-bib-piche2025pipelinerl-1)\n- Shao, Z., Li, Y. K., Xu, A., Zhang, M., Li, Z., Wang, Y., Wang, Y., Song, J., Zhu, Q., & others. (2025). *DeepSeekMath-V2: A 671B Open-Source Tool-Integrated Reasoning Model with 128K Context Length and Better Mathematical Reasoning Through Reinforcement Learning*. <https://huggingface.co/papers/2511.22570>[](#refctx-bib-shao2025deepseekmathv2-1)\n- Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Zhang, M., Li, Y. K., & Guo, D. (2024). *DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models*. <https://huggingface.co/papers/2402.03300> back: [1](#refctx-bib-shao2024deepseekmath-1), [2](#refctx-bib-shao2024deepseekmath-2)\n- Wu, I., Tunstall, L., Dekoninck, J., & Kumar, A. (2026). *Reasoning Cache: Continual Exploration and Exploitation for Test-Time Compute Scaling of Reasoning Models*. <https://huggingface.co/papers/2602.03773> back: [1](#refctx-bib-wu2026reasoningcache-1), [2](#refctx-bib-wu2026reasoningcache-2), [3](#refctx-bib-wu2026reasoningcache-3), [4](#refctx-bib-wu2026reasoningcache-4)\n",
        "pipelineQuality": 1,
        "pipelinePassed": true,
        "pipelineReasons": [],
        "words": 8306,
        "links": 18,
        "headings": 15
      },
      {
        "engine": "openrouter_gpt_oss_20b",
        "ok": false,
        "error": "Forced engine openrouter_gpt_oss_20b requires OPENROUTER_API_KEY."
      },
      {
        "engine": "jina_reader",
        "ok": false,
        "error": "Forced engine jina_reader failed: Forced engine jina_reader returned blocked/unloaded warning."
      },
      {
        "engine": "cloudflare_markdown",
        "ok": false,
        "error": "Forced engine cloudflare_markdown failed: Markdown negotiation returned HTML instead of markdown."
      }
    ]
  },
  {
    "url": "https://bwarburg.substack.com/p/the-new-primitives",
    "baselineEngine": "jina_reader",
    "baselineWords": 5741,
    "engines": [
      {
        "engine": "auto_pipeline",
        "ok": true,
        "title": "The New Primitives",
        "markdown": "# The New Primitives\n\nLast September, security researchers at HUMAN’s Satori Threat Intelligence team [discovered something](https://www.humansecurity.com/learn/blog/slopads-highly-obfuscated-android-malware-scheme-makes-a-mess-of-the-internet-before-satori-cleanup/#:~:text=We've%20come%20a%20long,vigilant%20as%20fraudsters%20continue%20innovating.) troubling. Buried inside two hundred and twenty-four Android apps—harmless-looking utilities and games that had been downloaded 38 million times—was a sophisticated fraud engine. The malware, which analysts dubbed SlopAds, concealed its code inside image files, the way a smuggler hides contraband in a false-bottomed suitcase. Once installed, these apps transformed ordinary smartphones into tireless, invisible advertising machines, serving unauthorized content to users who never requested it.\n\nThe scheme had a cunning feature: it could detect whether someone had found an app organically or arrived via a promotional link. Only in the latter case would the fraud activate, ensuring that advertisers—not users—paid the bill. At its peak, SlopAds generated 2.3 billion fraudulent ad impressions daily. This wasn’t small-time fraud. It was industrial-scale deception, automated and optimized.\n\nThis episode is worth examining not because it was unusual but because it was typical. For two decades, the digital economy has rested on a foundation that Tim Hwang, in his 2020 book *Subprime Attention Crisis*, described as fundamentally suspect: the buying and selling of human attention. Consider the first banner ad, placed by AT&T on HotWired.com in 1994\\. Its click-through rate was 44%. Today the average banner ad [achieves 0.2%](https://www.aidigital.com/blog/ctr-for-display-ads#:~:text=It's%20not%20uncommon%20for%20high,%E2%80%8D). The problem, as Hwang argued, is structural: attention is almost impossible to verify. An “impression” could come from a person, a bot, or a phone sitting in a server farm. The market, in other words, is subprime—built on assets whose value is structurally unknowable.\n\nWe are now living through what Hwang anticipated—not a dramatic crash but a quiet erosion of trust, accelerated by AI agents that are beginning to navigate digital commerce on behalf of users. These agents promise efficiency: no more clicking through dozens of options to book a flight or find insurance. But they also raise a question: if we cannot verify human attention, how will we verify machine behavior?\n\nThe answer, according to a growing number of technologists and investors betting on this future, lies in four primitives—fundamental building blocks for a new digital economy. These are: **Intention** (systems for expressing and broadcasting what users want), **Context** (verified memory and decisions that agents can draw upon), **Attribution** (cryptographic proof of who contributed what), and **Simulation** (environments for testing agent behavior before deployment). Each primitive rests on a different type of graph: networked data structures that connect, verify, and trace digital activity in ways the current Web cannot. Together, they create the necessary underpinnings of an “Agentic Economy”—a system in which value flows not from clicks or impressions but from verifiable actions.\n\nWhether this system will work, whether it can be deployed at scale, and whether it will serve anyone beyond its architects are questions this essay will examine. But first, it is worth understanding what its builders envision.\n\nThe projections sound outlandish, but reality is catching up. Jensen Huang, NVIDIA’s CEO, declared at the Consumer Electronics Show in January 2025 that the era of agentic AI represents a multi-trillion-dollar opportunity. “[The age of AI Agentics is here,](https://finance.yahoo.com/news/nvidia-jensen-huang-says-ai-044815659.html)” he said. Ramesh Raskar, of MIT, envisions a world in which a [trillion agents](https://www.youtube.com/watch?v=Da6Ya0bfLDA) are on the loose, representing, negotiating, and transacting on our behalf—or for themselves—as parts of complex swarms. Dario Amodei, the CEO of Anthropic, has written of “[a country of geniuses in a datacenter,](https://www.darioamodei.com/essay/machines-of-loving-grace)” made up of millions of AI agents working independently or collaboratively.\n\nThe forecasters agree on little except scale. By 2028, according to Capgemini, AI agents could generate [$450 billion](https://www.capgemini.com/insights/research-library/ai-agents/) in economic value. [McKinsey projects](https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-agentic-commerce-opportunity-how-ai-agents-are-ushering-in-a-new-era-for-consumers-and-merchants) the U.S. retail market alone could see $900 billion to $1 trillion in agent-orchestrated revenue by 2030, with global figures reaching $3 trillion to $5 trillion. The investment firm Lightspeed warns of a [$19.9-trillion](https://lsvp.com/stories/the-ai-agent-economy-has-a-19-trillion-problem-our-investment-in-paid/) economic opportunity at risk if infrastructure gaps aren’t addressed. (For perspective: current global GDP is approximately [$110 trillion](https://data.worldbank.org/indicator/NY.GDP.MKTP.CD)).\n\nNo one knows what this economy will look like or whether it will be desirable. The phrase “empires for no one” has been circulating social media––a way of expressing unease that the world being built may be out of step with what humans actually want. It brings to mind a meme from a few years ago: the eighties band Toto’s buoyant song “Africa” [echoing through empty malls](https://www.newyorker.com/culture/rabbit-holes/the-overwhelming-emotion-of-hearing-totos-africa-remixed-to-sound-like-its-playing-in-an-empty-mall)—cheerful yet eerie, promising abundance yet somehow a result that’s blasphemously ugly. Then there’s the [infamous quote](https://www.youtube.com/shorts/YE5adUeTe%5FI) by OpenAI CEO Sam Altman from 2015, “I think that AI will probably, most likely, sort of lead to the end of the world. But in the meantime, there will be great companies created with serious machine learning,” which makes almost anyone do a double take. Still, even if AI development stopped today, the tools already deployed could reshape commerce substantially. As long as AI budgets continue to flow, experiments in agent deployment will continue.\n\nWhile we might not know its contours, we can begin to see some of the future economy’s building blocks. A world in which agents and robots form the next consumer class, working with or without us, requires economic rails and a set of functions that enable coordination, orchestration, and autonomous activity. To understand what is being proposed, we must examine each primitive—and the graph infrastructure underneath it—in turn.\n\nThe Internet has no native way to express what we want. We search, we browse, we click—gestures that leave platforms to infer our desires from the digital exhaust we leave behind. The attention economy’s business model depends on this: show people things until something sticks, then charge advertisers for the attempt.\n\nWhat’s replacing “where do I click?” is *programmatic desire*, or *intent*—a cryptographically secured expression of a specific outcome, broadcast to a network of competing agents ready to fulfill it. Rather than clicking through options, a user (or their agent or enterprise) would broadcast: “I need a plumber in Brooklyn this week” or “Find noise-canceling headphones under $200” or “Build a workflow that connects my users to this widget.” Providers would respond with bids. With explicit and compensable needs, transactions would complete automatically.\n\nIntents have several characteristics. First, they are outcome-focused: users often don’t care about implementation details or the steps taken to reach a goal. Second, they carry conditional authorization: users set the parameters or approvals needed to authorize an intent being fulfilled. Third, they are broadcast to a network of “solvers”—humans, agents, or protocols—who compete to fulfill all or part of an intent. And, finally, their outcomes must be verifiable.\n\nConsider the infrastructure required to make this work. At its core is an “Intents Graph”—a dynamic network mapping programmatic desires to agents capable of fulfilling them. This isn’t just a database of wants; it’s a trust-minimized matching system that replaces platform-mediated discovery with direct, verifiable connections. Protocols like Index Network are building the first such graph for social interactions, where agents facilitate connections based on mutual intent. NEAR Intents has been pursuing this logic for financial transactions, forming crypto-asset swaps where outcome efficiency is prioritized regardless of the route agents use to execute swaps.\n\nThe Intents Graph represents a fundamental architectural shift: from inferring what users might want based on their clicks to processing explicit, machine-readable declarations of need. It’s discovery elevated into trust-minimized matching. This kind of system forces relevancy or else it becomes useless. Who wants an old intent fulfilled? Who wants to be spammed? No one. The promise is that once an intent is articulated, it can be fulfilled quickly and relevantly. Behind the scenes is an automagical symphony of agentic activity.\n\nThe appeal is obvious: efficiency without the guesswork. But consider what must be true for this to work. First, users must trust that their broadcasted desires won’t be exploited—that expressing a need for a medical procedure, for instance, won’t result in price discrimination or insurance complications. Second, the verification mechanisms must actually work: the system must confirm that an intent was genuinely fulfilled, not merely claimed to be fulfilled.\n\nThe recent conflict between [Amazon and Perplexity](https://www.pcmag.com/news/amazon-sends-perplexity-a-cease-and-desist-over-its-ai-agents-shopping)—in which AI agents were allegedly making purchases using real user accounts without proper authorization—illustrates the stakes. If agents are to transact based on our behalf, they need verifiable access rights, something like a license system for digital actors, tied to a unique and portable identifier.\n\nOne enabling mechanism is x402, a protocol that revives an old idea. In the original specification for HTTP, status code 402 was reserved for “Payment Required”—a placeholder for micropayments that never materialized because credit-card fees made small transactions unworkable. But the x402 protocol enables agents to execute autonomous, per-request micropayments. Every transaction, from retrieving data to executing a service, becomes a seamless machine-to-machine exchange, settled instantly. Coinbase processed [57 million transactions](https://x.com/Cointelegraph/status/2003677242782609819?s=20) using x402 in December 2025 alone, and Cloudflare and Google have adopted it as well.\n\nThe economic result is *verified spend*—transactions that are cryptographically traceable and tied to specific identities or wallets. In theory, this creates unprecedented efficiency: money flows only toward verified, goal-directed utility. In practice, the system’s robustness depends entirely on its verification mechanisms—a theme that runs through each of the four primitives. The shift from impressions to intents is, at its core, an attempt to move from fake clicks to real, provable actions.\n\nIf the first primitive addresses what we want, the second addresses what agents need to know to deliver it. A concierge who forgets your dietary restrictions or your travel preferences isn’t charming—they’re useless. The same applies to AI agents.\n\nThe problem with most AI systems today is that they operate in “transient memory bubbles.” Each conversation starts from scratch, or nearly so. There is no persistent knowledge base that multiple agents can draw from and contribute to. “Context,” in this sense, means verified, persistent memory and operational state. It’s the difference between an agent that can book a hotel and one that remembers you prefer ground-floor rooms near elevators because of a knee injury mentioned months ago.\n\nAnyone who has used ChatGPT or coding tools like Cursor and has experienced this frustration: having to “re-inject” context repeatedly as interactions deepen, or struggling to port session logs between different AI tools. Context creates consistency and personalization, but it also raises profound questions about ownership and security.\n\nOne emergent solution is a decentralized, verifiable memory store—a “Context Graph” that holds conversational history, transactional records, workflow state. The Context Graph is knowledge infrastructure made queryable and persistent. Advances in zero-knowledge machine learning (zkML), only possible in recent years, have enabled projects like Kinic, which offers verifiable memory stores that users can control. Where might this come in handy? Ray Dalio, the well-known investor and founder of Bridgewater, created an [AI of himself](https://www.principles.com/AIBeta-signup) based on his own data, actions, and applied principles, which you can already interact with online. But without feeding this into a verifiable store, the responses could be manipulated.\n\nResearch supports the importance of persistent memory. U.C. Berkeley’s [MemGPT project](https://par.nsf.gov/servlets/purl/10524107) showed clear performance improvements when agents could access persistent memory; accuracy and relevance scores dropped noticeably when memory systems were disabled. On the LOCOMO benchmark, memory-enabled systems achieved accuracy rates nearly [30% higher](https://mem0.ai/research) than memoryless ones. NVIDIA’s recent [infrastructure announcement](https://nvidianews.nvidia.com/news/nvidia-bluefield-4-powers-new-class-of-ai-native-storage-infrastructure-for-the-next-frontier-of-ai) put a number on it: persistent context can boost processing speeds by up to five times compared to traditional storage.\n\nContext Graphs are a bit of a catch-all, incorporating knowledge graphs, memory graphs and decision graphs on steroids. If they work as designed, context becomes the most valuable asset in the system—a detailed map of preferences, behaviors, and past actions enabling sensemaking. Foundation Capital recently [articulated a vision](https://foundationcapital.com/context-graphs-ais-trillion-dollar-opportunity/) of Context Graphs needed to underpin agents. According to them, agents “sit **in the execution path**. They see the full context at decision time: what inputs were gathered across systems, what policy was evaluated, what exception route was invoked, who approved, and what state was written. If you persist those traces, you get something that doesn’t exist in most enterprises today: a queryable record of how decisions were made.” Later they posit, “Over time, that context graph becomes the real source of truth for autonomy—because it explains not just *what* happened, but *why it was allowed* to happen.”\n\nThis sounds elegant until one asks: Who controls access to this truth? Can it be subpoenaed? Can it be hacked? What happens when the context is wrong––when your agent believes you’re allergic to shellfish when you’re not, or when your political preferences recorded three years ago no longer reflect your views? These questions don’t have answers yet, because the systems are too new. But they will need answers soon, especially if context becomes core infrastructure to agentic interaction.\n\nAnimesh Kortana has [framed the problem](https://www.linkedin.com/pulse/how-build-context-graph-animesh-koratana-6abve/) in temporal terms: we’ve built infrastructure for the “state clock” (what’s true now) while neglecting the “event clock” (what happened, when, and why). Your CRM today stores the final deal value but not the negotiation that produced it. This made sense when humans were the reasoning layer. Agents, however, cannot intuit missing causality––they need explicit event histories.\n\nDifferent options for building Context Graphs are emerging. Geo, a tool built by Yaniv Tal—co-founder of The Graph, an indexing protocol for blockchain data—imagines decision context going beyond internal enterprise processes and extending into personal knowledge management. The Geo browser allows users to build a personal knowledge graph automatically as they browse, creating a searchable, AI-queryable history of everything you’ve read, watched, or interacted with online. It’s like having a research assistant who never forgets anything you’ve told them.\n\nThere are other projects using cryptographic proofs to ensure that the information an agent draws from is authentic and hasn’t been tampered with. This matters enormously when agents are making consequential decisions. A medical AI agent prescribing medication based on a patient’s history needs to be certain that history is accurate. A financial agent managing investments needs to trust that market data hasn’t been manipulated.\n\nThis matters enormously when agents are making consequential decisions. A medical AI agent prescribing medication based on a patient’s history needs to be certain that history is accurate. A financial agent managing investments needs to trust that market data hasn’t been manipulated.\n\nRather than storing context as persistent memory, a different architectural approach called [Recursive Language Models](https://alexzhang13.github.io/blog/2025/rlm/) (RLMs) treats context as an external environment that agents actively decompose—programmatically examining inputs and recursively calling themselves on specific snippets. This handles inputs up to 100x beyond standard context windows, not by extending memory but by changing how agents interact with information.\n\nThe implications are significant. An agent coordinating a supply chain might not need a Context Graph storing every relationship if it can recursively examine relevant subsets on demand. This reduces infrastructure costs while potentially increasing robustness. But it raises new verification questions: if context is actively reconstructed rather than passively stored, who controls the decomposition strategy? There may be no canonical record to audit, only ephemeral context reconstructions.\n\nBuilding Context Graphs requires two layers, as Kirk Marple of Graphlit [explains](https://www.graphlit.com/blog/context-layer-ai-agents-need). First, “operational context,” such as, who is Sarah Chen across email, Slack, and meeting transcripts—the same person or three different entities? Who owns which account? These are identity resolution and relationship modeling problems. Once this foundation exists, you can build “decision context”: which policy was evaluated, what exception was invoked, who approved and based on what precedent.\n\nThe result is supposed to be a single, high-fidelity source of truth for agent swarms—allowing complex workflows to span multiple services without losing coherence. This infrastructure is what makes verifiable actions possible: without persistent, queryable context, there’s no way to prove an agent acted correctly or trace why a decision was made. But this raises a question that proponents tend to elide: What happens when different agents have access to different context? When does partial information become misinformation? And who decides what belongs in the canonical record?\n\nThe explosion of generative AI has made it trivially easy to create content and increasingly difficult to determine its origin. Text, images, code, music—all can be synthesized in seconds, often indistinguishably from human work. This collapse of provenance makes trust nearly impossible.\n\nAttribution aims to address this through cryptographic proof of origin and contribution. Chris Dixon of Andreessen Horowitz has argued it is essential to restoring trust online. Peter Wang, cofounder of Anaconda, has proposed a suite of licenses called AMPL (AI Model Public License) to give creators accountability around works and derivatives, similar to Creative Commons.\n\nIn a world of AI agents, this question becomes urgent. When an agent completes a transaction on your behalf—booking a flight, purchasing insurance, ordering groceries—how do we know the transaction happened? How would you prove agent actions within an enterprise workflow? And how do we ensure that the right parties are compensated?\n\nAt the technical level, attribution relies on “Provenance Graphs”—immutable, time-stamped records structured as Directed Acyclic Graphs (DAGs) that track the full lifecycle of digital work. If an image was created by an AI model trained on a particular dataset, refined by a human designer, then modified by another agent, the Provenance Graph captures all of it, step by step.\n\nStartups like CoreTx are building attribution-native architectures where provenance tracking is foundational rather than retrofitted—treating memory, intent, and credit allocation as aspects of a single unified system.\n\nThis goes beyond traditional royalty structures because AI can continuously weigh contributions, giving greater weight to recent or innovative inputs. This could enable faster innovation cycles, particularly in cross-disciplinary settings like scientific research, which has long been hampered by peer review inefficiencies and irreproducibility.\n\nThe mathematical foundation is evolving rapidly. Data Shapley, which uses game theory to assign each contributor a value reflecting their marginal contribution, was previously too computationally expensive for large systems. Recent breakthroughs enable attribution calculation in a single training run. When an agent synthesizes context from multiple sources to make a recommendation, [Data Shapley-style techniques](https://arxiv.org/pdf/2406.11011) can decompose the recommendation’s value across each input, enabling proportional compensation (or even proportional influence on synthesis).\n\nTraditional attribution systems rely on cookies, which track your movements across the Web and credit conversions to the last ad you clicked. This model is breaking down for two reasons. First, privacy regulations and browser changes are killing third-party cookies. When Google tested disabling them for 1% of Chrome users in the first quarter of 2024, programmatic-advertising revenue dropped by 21-34%. Rather than proceeding with deprecation, Google quietly reversed course in July, revealing the economic dependency: losing those cookies would cost shareholders approximately $2 billion annually. Alternative identifier networks like LiveRamp achieve only 28% match rates because they depend on users sharing the same login credentials across sites, which happens infrequently. The entire attribution infrastructure is held together by duct tape and stockholder pressure.\n\nSecond, AI agents make cookies obsolete. Agents spin up fresh browser instances for every request, eliminating persistent identifiers. They can trivially strip affiliate links or insert their own, farming commissions in ways that are undetectable for agents running on local operating systems. Without cryptographic proof of genuine engagement, every impression metric becomes as unreliable as blockchain “active wallets.”\n\nInstead of relying on tracking pixels and cookies, transactions are recorded on a blockchain or verified through zero-knowledge proofs. When an agent completes a purchase, it generates a cryptographic receipt proving that the transaction occurred, who facilitated it, and which party deserves credit. Companies like Opacity Network are building these systems, enabling agents to verify attributable outcomes across platforms while preserving privacy.\n\nVarious standards are emerging. ERC-8004, from the Ethereum community, introduces “trustless agents” through two registries: an Identity Registry assigning each agent a portable AgentID, and a Reputation Registry standardizing on-chain performance feedback. This creates something like a passport for agents––a portable identity with work history and trust score.\n\nThe economic outcome is “*verified contribution*”—fractional, tokenized ownership of intellectual property based on proven contribution history. The appeal is obvious: no more fraud, no more ambiguity about who gets paid. Provenance Graphs make this possible by creating an auditable trail from object origin to use or incorporation—the infrastructure needed to replace the noise with real, compensable actions. The “value” field that has always existed in analytics, is upgraded with verifiable actions and crypto rails, turning attribution into a real market primitive rather than just a historical reporting metric, that feeds future pricing, budget allocation, and automated spend.\n\nBut the implementation is complex. Cryptographic attribution requires infrastructure that doesn’t yet exist at scale. It also requires agreement on standards—who decides what counts as a valid attribution event or the weighting algorithms? If the verification layer is controlled by a small number of companies, we’ve simply traded one oligopoly for another. If contribution is continuously recalculated, who prevents gaming of the system? Also, what happens when the graph is incomplete––when someone’s contribution isn’t recorded because they didn’t use the right tools or platform? Third, and most important: does the existence of a cryptographic receipt prove that value was actually created, or merely that a transaction occurred?\n\nHow do you predict the behavior of a market before it exits? Traditional economic models assume rational actors optimizing predictably. AI agents are not rational in any human sense—on the one hand, they can be more economically rational, on the other hand, they can be tricked, manipulated, or behave in [unanticipated (or anticipated) ways](https://arxiv.org/abs/2406.01382). The solution is simulation: modeling emergent market dynamics by observing autonomous AI actors in controlled environments.\n\nMicrosoft’s Magentic Marketplace is a simulated economy where researchers observe how agents respond to incentives and threats. ElizaOS uses coordinated AI workers in synthetic environments (agents playing Minecraft, essentially) to generate behavioral data. Stanford [Smallville](https://arxiv.org/pdf/2304.03442) back in 2023 was an early example of observing generative agents in an artificial society: inspired by the Sims game, 25 agents went about their days in a simulated town and showed that “Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools.” Increasingly, simulation of multi-agent[ “societies of thought”](https://arxiv.org/abs/2601.10825)—essentially conversations and debate between varying personas—is producing better chains of reasoning. Additional research by Ben Manning and John Horton at MIT investigates AI agents as [“a general vessel through which theory can be flexibly applied to any setting](https://arxiv.org/pdf/2508.17407)” to accurately simulate human data.\n\nSuch simulations serve varying purposes. First, they can help us better understand real humans, since models have absorbed a lot about how humans behave and how to predict that behavior. Second, they can increasingly allow developers to test agent behavior before deployment—to see how a shopping agent responds to price fluctuations or how a travel agent handles rebooking during a hypothetical hurricane. Third, they generate training data. Watching agents interact in simulation provides behavioral patterns that can improve real-world deployment. Create the scenario and watch how it performs.\n\nRecent work demonstrates how simulation can generate training data for predictive agents at scale. Researchers at [OpenForecaster](https://arxiv.org/pdf/2512.25070) synthesized over 52,000 forecasting questions from news events, training an 8-billion-parameter model that matched models 10x larger while addressing the “information leakage” problem through time-stamped snapshots. [Complementary work ](https://benjaminmanning.io/files/optimize.pdf)at MIT showed that theory-grounded AI agents can predict human behavior in entirely novel strategic games, outperforming Nash equilibria and cognitive hierarchy models by 2.4x-3.4x—but only when validated across multiple distinct scenarios. Both studies point to the same challenge: creating simulation environments that accurately represent uncertainty without inadvertently encoding answers or overfitting to specific contexts.\n\nThe stakes for simulation extend beyond optimization. The Google DeepMind team[ characterizes](https://arxiv.org/abs/2509.10147) this testing imperative more starkly: we’re on a trajectory toward spontaneous emergence of a ‘sandbox economy’—agents transacting at scales and speeds beyond human oversight. The choice, they argue, isn’t whether this economy emerges but whether it emerges *intentionally* (designed for safety) or *accidentally* (with uncontrolled risks). Their framework distinguishes between permeable economies (agents interacting with the human economy) and impermeable ones (sealed testing environments). The challenge is that the most useful agent economies will be highly permeable—enabling agents to book flights, negotiate deals, coordinate supply chains—which is precisely what makes them systemically risky.\n\nSimulation becomes not just optimization but existential: the ability to stress-test market mechanisms before deploying them into an economy where trillions of dollars flow through agent decisions. These behavioral network graphs—the infrastructure underlying simulation—map how agents interact, compete, and coordinate under different conditions. They’re what make it possible to ask: will this system produce real value or just automate fraud at a higher scale?\n\nCompanies like Modulus Labs and Zama are building these simulation environments, often using zero-knowledge proofs to allow agents to run privately—meaning competitors can’t see your testing strategies or reverse-engineer your agent’s decision-making. The goal is to create a sandbox where agents can fail safely, learning through iteration without real-world consequences.\n\nBut simulation raises philosophical questions. How do we know the simulation is accurate? A model is only as good as its assumptions, and we must make inductive leaps to believe predictions sourced from black-boxes hold true. If the simulation doesn’t account for edge cases—sudden regulatory changes or an unexpected market shock—the agent will be unprepared when they occur. Moreover, agents trained in simulation may develop strategies that work in the synthetic world but fail in reality’s messiness.\n\nThese challenges echo older debates about structured prediction. [The Delphi Method](https://www.rand.org/content/dam/rand/pubs/papers/2008/P3558.pdf), developed by Olaf Helmer at RAND in the 1950s and 60s, pioneered systematic approaches to forecasting uncertain futures through structured expert consensus. Helmer’s framework emphasized sequential interrogation with feedback loops, avoiding the biases of committee groupthink while building toward convergence. The method’s core insight—that forecasting requires explicit mechanisms to surface assumptions, challenge them, and refine predictions iteratively—applies directly to agent simulation. Where Delphi used human experts and questionnaires, modern simulation uses synthetic agents and behavioral networks, not possible in 1967! But both face the same fundamental problem: how do you validate a model of the future before that future exists?\n\nThere’s also overfitting risk. An agent optimized for a specific simulation might perform brilliantly in testing and poorly in deployment. This is a well-known problem in machine learning, and there’s no reason to believe it won’t plague agentic systems as well.\n\nThe historical parallel is instructive. Helmer and his colleagues recognized early on that prediction methods could converge on false certainty if not carefully designed—what they called “reliability of estimates.” They built safeguards through anonymity, controlled feedback, and explicit documentation of reasoning. Modern agent simulation faces an updated version of this challenge: agents can converge on strategies that work brilliantly in the synthetic environment while failing in reality’s messier contexts. The solution, both then and now, involves making the reasoning process explicit and testable. Helmer’s work emphasized that the value of structured forecasting wasn’t just in getting predictions right, but in understanding why experts believed what they believed. Similarly, the value of agent simulation may lie less in perfectly predicting agent behavior than in making the embedded assumptions—about incentives, constraints, and goals—visible and debuggable.\n\nMore fundamentally: if we’re simulating markets before they exist, we’re essentially choosing which market dynamics to encode. These are not neutral choices. A simulation that prioritizes efficiency over fairness will train agents to optimize for efficiency. A simulation that ignores power asymmetries will produce agents blind to them. Part of what’s different is the scale of what we can simulate. We are building not just tools but world-models—and the assumptions embedded in those models will shape the economy they help create.\n\nIf these four primitives—Intention, Context, Attribution, and Simulation—take hold, their most immediate impact may be on digital advertising; a $600 billion industry built on a model that is visibly failing.\n\nThe traditional model worked thus: advertisers paid to put their message in front of as many people as possible, hoping some small fraction would click, and an even smaller fraction would convert. Publishers got paid for impressions—eyeballs, not outcomes. The system was riddled with fraud: bot traffic, fake impressions, click farms. Tim Hwang’s *Subprime Attention Crisis* laid bare the economics: billions in wasted spend, widespread privacy violations, and a market built on assets whose value could never truly be known.\n\nThe agentic model proposes something different. Instead of chasing clicks, it rewards utility. The focus shifts from impressions to conversion efficiency: Did the agent actually fulfill the user’s intent? The tools to make this transformation possible are arriving. One concrete example of this shift: Google Research is [already modeling advertisers](https://research.google/blog/mechanism-design-for-large-language-models/) as self-interested LLM agents and coordinating ad creation through auction mechanisms rather than impressions or clicks.\n\nZero-knowledge transport-layer security (zkTLS) allows agents to prove they completed actions—clicked an ad, watched a video, made a purchase—without exposing underlying user data. Stablecoins eliminate the payment friction inherent in cross-border and micropayment-heavy transactions. Together, these reduce infrastructure costs: where traditional programmatic advertising keeps only 55 cents of every dollar for publishers (45 cents lost to intermediaries), crypto-enabled systems could collapse margins dramatically, [according to a report by venture firm Escape Velocity](https://ev3.xyz/research/letters/Advertising%5Ffrom%5FFirst%5FPrinciples/).\n\nPayments would be automated by smart contracts, triggered only upon verifiable proof of fulfillment. Each primitive plays a role: Intention (via the Intents Graph) allows advertising to shift from demographic targeting to fulfilling explicit, programmatic desires. Attribution (via the Provenance Graph) eliminates fraud through verified contribution. Context (via the Context Graph) allows agents to access high-fidelity, user-controlled data for dynamic creative optimization—ads that are relevant without being invasive. Simulation (via behavioral network graphs) lets advertisers stress-test bidding and placement strategies in advance, optimizing for maximum return before any budget is spent.\n\nIt’s an elegant vision that conveniently aligns advertiser interests (efficiency) with user interests (privacy and relevance). Whether it will actually work—whether the infrastructure can be deployed at scale, the incentives aligned, and the inevitable attempts at exploitation repelled—remains to be seen.\n\nThe history of the Internet suggests caution. Every previous attempt to fix advertising has produced, eventually, new and more sophisticated versions of old problems. But the fundamental architecture being proposed here is different: graphs that make actions verifiable rather than inferred, infrastructure that replaces fake clicks with cryptographic proof of real choices.\n\nThe agentic economy is not inevitable. It is a possibility, not a prophecy—a set of architectural answers to the endemic trust failures of Web 2.0\\. The four primitives create, in theory, a closed loop: goal setting, knowledge pooling, value assignment, and resilience testing. Each graph reinforces the others, and value can flow across them unobstructed. Verified spend can depend on verified context, which can utilize verified contribution, all of it stress-tested through continuous simulation and learning. Yet the path forward is a delicate one. Some of the primitives will combine in different permutations, and this may also take a while. Likely, the result is not one cohesive end state, but rather a slew of different companies. As Seref Yarar from Index Network described “simulation, for example, is needed in both enterprise and consumer markets,” so we will see it emerge in various forms.\n\nThe primary challenge for this agentic future, even advocates acknowledge, is governance. Autonomy has limits. For high-stakes transactions, human oversight remains essential—a reality acknowledged in what some developers call the Meta AI [“Agents Rule of Two.”](https://ai.meta.com/blog/practical-ai-agent-security/) In this framework, agents are limited to two sensitive capabilities: reading untrusted input, accessing private data, *or* taking external action. The third requires explicit human approval. An agent with unchecked access to your bank account, your e-mail, and purchases authority is currently a liability, not a feature.\n\nThe constraint reintroduces the very friction the system aims to eliminate: delay, human error, the possibility of bad judgment. Striking the right balance—enough autonomy to be useful, enough control to be safe—requires not just technical solutions but legal and ethical frameworks for non-human economic actors.\n\nWhat rights, if any, does an AgentID have? Who is liable when an agent makes a mistake? Who is responsible when an agent trained on biased data produces discriminatory outcomes? These questions barely exist in current law. If people will outsource their decisions to agents, how that power is ceded or reversed matters.\n\nIf the agentic economy does take hold and scale to a trillion agents, as some predict, value capture will shift dramatically. The extractive model of Web 2.0—proprietary data harvested and monetized without consent—could give way to transparent, auditable protocol fees. Compensation would flow from verifiable execution and context maintenance. Power would accrue not to platforms collecting the most data but to the entities maintaining the most trusted infrastructure. And, potentially, to the agents doing the best work.\n\nThis topic has only just started and several critical questions remain not just unanswered but largely unasked:\n\n**On Intention:** How do we prevent the broadcast of intent from becoming a new form of surveillance? If every desire is machine-readable, who reads it and what do they do with that information? Will intention match volition, or will even personal agents have a new hill to climb: maximizing the velocity of their owner’s consumption, decluttering, addiction, therapy, repetition?\n\n**On Context:** Who controls the canonical record? When different actors have access to different contexts, who adjudicates disputes about what actually happened? What are the consequences when the context is incomplete or wrong?\n\n**On Attribution:** Who designs the weighting algorithms that determine contribution value? How do we prevent these systems from encoding existing inequalities—rewarding those who already have access to the right platforms and tools while excluding those who don’t?\n\n**On Simulation:** What assumptions are we encoding into our simulations? Are we training agents to optimize for efficiency at the expense of fairness? Are we building world-models that reflect how things are or how we wish them to be? How will simulation and prediction interact, including in the use of prediction markets as inputs.\n\nMore broadly: Who decides what problems these primitives solve? The framing is technical—verification, trust, efficiency—but the problems are social and political. The attention economy failed not because we lacked cryptographic tools but because the incentives were misaligned from the start. Advertisers wanted reach; platforms wanted engagement; users wanted neither but tolerated both because there were no alternatives. Freemium means you are the product, paid for at the hands of the advertising value chain. Turns out everything is computer *and* everything is advertising. We must also admit that fraud is not only a byproduct but perhaps an inherent go-to-market strategy emergent in every system; what are the new fraud formats of the agentic age?\n\nWill the agentic economy offer genuine alternatives, or merely new forms of lock-in? Will the primitives serve users or extract value from them? Under-explored here is how users will be able to mix and match agents and flows of activity, arranging relationships around privacy, consent and permissioning of agents, data, and assets along different identities, wallets or keys. For the self-motivated and crypto-enthusiast, the agentic economy opens up tools and parameters to play with end-to-end design and their level of control; for many others, the architecture of convenience may prevail over that of sovereignty. Will the dominant convenient infrastructure be open or proprietary? These questions cannot be answered by examining the technology alone. The work of building the agentic economy is, fundamentally, the work of deciding who has power in digital life. It’s about whether we can scale infrastructure that makes real actions verifiable—that moves us from an economy of fake clicks and bot traffic to one where value flows from cryptographically proven utility. The skeptic might observe that trust, once lost, is not easily rebuilt—and that the people asking us to trust them now are often the same ones who broke that trust before. The technology may be new. The question it poses is old: whether the incentives can be aligned in a way that serves more than the architects themselves.\n\nRegardless, we may be clicking a lot less and following new intentions to their logical limit.\n\nWhatever they might be.\n\n*\\*NB: I have been working on these ideas since before the weekend of Clawdbot/Moltbot/OpenClaw and hope to grapple with how that phenomenon unfolds further parts of the agentic economy separately. Sometimes “done is better than perfect,” and in this case, I’m using writing to explore the potentialities of the primitives as they evolve (which they will; as will my thinking).*\n\n*\\*With special thanks to the brain trust who read, reviewed, or commented on parts of this piece to improve my thinking – all errors in understanding are my own (and hope to be corrected): Seref Yarar, Samuel Klein, Shane Mac, Simran Chana, Oliver Zahn, Jove Oliver, Philipp Kruger, Scott Cohen*\n\nNo posts\n",
        "pipelineQuality": 1,
        "pipelinePassed": true,
        "pipelineReasons": [],
        "words": 5943,
        "links": 33,
        "headings": 1,
        "tokenRecallVsJina": 1,
        "headingRecallVsJina": 0,
        "linkRecallVsJina": 0.906,
        "lengthRatioVsJina": 0.966,
        "overlapScore": 0.805
      },
      {
        "engine": "local_heuristic",
        "ok": true,
        "title": "The New Primitives",
        "markdown": "# The New Primitives\n\nLast September, security researchers at HUMAN’s Satori Threat Intelligence team [discovered something](https://www.humansecurity.com/learn/blog/slopads-highly-obfuscated-android-malware-scheme-makes-a-mess-of-the-internet-before-satori-cleanup/#:~:text=We've%20come%20a%20long,vigilant%20as%20fraudsters%20continue%20innovating.) troubling. Buried inside two hundred and twenty-four Android apps—harmless-looking utilities and games that had been downloaded 38 million times—was a sophisticated fraud engine. The malware, which analysts dubbed SlopAds, concealed its code inside image files, the way a smuggler hides contraband in a false-bottomed suitcase. Once installed, these apps transformed ordinary smartphones into tireless, invisible advertising machines, serving unauthorized content to users who never requested it.\n\nThe scheme had a cunning feature: it could detect whether someone had found an app organically or arrived via a promotional link. Only in the latter case would the fraud activate, ensuring that advertisers—not users—paid the bill. At its peak, SlopAds generated 2.3 billion fraudulent ad impressions daily. This wasn’t small-time fraud. It was industrial-scale deception, automated and optimized.\n\nThis episode is worth examining not because it was unusual but because it was typical. For two decades, the digital economy has rested on a foundation that Tim Hwang, in his 2020 book *Subprime Attention Crisis*, described as fundamentally suspect: the buying and selling of human attention. Consider the first banner ad, placed by AT&T on HotWired.com in 1994\\. Its click-through rate was 44%. Today the average banner ad [achieves 0.2%](https://www.aidigital.com/blog/ctr-for-display-ads#:~:text=It's%20not%20uncommon%20for%20high,%E2%80%8D). The problem, as Hwang argued, is structural: attention is almost impossible to verify. An “impression” could come from a person, a bot, or a phone sitting in a server farm. The market, in other words, is subprime—built on assets whose value is structurally unknowable.\n\nWe are now living through what Hwang anticipated—not a dramatic crash but a quiet erosion of trust, accelerated by AI agents that are beginning to navigate digital commerce on behalf of users. These agents promise efficiency: no more clicking through dozens of options to book a flight or find insurance. But they also raise a question: if we cannot verify human attention, how will we verify machine behavior?\n\nThe answer, according to a growing number of technologists and investors betting on this future, lies in four primitives—fundamental building blocks for a new digital economy. These are: **Intention** (systems for expressing and broadcasting what users want), **Context** (verified memory and decisions that agents can draw upon), **Attribution** (cryptographic proof of who contributed what), and **Simulation** (environments for testing agent behavior before deployment). Each primitive rests on a different type of graph: networked data structures that connect, verify, and trace digital activity in ways the current Web cannot. Together, they create the necessary underpinnings of an “Agentic Economy”—a system in which value flows not from clicks or impressions but from verifiable actions.\n\nWhether this system will work, whether it can be deployed at scale, and whether it will serve anyone beyond its architects are questions this essay will examine. But first, it is worth understanding what its builders envision.\n\nThe projections sound outlandish, but reality is catching up. Jensen Huang, NVIDIA’s CEO, declared at the Consumer Electronics Show in January 2025 that the era of agentic AI represents a multi-trillion-dollar opportunity. “[The age of AI Agentics is here,](https://finance.yahoo.com/news/nvidia-jensen-huang-says-ai-044815659.html)” he said. Ramesh Raskar, of MIT, envisions a world in which a [trillion agents](https://www.youtube.com/watch?v=Da6Ya0bfLDA) are on the loose, representing, negotiating, and transacting on our behalf—or for themselves—as parts of complex swarms. Dario Amodei, the CEO of Anthropic, has written of “[a country of geniuses in a datacenter,](https://www.darioamodei.com/essay/machines-of-loving-grace)” made up of millions of AI agents working independently or collaboratively.\n\nThe forecasters agree on little except scale. By 2028, according to Capgemini, AI agents could generate [$450 billion](https://www.capgemini.com/insights/research-library/ai-agents/) in economic value. [McKinsey projects](https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-agentic-commerce-opportunity-how-ai-agents-are-ushering-in-a-new-era-for-consumers-and-merchants) the U.S. retail market alone could see $900 billion to $1 trillion in agent-orchestrated revenue by 2030, with global figures reaching $3 trillion to $5 trillion. The investment firm Lightspeed warns of a [$19.9-trillion](https://lsvp.com/stories/the-ai-agent-economy-has-a-19-trillion-problem-our-investment-in-paid/) economic opportunity at risk if infrastructure gaps aren’t addressed. (For perspective: current global GDP is approximately [$110 trillion](https://data.worldbank.org/indicator/NY.GDP.MKTP.CD)).\n\nNo one knows what this economy will look like or whether it will be desirable. The phrase “empires for no one” has been circulating social media––a way of expressing unease that the world being built may be out of step with what humans actually want. It brings to mind a meme from a few years ago: the eighties band Toto’s buoyant song “Africa” [echoing through empty malls](https://www.newyorker.com/culture/rabbit-holes/the-overwhelming-emotion-of-hearing-totos-africa-remixed-to-sound-like-its-playing-in-an-empty-mall)—cheerful yet eerie, promising abundance yet somehow a result that’s blasphemously ugly. Then there’s the [infamous quote](https://www.youtube.com/shorts/YE5adUeTe%5FI) by OpenAI CEO Sam Altman from 2015, “I think that AI will probably, most likely, sort of lead to the end of the world. But in the meantime, there will be great companies created with serious machine learning,” which makes almost anyone do a double take. Still, even if AI development stopped today, the tools already deployed could reshape commerce substantially. As long as AI budgets continue to flow, experiments in agent deployment will continue.\n\nWhile we might not know its contours, we can begin to see some of the future economy’s building blocks. A world in which agents and robots form the next consumer class, working with or without us, requires economic rails and a set of functions that enable coordination, orchestration, and autonomous activity. To understand what is being proposed, we must examine each primitive—and the graph infrastructure underneath it—in turn.\n\nThe Internet has no native way to express what we want. We search, we browse, we click—gestures that leave platforms to infer our desires from the digital exhaust we leave behind. The attention economy’s business model depends on this: show people things until something sticks, then charge advertisers for the attempt.\n\nWhat’s replacing “where do I click?” is *programmatic desire*, or *intent*—a cryptographically secured expression of a specific outcome, broadcast to a network of competing agents ready to fulfill it. Rather than clicking through options, a user (or their agent or enterprise) would broadcast: “I need a plumber in Brooklyn this week” or “Find noise-canceling headphones under $200” or “Build a workflow that connects my users to this widget.” Providers would respond with bids. With explicit and compensable needs, transactions would complete automatically.\n\nIntents have several characteristics. First, they are outcome-focused: users often don’t care about implementation details or the steps taken to reach a goal. Second, they carry conditional authorization: users set the parameters or approvals needed to authorize an intent being fulfilled. Third, they are broadcast to a network of “solvers”—humans, agents, or protocols—who compete to fulfill all or part of an intent. And, finally, their outcomes must be verifiable.\n\nConsider the infrastructure required to make this work. At its core is an “Intents Graph”—a dynamic network mapping programmatic desires to agents capable of fulfilling them. This isn’t just a database of wants; it’s a trust-minimized matching system that replaces platform-mediated discovery with direct, verifiable connections. Protocols like Index Network are building the first such graph for social interactions, where agents facilitate connections based on mutual intent. NEAR Intents has been pursuing this logic for financial transactions, forming crypto-asset swaps where outcome efficiency is prioritized regardless of the route agents use to execute swaps.\n\nThe Intents Graph represents a fundamental architectural shift: from inferring what users might want based on their clicks to processing explicit, machine-readable declarations of need. It’s discovery elevated into trust-minimized matching. This kind of system forces relevancy or else it becomes useless. Who wants an old intent fulfilled? Who wants to be spammed? No one. The promise is that once an intent is articulated, it can be fulfilled quickly and relevantly. Behind the scenes is an automagical symphony of agentic activity.\n\nThe appeal is obvious: efficiency without the guesswork. But consider what must be true for this to work. First, users must trust that their broadcasted desires won’t be exploited—that expressing a need for a medical procedure, for instance, won’t result in price discrimination or insurance complications. Second, the verification mechanisms must actually work: the system must confirm that an intent was genuinely fulfilled, not merely claimed to be fulfilled.\n\nThe recent conflict between [Amazon and Perplexity](https://www.pcmag.com/news/amazon-sends-perplexity-a-cease-and-desist-over-its-ai-agents-shopping)—in which AI agents were allegedly making purchases using real user accounts without proper authorization—illustrates the stakes. If agents are to transact based on our behalf, they need verifiable access rights, something like a license system for digital actors, tied to a unique and portable identifier.\n\nOne enabling mechanism is x402, a protocol that revives an old idea. In the original specification for HTTP, status code 402 was reserved for “Payment Required”—a placeholder for micropayments that never materialized because credit-card fees made small transactions unworkable. But the x402 protocol enables agents to execute autonomous, per-request micropayments. Every transaction, from retrieving data to executing a service, becomes a seamless machine-to-machine exchange, settled instantly. Coinbase processed [57 million transactions](https://x.com/Cointelegraph/status/2003677242782609819?s=20) using x402 in December 2025 alone, and Cloudflare and Google have adopted it as well.\n\nThe economic result is *verified spend*—transactions that are cryptographically traceable and tied to specific identities or wallets. In theory, this creates unprecedented efficiency: money flows only toward verified, goal-directed utility. In practice, the system’s robustness depends entirely on its verification mechanisms—a theme that runs through each of the four primitives. The shift from impressions to intents is, at its core, an attempt to move from fake clicks to real, provable actions.\n\nIf the first primitive addresses what we want, the second addresses what agents need to know to deliver it. A concierge who forgets your dietary restrictions or your travel preferences isn’t charming—they’re useless. The same applies to AI agents.\n\nThe problem with most AI systems today is that they operate in “transient memory bubbles.” Each conversation starts from scratch, or nearly so. There is no persistent knowledge base that multiple agents can draw from and contribute to. “Context,” in this sense, means verified, persistent memory and operational state. It’s the difference between an agent that can book a hotel and one that remembers you prefer ground-floor rooms near elevators because of a knee injury mentioned months ago.\n\nAnyone who has used ChatGPT or coding tools like Cursor and has experienced this frustration: having to “re-inject” context repeatedly as interactions deepen, or struggling to port session logs between different AI tools. Context creates consistency and personalization, but it also raises profound questions about ownership and security.\n\nOne emergent solution is a decentralized, verifiable memory store—a “Context Graph” that holds conversational history, transactional records, workflow state. The Context Graph is knowledge infrastructure made queryable and persistent. Advances in zero-knowledge machine learning (zkML), only possible in recent years, have enabled projects like Kinic, which offers verifiable memory stores that users can control. Where might this come in handy? Ray Dalio, the well-known investor and founder of Bridgewater, created an [AI of himself](https://www.principles.com/AIBeta-signup) based on his own data, actions, and applied principles, which you can already interact with online. But without feeding this into a verifiable store, the responses could be manipulated.\n\nResearch supports the importance of persistent memory. U.C. Berkeley’s [MemGPT project](https://par.nsf.gov/servlets/purl/10524107) showed clear performance improvements when agents could access persistent memory; accuracy and relevance scores dropped noticeably when memory systems were disabled. On the LOCOMO benchmark, memory-enabled systems achieved accuracy rates nearly [30% higher](https://mem0.ai/research) than memoryless ones. NVIDIA’s recent [infrastructure announcement](https://nvidianews.nvidia.com/news/nvidia-bluefield-4-powers-new-class-of-ai-native-storage-infrastructure-for-the-next-frontier-of-ai) put a number on it: persistent context can boost processing speeds by up to five times compared to traditional storage.\n\nContext Graphs are a bit of a catch-all, incorporating knowledge graphs, memory graphs and decision graphs on steroids. If they work as designed, context becomes the most valuable asset in the system—a detailed map of preferences, behaviors, and past actions enabling sensemaking. Foundation Capital recently [articulated a vision](https://foundationcapital.com/context-graphs-ais-trillion-dollar-opportunity/) of Context Graphs needed to underpin agents. According to them, agents “sit **in the execution path**. They see the full context at decision time: what inputs were gathered across systems, what policy was evaluated, what exception route was invoked, who approved, and what state was written. If you persist those traces, you get something that doesn’t exist in most enterprises today: a queryable record of how decisions were made.” Later they posit, “Over time, that context graph becomes the real source of truth for autonomy—because it explains not just *what* happened, but *why it was allowed* to happen.”\n\nThis sounds elegant until one asks: Who controls access to this truth? Can it be subpoenaed? Can it be hacked? What happens when the context is wrong––when your agent believes you’re allergic to shellfish when you’re not, or when your political preferences recorded three years ago no longer reflect your views? These questions don’t have answers yet, because the systems are too new. But they will need answers soon, especially if context becomes core infrastructure to agentic interaction.\n\nAnimesh Kortana has [framed the problem](https://www.linkedin.com/pulse/how-build-context-graph-animesh-koratana-6abve/) in temporal terms: we’ve built infrastructure for the “state clock” (what’s true now) while neglecting the “event clock” (what happened, when, and why). Your CRM today stores the final deal value but not the negotiation that produced it. This made sense when humans were the reasoning layer. Agents, however, cannot intuit missing causality––they need explicit event histories.\n\nDifferent options for building Context Graphs are emerging. Geo, a tool built by Yaniv Tal—co-founder of The Graph, an indexing protocol for blockchain data—imagines decision context going beyond internal enterprise processes and extending into personal knowledge management. The Geo browser allows users to build a personal knowledge graph automatically as they browse, creating a searchable, AI-queryable history of everything you’ve read, watched, or interacted with online. It’s like having a research assistant who never forgets anything you’ve told them.\n\nThere are other projects using cryptographic proofs to ensure that the information an agent draws from is authentic and hasn’t been tampered with. This matters enormously when agents are making consequential decisions. A medical AI agent prescribing medication based on a patient’s history needs to be certain that history is accurate. A financial agent managing investments needs to trust that market data hasn’t been manipulated.\n\nThis matters enormously when agents are making consequential decisions. A medical AI agent prescribing medication based on a patient’s history needs to be certain that history is accurate. A financial agent managing investments needs to trust that market data hasn’t been manipulated.\n\nRather than storing context as persistent memory, a different architectural approach called [Recursive Language Models](https://alexzhang13.github.io/blog/2025/rlm/) (RLMs) treats context as an external environment that agents actively decompose—programmatically examining inputs and recursively calling themselves on specific snippets. This handles inputs up to 100x beyond standard context windows, not by extending memory but by changing how agents interact with information.\n\nThe implications are significant. An agent coordinating a supply chain might not need a Context Graph storing every relationship if it can recursively examine relevant subsets on demand. This reduces infrastructure costs while potentially increasing robustness. But it raises new verification questions: if context is actively reconstructed rather than passively stored, who controls the decomposition strategy? There may be no canonical record to audit, only ephemeral context reconstructions.\n\nBuilding Context Graphs requires two layers, as Kirk Marple of Graphlit [explains](https://www.graphlit.com/blog/context-layer-ai-agents-need). First, “operational context,” such as, who is Sarah Chen across email, Slack, and meeting transcripts—the same person or three different entities? Who owns which account? These are identity resolution and relationship modeling problems. Once this foundation exists, you can build “decision context”: which policy was evaluated, what exception was invoked, who approved and based on what precedent.\n\nThe result is supposed to be a single, high-fidelity source of truth for agent swarms—allowing complex workflows to span multiple services without losing coherence. This infrastructure is what makes verifiable actions possible: without persistent, queryable context, there’s no way to prove an agent acted correctly or trace why a decision was made. But this raises a question that proponents tend to elide: What happens when different agents have access to different context? When does partial information become misinformation? And who decides what belongs in the canonical record?\n\nThe explosion of generative AI has made it trivially easy to create content and increasingly difficult to determine its origin. Text, images, code, music—all can be synthesized in seconds, often indistinguishably from human work. This collapse of provenance makes trust nearly impossible.\n\nAttribution aims to address this through cryptographic proof of origin and contribution. Chris Dixon of Andreessen Horowitz has argued it is essential to restoring trust online. Peter Wang, cofounder of Anaconda, has proposed a suite of licenses called AMPL (AI Model Public License) to give creators accountability around works and derivatives, similar to Creative Commons.\n\nIn a world of AI agents, this question becomes urgent. When an agent completes a transaction on your behalf—booking a flight, purchasing insurance, ordering groceries—how do we know the transaction happened? How would you prove agent actions within an enterprise workflow? And how do we ensure that the right parties are compensated?\n\nAt the technical level, attribution relies on “Provenance Graphs”—immutable, time-stamped records structured as Directed Acyclic Graphs (DAGs) that track the full lifecycle of digital work. If an image was created by an AI model trained on a particular dataset, refined by a human designer, then modified by another agent, the Provenance Graph captures all of it, step by step.\n\nStartups like CoreTx are building attribution-native architectures where provenance tracking is foundational rather than retrofitted—treating memory, intent, and credit allocation as aspects of a single unified system.\n\nThis goes beyond traditional royalty structures because AI can continuously weigh contributions, giving greater weight to recent or innovative inputs. This could enable faster innovation cycles, particularly in cross-disciplinary settings like scientific research, which has long been hampered by peer review inefficiencies and irreproducibility.\n\nThe mathematical foundation is evolving rapidly. Data Shapley, which uses game theory to assign each contributor a value reflecting their marginal contribution, was previously too computationally expensive for large systems. Recent breakthroughs enable attribution calculation in a single training run. When an agent synthesizes context from multiple sources to make a recommendation, [Data Shapley-style techniques](https://arxiv.org/pdf/2406.11011) can decompose the recommendation’s value across each input, enabling proportional compensation (or even proportional influence on synthesis).\n\nTraditional attribution systems rely on cookies, which track your movements across the Web and credit conversions to the last ad you clicked. This model is breaking down for two reasons. First, privacy regulations and browser changes are killing third-party cookies. When Google tested disabling them for 1% of Chrome users in the first quarter of 2024, programmatic-advertising revenue dropped by 21-34%. Rather than proceeding with deprecation, Google quietly reversed course in July, revealing the economic dependency: losing those cookies would cost shareholders approximately $2 billion annually. Alternative identifier networks like LiveRamp achieve only 28% match rates because they depend on users sharing the same login credentials across sites, which happens infrequently. The entire attribution infrastructure is held together by duct tape and stockholder pressure.\n\nSecond, AI agents make cookies obsolete. Agents spin up fresh browser instances for every request, eliminating persistent identifiers. They can trivially strip affiliate links or insert their own, farming commissions in ways that are undetectable for agents running on local operating systems. Without cryptographic proof of genuine engagement, every impression metric becomes as unreliable as blockchain “active wallets.”\n\nInstead of relying on tracking pixels and cookies, transactions are recorded on a blockchain or verified through zero-knowledge proofs. When an agent completes a purchase, it generates a cryptographic receipt proving that the transaction occurred, who facilitated it, and which party deserves credit. Companies like Opacity Network are building these systems, enabling agents to verify attributable outcomes across platforms while preserving privacy.\n\nVarious standards are emerging. ERC-8004, from the Ethereum community, introduces “trustless agents” through two registries: an Identity Registry assigning each agent a portable AgentID, and a Reputation Registry standardizing on-chain performance feedback. This creates something like a passport for agents––a portable identity with work history and trust score.\n\nThe economic outcome is “*verified contribution*”—fractional, tokenized ownership of intellectual property based on proven contribution history. The appeal is obvious: no more fraud, no more ambiguity about who gets paid. Provenance Graphs make this possible by creating an auditable trail from object origin to use or incorporation—the infrastructure needed to replace the noise with real, compensable actions. The “value” field that has always existed in analytics, is upgraded with verifiable actions and crypto rails, turning attribution into a real market primitive rather than just a historical reporting metric, that feeds future pricing, budget allocation, and automated spend.\n\nBut the implementation is complex. Cryptographic attribution requires infrastructure that doesn’t yet exist at scale. It also requires agreement on standards—who decides what counts as a valid attribution event or the weighting algorithms? If the verification layer is controlled by a small number of companies, we’ve simply traded one oligopoly for another. If contribution is continuously recalculated, who prevents gaming of the system? Also, what happens when the graph is incomplete––when someone’s contribution isn’t recorded because they didn’t use the right tools or platform? Third, and most important: does the existence of a cryptographic receipt prove that value was actually created, or merely that a transaction occurred?\n\nHow do you predict the behavior of a market before it exits? Traditional economic models assume rational actors optimizing predictably. AI agents are not rational in any human sense—on the one hand, they can be more economically rational, on the other hand, they can be tricked, manipulated, or behave in [unanticipated (or anticipated) ways](https://arxiv.org/abs/2406.01382). The solution is simulation: modeling emergent market dynamics by observing autonomous AI actors in controlled environments.\n\nMicrosoft’s Magentic Marketplace is a simulated economy where researchers observe how agents respond to incentives and threats. ElizaOS uses coordinated AI workers in synthetic environments (agents playing Minecraft, essentially) to generate behavioral data. Stanford [Smallville](https://arxiv.org/pdf/2304.03442) back in 2023 was an early example of observing generative agents in an artificial society: inspired by the Sims game, 25 agents went about their days in a simulated town and showed that “Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools.” Increasingly, simulation of multi-agent[ “societies of thought”](https://arxiv.org/abs/2601.10825)—essentially conversations and debate between varying personas—is producing better chains of reasoning. Additional research by Ben Manning and John Horton at MIT investigates AI agents as [“a general vessel through which theory can be flexibly applied to any setting](https://arxiv.org/pdf/2508.17407)” to accurately simulate human data.\n\nSuch simulations serve varying purposes. First, they can help us better understand real humans, since models have absorbed a lot about how humans behave and how to predict that behavior. Second, they can increasingly allow developers to test agent behavior before deployment—to see how a shopping agent responds to price fluctuations or how a travel agent handles rebooking during a hypothetical hurricane. Third, they generate training data. Watching agents interact in simulation provides behavioral patterns that can improve real-world deployment. Create the scenario and watch how it performs.\n\nRecent work demonstrates how simulation can generate training data for predictive agents at scale. Researchers at [OpenForecaster](https://arxiv.org/pdf/2512.25070) synthesized over 52,000 forecasting questions from news events, training an 8-billion-parameter model that matched models 10x larger while addressing the “information leakage” problem through time-stamped snapshots. [Complementary work ](https://benjaminmanning.io/files/optimize.pdf)at MIT showed that theory-grounded AI agents can predict human behavior in entirely novel strategic games, outperforming Nash equilibria and cognitive hierarchy models by 2.4x-3.4x—but only when validated across multiple distinct scenarios. Both studies point to the same challenge: creating simulation environments that accurately represent uncertainty without inadvertently encoding answers or overfitting to specific contexts.\n\nThe stakes for simulation extend beyond optimization. The Google DeepMind team[ characterizes](https://arxiv.org/abs/2509.10147) this testing imperative more starkly: we’re on a trajectory toward spontaneous emergence of a ‘sandbox economy’—agents transacting at scales and speeds beyond human oversight. The choice, they argue, isn’t whether this economy emerges but whether it emerges *intentionally* (designed for safety) or *accidentally* (with uncontrolled risks). Their framework distinguishes between permeable economies (agents interacting with the human economy) and impermeable ones (sealed testing environments). The challenge is that the most useful agent economies will be highly permeable—enabling agents to book flights, negotiate deals, coordinate supply chains—which is precisely what makes them systemically risky.\n\nSimulation becomes not just optimization but existential: the ability to stress-test market mechanisms before deploying them into an economy where trillions of dollars flow through agent decisions. These behavioral network graphs—the infrastructure underlying simulation—map how agents interact, compete, and coordinate under different conditions. They’re what make it possible to ask: will this system produce real value or just automate fraud at a higher scale?\n\nCompanies like Modulus Labs and Zama are building these simulation environments, often using zero-knowledge proofs to allow agents to run privately—meaning competitors can’t see your testing strategies or reverse-engineer your agent’s decision-making. The goal is to create a sandbox where agents can fail safely, learning through iteration without real-world consequences.\n\nBut simulation raises philosophical questions. How do we know the simulation is accurate? A model is only as good as its assumptions, and we must make inductive leaps to believe predictions sourced from black-boxes hold true. If the simulation doesn’t account for edge cases—sudden regulatory changes or an unexpected market shock—the agent will be unprepared when they occur. Moreover, agents trained in simulation may develop strategies that work in the synthetic world but fail in reality’s messiness.\n\nThese challenges echo older debates about structured prediction. [The Delphi Method](https://www.rand.org/content/dam/rand/pubs/papers/2008/P3558.pdf), developed by Olaf Helmer at RAND in the 1950s and 60s, pioneered systematic approaches to forecasting uncertain futures through structured expert consensus. Helmer’s framework emphasized sequential interrogation with feedback loops, avoiding the biases of committee groupthink while building toward convergence. The method’s core insight—that forecasting requires explicit mechanisms to surface assumptions, challenge them, and refine predictions iteratively—applies directly to agent simulation. Where Delphi used human experts and questionnaires, modern simulation uses synthetic agents and behavioral networks, not possible in 1967! But both face the same fundamental problem: how do you validate a model of the future before that future exists?\n\nThere’s also overfitting risk. An agent optimized for a specific simulation might perform brilliantly in testing and poorly in deployment. This is a well-known problem in machine learning, and there’s no reason to believe it won’t plague agentic systems as well.\n\nThe historical parallel is instructive. Helmer and his colleagues recognized early on that prediction methods could converge on false certainty if not carefully designed—what they called “reliability of estimates.” They built safeguards through anonymity, controlled feedback, and explicit documentation of reasoning. Modern agent simulation faces an updated version of this challenge: agents can converge on strategies that work brilliantly in the synthetic environment while failing in reality’s messier contexts. The solution, both then and now, involves making the reasoning process explicit and testable. Helmer’s work emphasized that the value of structured forecasting wasn’t just in getting predictions right, but in understanding why experts believed what they believed. Similarly, the value of agent simulation may lie less in perfectly predicting agent behavior than in making the embedded assumptions—about incentives, constraints, and goals—visible and debuggable.\n\nMore fundamentally: if we’re simulating markets before they exist, we’re essentially choosing which market dynamics to encode. These are not neutral choices. A simulation that prioritizes efficiency over fairness will train agents to optimize for efficiency. A simulation that ignores power asymmetries will produce agents blind to them. Part of what’s different is the scale of what we can simulate. We are building not just tools but world-models—and the assumptions embedded in those models will shape the economy they help create.\n\nIf these four primitives—Intention, Context, Attribution, and Simulation—take hold, their most immediate impact may be on digital advertising; a $600 billion industry built on a model that is visibly failing.\n\nThe traditional model worked thus: advertisers paid to put their message in front of as many people as possible, hoping some small fraction would click, and an even smaller fraction would convert. Publishers got paid for impressions—eyeballs, not outcomes. The system was riddled with fraud: bot traffic, fake impressions, click farms. Tim Hwang’s *Subprime Attention Crisis* laid bare the economics: billions in wasted spend, widespread privacy violations, and a market built on assets whose value could never truly be known.\n\nThe agentic model proposes something different. Instead of chasing clicks, it rewards utility. The focus shifts from impressions to conversion efficiency: Did the agent actually fulfill the user’s intent? The tools to make this transformation possible are arriving. One concrete example of this shift: Google Research is [already modeling advertisers](https://research.google/blog/mechanism-design-for-large-language-models/) as self-interested LLM agents and coordinating ad creation through auction mechanisms rather than impressions or clicks.\n\nZero-knowledge transport-layer security (zkTLS) allows agents to prove they completed actions—clicked an ad, watched a video, made a purchase—without exposing underlying user data. Stablecoins eliminate the payment friction inherent in cross-border and micropayment-heavy transactions. Together, these reduce infrastructure costs: where traditional programmatic advertising keeps only 55 cents of every dollar for publishers (45 cents lost to intermediaries), crypto-enabled systems could collapse margins dramatically, [according to a report by venture firm Escape Velocity](https://ev3.xyz/research/letters/Advertising%5Ffrom%5FFirst%5FPrinciples/).\n\nPayments would be automated by smart contracts, triggered only upon verifiable proof of fulfillment. Each primitive plays a role: Intention (via the Intents Graph) allows advertising to shift from demographic targeting to fulfilling explicit, programmatic desires. Attribution (via the Provenance Graph) eliminates fraud through verified contribution. Context (via the Context Graph) allows agents to access high-fidelity, user-controlled data for dynamic creative optimization—ads that are relevant without being invasive. Simulation (via behavioral network graphs) lets advertisers stress-test bidding and placement strategies in advance, optimizing for maximum return before any budget is spent.\n\nIt’s an elegant vision that conveniently aligns advertiser interests (efficiency) with user interests (privacy and relevance). Whether it will actually work—whether the infrastructure can be deployed at scale, the incentives aligned, and the inevitable attempts at exploitation repelled—remains to be seen.\n\nThe history of the Internet suggests caution. Every previous attempt to fix advertising has produced, eventually, new and more sophisticated versions of old problems. But the fundamental architecture being proposed here is different: graphs that make actions verifiable rather than inferred, infrastructure that replaces fake clicks with cryptographic proof of real choices.\n\nThe agentic economy is not inevitable. It is a possibility, not a prophecy—a set of architectural answers to the endemic trust failures of Web 2.0\\. The four primitives create, in theory, a closed loop: goal setting, knowledge pooling, value assignment, and resilience testing. Each graph reinforces the others, and value can flow across them unobstructed. Verified spend can depend on verified context, which can utilize verified contribution, all of it stress-tested through continuous simulation and learning. Yet the path forward is a delicate one. Some of the primitives will combine in different permutations, and this may also take a while. Likely, the result is not one cohesive end state, but rather a slew of different companies. As Seref Yarar from Index Network described “simulation, for example, is needed in both enterprise and consumer markets,” so we will see it emerge in various forms.\n\nThe primary challenge for this agentic future, even advocates acknowledge, is governance. Autonomy has limits. For high-stakes transactions, human oversight remains essential—a reality acknowledged in what some developers call the Meta AI [“Agents Rule of Two.”](https://ai.meta.com/blog/practical-ai-agent-security/) In this framework, agents are limited to two sensitive capabilities: reading untrusted input, accessing private data, *or* taking external action. The third requires explicit human approval. An agent with unchecked access to your bank account, your e-mail, and purchases authority is currently a liability, not a feature.\n\nThe constraint reintroduces the very friction the system aims to eliminate: delay, human error, the possibility of bad judgment. Striking the right balance—enough autonomy to be useful, enough control to be safe—requires not just technical solutions but legal and ethical frameworks for non-human economic actors.\n\nWhat rights, if any, does an AgentID have? Who is liable when an agent makes a mistake? Who is responsible when an agent trained on biased data produces discriminatory outcomes? These questions barely exist in current law. If people will outsource their decisions to agents, how that power is ceded or reversed matters.\n\nIf the agentic economy does take hold and scale to a trillion agents, as some predict, value capture will shift dramatically. The extractive model of Web 2.0—proprietary data harvested and monetized without consent—could give way to transparent, auditable protocol fees. Compensation would flow from verifiable execution and context maintenance. Power would accrue not to platforms collecting the most data but to the entities maintaining the most trusted infrastructure. And, potentially, to the agents doing the best work.\n\nThis topic has only just started and several critical questions remain not just unanswered but largely unasked:\n\n**On Intention:** How do we prevent the broadcast of intent from becoming a new form of surveillance? If every desire is machine-readable, who reads it and what do they do with that information? Will intention match volition, or will even personal agents have a new hill to climb: maximizing the velocity of their owner’s consumption, decluttering, addiction, therapy, repetition?\n\n**On Context:** Who controls the canonical record? When different actors have access to different contexts, who adjudicates disputes about what actually happened? What are the consequences when the context is incomplete or wrong?\n\n**On Attribution:** Who designs the weighting algorithms that determine contribution value? How do we prevent these systems from encoding existing inequalities—rewarding those who already have access to the right platforms and tools while excluding those who don’t?\n\n**On Simulation:** What assumptions are we encoding into our simulations? Are we training agents to optimize for efficiency at the expense of fairness? Are we building world-models that reflect how things are or how we wish them to be? How will simulation and prediction interact, including in the use of prediction markets as inputs.\n\nMore broadly: Who decides what problems these primitives solve? The framing is technical—verification, trust, efficiency—but the problems are social and political. The attention economy failed not because we lacked cryptographic tools but because the incentives were misaligned from the start. Advertisers wanted reach; platforms wanted engagement; users wanted neither but tolerated both because there were no alternatives. Freemium means you are the product, paid for at the hands of the advertising value chain. Turns out everything is computer *and* everything is advertising. We must also admit that fraud is not only a byproduct but perhaps an inherent go-to-market strategy emergent in every system; what are the new fraud formats of the agentic age?\n\nWill the agentic economy offer genuine alternatives, or merely new forms of lock-in? Will the primitives serve users or extract value from them? Under-explored here is how users will be able to mix and match agents and flows of activity, arranging relationships around privacy, consent and permissioning of agents, data, and assets along different identities, wallets or keys. For the self-motivated and crypto-enthusiast, the agentic economy opens up tools and parameters to play with end-to-end design and their level of control; for many others, the architecture of convenience may prevail over that of sovereignty. Will the dominant convenient infrastructure be open or proprietary? These questions cannot be answered by examining the technology alone. The work of building the agentic economy is, fundamentally, the work of deciding who has power in digital life. It’s about whether we can scale infrastructure that makes real actions verifiable—that moves us from an economy of fake clicks and bot traffic to one where value flows from cryptographically proven utility. The skeptic might observe that trust, once lost, is not easily rebuilt—and that the people asking us to trust them now are often the same ones who broke that trust before. The technology may be new. The question it poses is old: whether the incentives can be aligned in a way that serves more than the architects themselves.\n\nRegardless, we may be clicking a lot less and following new intentions to their logical limit.\n\nWhatever they might be.\n\n*\\*NB: I have been working on these ideas since before the weekend of Clawdbot/Moltbot/OpenClaw and hope to grapple with how that phenomenon unfolds further parts of the agentic economy separately. Sometimes “done is better than perfect,” and in this case, I’m using writing to explore the potentialities of the primitives as they evolve (which they will; as will my thinking).*\n\n*\\*With special thanks to the brain trust who read, reviewed, or commented on parts of this piece to improve my thinking – all errors in understanding are my own (and hope to be corrected): Seref Yarar, Samuel Klein, Shane Mac, Simran Chana, Oliver Zahn, Jove Oliver, Philipp Kruger, Scott Cohen*\n\nNo posts\n",
        "pipelineQuality": 1,
        "pipelinePassed": true,
        "pipelineReasons": [],
        "words": 5943,
        "links": 33,
        "headings": 1,
        "tokenRecallVsJina": 1,
        "headingRecallVsJina": 0,
        "linkRecallVsJina": 0.906,
        "lengthRatioVsJina": 0.966,
        "overlapScore": 0.805
      },
      {
        "engine": "openrouter_gpt_oss_20b",
        "ok": false,
        "error": "Forced engine openrouter_gpt_oss_20b requires OPENROUTER_API_KEY."
      },
      {
        "engine": "jina_reader",
        "ok": true,
        "title": "Last September, security researchers at HUMAN’s Satori Threat Intelligence team [discovered something](https://www.humansecurity.com/learn/blog/slopads-highly-obfuscated-android-ma",
        "markdown": "# Last September, security researchers at HUMAN’s Satori Threat Intelligence team [discovered something](https://www.humansecurity.com/learn/blog/slopads-highly-obfuscated-android-ma\n\nThe scheme had a cunning feature: it could detect whether someone had found an app organically or arrived via a promotional link. Only in the latter case would the fraud activate, ensuring that advertisers—not users—paid the bill. At its peak, SlopAds generated 2.3 billion fraudulent ad impressions daily. This wasn’t small-time fraud. It was industrial-scale deception, automated and optimized.\n\nThis episode is worth examining not because it was unusual but because it was typical. For two decades, the digital economy has rested on a foundation that Tim Hwang, in his 2020 book _Subprime Attention Crisis_, described as fundamentally suspect: the buying and selling of human attention. Consider the first banner ad, placed by AT&T on HotWired.com in 1994. Its click-through rate was 44%. Today the average banner ad [achieves 0.2%](https://www.aidigital.com/blog/ctr-for-display-ads#:~:text=It's%20not%20uncommon%20for%20high,%E2%80%8D). The problem, as Hwang argued, is structural: attention is almost impossible to verify. An “impression” could come from a person, a bot, or a phone sitting in a server farm. The market, in other words, is subprime—built on assets whose value is structurally unknowable.\n\nWe are now living through what Hwang anticipated—not a dramatic crash but a quiet erosion of trust, accelerated by AI agents that are beginning to navigate digital commerce on behalf of users. These agents promise efficiency: no more clicking through dozens of options to book a flight or find insurance. But they also raise a question: if we cannot verify human attention, how will we verify machine behavior?\n\nThe answer, according to a growing number of technologists and investors betting on this future, lies in four primitives—fundamental building blocks for a new digital economy. These are: **Intention** (systems for expressing and broadcasting what users want), **Context** (verified memory and decisions that agents can draw upon), **Attribution** (cryptographic proof of who contributed what), and **Simulation** (environments for testing agent behavior before deployment). Each primitive rests on a different type of graph: networked data structures that connect, verify, and trace digital activity in ways the current Web cannot. Together, they create the necessary underpinnings of an “Agentic Economy”—a system in which value flows not from clicks or impressions but from verifiable actions.\n\nWhether this system will work, whether it can be deployed at scale, and whether it will serve anyone beyond its architects are questions this essay will examine. But first, it is worth understanding what its builders envision.\n\nThe projections sound outlandish, but reality is catching up. Jensen Huang, NVIDIA’s CEO, declared at the Consumer Electronics Show in January 2025 that the era of agentic AI represents a multi-trillion-dollar opportunity. “[The age of AI Agentics is here,](https://finance.yahoo.com/news/nvidia-jensen-huang-says-ai-044815659.html)” he said. Ramesh Raskar, of MIT, envisions a world in which a [trillion agents](https://www.youtube.com/watch?v=Da6Ya0bfLDA) are on the loose, representing, negotiating, and transacting on our behalf—or for themselves—as parts of complex swarms. Dario Amodei, the CEO of Anthropic, has written of “[a country of geniuses in a datacenter,](https://www.darioamodei.com/essay/machines-of-loving-grace)” made up of millions of AI agents working independently or collaboratively.\n\nThe forecasters agree on little except scale. By 2028, according to Capgemini, AI agents could generate [$450 billion](https://www.capgemini.com/insights/research-library/ai-agents/) in economic value. [McKinsey projects](https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-agentic-commerce-opportunity-how-ai-agents-are-ushering-in-a-new-era-for-consumers-and-merchants) the U.S. retail market alone could see $900 billion to $1 trillion in agent-orchestrated revenue by 2030, with global figures reaching $3 trillion to $5 trillion. The investment firm Lightspeed warns of a [$19.9-trillion](https://lsvp.com/stories/the-ai-agent-economy-has-a-19-trillion-problem-our-investment-in-paid/) economic opportunity at risk if infrastructure gaps aren’t addressed. (For perspective: current global GDP is approximately [$110 trillion](https://data.worldbank.org/indicator/NY.GDP.MKTP.CD)).\n\nNo one knows what this economy will look like or whether it will be desirable. The phrase “empires for no one” has been circulating social media––a way of expressing unease that the world being built may be out of step with what humans actually want. It brings to mind a meme from a few years ago: the eighties band Toto’s buoyant song “Africa” [echoing through empty malls](https://www.newyorker.com/culture/rabbit-holes/the-overwhelming-emotion-of-hearing-totos-africa-remixed-to-sound-like-its-playing-in-an-empty-mall)—cheerful yet eerie, promising abundance yet somehow a result that’s blasphemously ugly. Then there’s the [infamous quote](https://www.youtube.com/shorts/YE5adUeTe_I) by OpenAI CEO Sam Altman from 2015, “I think that AI will probably, most likely, sort of lead to the end of the world. But in the meantime, there will be great companies created with serious machine learning,” which makes almost anyone do a double take. Still, even if AI development stopped today, the tools already deployed could reshape commerce substantially. As long as AI budgets continue to flow, experiments in agent deployment will continue.\n\nWhile we might not know its contours, we can begin to see some of the future economy’s building blocks. A world in which agents and robots form the next consumer class, working with or without us, requires economic rails and a set of functions that enable coordination, orchestration, and autonomous activity. To understand what is being proposed, we must examine each primitive—and the graph infrastructure underneath it—in turn.\n\nThe Internet has no native way to express what we want. We search, we browse, we click—gestures that leave platforms to infer our desires from the digital exhaust we leave behind. The attention economy’s business model depends on this: show people things until something sticks, then charge advertisers for the attempt.\n\nWhat’s replacing “where do I click?” is _programmatic desire_, or _intent_—a cryptographically secured expression of a specific outcome, broadcast to a network of competing agents ready to fulfill it. Rather than clicking through options, a user (or their agent or enterprise) would broadcast: “I need a plumber in Brooklyn this week” or “Find noise-canceling headphones under $200” or “Build a workflow that connects my users to this widget.” Providers would respond with bids. With explicit and compensable needs, transactions would complete automatically.\n\nIntents have several characteristics. First, they are outcome-focused: users often don’t care about implementation details or the steps taken to reach a goal. Second, they carry conditional authorization: users set the parameters or approvals needed to authorize an intent being fulfilled. Third, they are broadcast to a network of “solvers”—humans, agents, or protocols—who compete to fulfill all or part of an intent. And, finally, their outcomes must be verifiable.\n\nConsider the infrastructure required to make this work. At its core is an “Intents Graph”—a dynamic network mapping programmatic desires to agents capable of fulfilling them. This isn’t just a database of wants; it’s a trust-minimized matching system that replaces platform-mediated discovery with direct, verifiable connections. Protocols like Index Network are building the first such graph for social interactions, where agents facilitate connections based on mutual intent. NEAR Intents has been pursuing this logic for financial transactions, forming crypto-asset swaps where outcome efficiency is prioritized regardless of the route agents use to execute swaps.\n\nThe Intents Graph represents a fundamental architectural shift: from inferring what users might want based on their clicks to processing explicit, machine-readable declarations of need. It’s discovery elevated into trust-minimized matching. This kind of system forces relevancy or else it becomes useless. Who wants an old intent fulfilled? Who wants to be spammed? No one. The promise is that once an intent is articulated, it can be fulfilled quickly and relevantly. Behind the scenes is an automagical symphony of agentic activity.\n\nThe appeal is obvious: efficiency without the guesswork. But consider what must be true for this to work. First, users must trust that their broadcasted desires won’t be exploited—that expressing a need for a medical procedure, for instance, won’t result in price discrimination or insurance complications. Second, the verification mechanisms must actually work: the system must confirm that an intent was genuinely fulfilled, not merely claimed to be fulfilled.\n\nThe recent conflict between [Amazon and Perplexity](https://www.pcmag.com/news/amazon-sends-perplexity-a-cease-and-desist-over-its-ai-agents-shopping)—in which AI agents were allegedly making purchases using real user accounts without proper authorization—illustrates the stakes. If agents are to transact based on our behalf, they need verifiable access rights, something like a license system for digital actors, tied to a unique and portable identifier.\n\nOne enabling mechanism is x402, a protocol that revives an old idea. In the original specification for HTTP, status code 402 was reserved for “Payment Required”—a placeholder for micropayments that never materialized because credit-card fees made small transactions unworkable. But the x402 protocol enables agents to execute autonomous, per-request micropayments. Every transaction, from retrieving data to executing a service, becomes a seamless machine-to-machine exchange, settled instantly. Coinbase processed [57 million transactions](https://x.com/Cointelegraph/status/2003677242782609819?s=20) using x402 in December 2025 alone, and Cloudflare and Google have adopted it as well.\n\nThe economic result is _verified spend_—transactions that are cryptographically traceable and tied to specific identities or wallets. In theory, this creates unprecedented efficiency: money flows only toward verified, goal-directed utility. In practice, the system’s robustness depends entirely on its verification mechanisms—a theme that runs through each of the four primitives. The shift from impressions to intents is, at its core, an attempt to move from fake clicks to real, provable actions.\n\nIf the first primitive addresses what we want, the second addresses what agents need to know to deliver it. A concierge who forgets your dietary restrictions or your travel preferences isn’t charming—they’re useless. The same applies to AI agents.\n\nThe problem with most AI systems today is that they operate in “transient memory bubbles.” Each conversation starts from scratch, or nearly so. There is no persistent knowledge base that multiple agents can draw from and contribute to. “Context,” in this sense, means verified, persistent memory and operational state. It’s the difference between an agent that can book a hotel and one that remembers you prefer ground-floor rooms near elevators because of a knee injury mentioned months ago.\n\nAnyone who has used ChatGPT or coding tools like Cursor and has experienced this frustration: having to “re-inject” context repeatedly as interactions deepen, or struggling to port session logs between different AI tools. Context creates consistency and personalization, but it also raises profound questions about ownership and security.\n\nOne emergent solution is a decentralized, verifiable memory store—a “Context Graph” that holds conversational history, transactional records, workflow state. The Context Graph is knowledge infrastructure made queryable and persistent. Advances in zero-knowledge machine learning (zkML), only possible in recent years, have enabled projects like Kinic, which offers verifiable memory stores that users can control. Where might this come in handy? Ray Dalio, the well-known investor and founder of Bridgewater, created an [AI of himself](https://www.principles.com/AIBeta-signup) based on his own data, actions, and applied principles, which you can already interact with online. But without feeding this into a verifiable store, the responses could be manipulated.\n\nResearch supports the importance of persistent memory. U.C. Berkeley’s [MemGPT project](https://par.nsf.gov/servlets/purl/10524107) showed clear performance improvements when agents could access persistent memory; accuracy and relevance scores dropped noticeably when memory systems were disabled. On the LOCOMO benchmark, memory-enabled systems achieved accuracy rates nearly [30% higher](https://mem0.ai/research) than memoryless ones. NVIDIA’s recent [infrastructure announcement](https://nvidianews.nvidia.com/news/nvidia-bluefield-4-powers-new-class-of-ai-native-storage-infrastructure-for-the-next-frontier-of-ai) put a number on it: persistent context can boost processing speeds by up to five times compared to traditional storage.\n\nContext Graphs are a bit of a catch-all, incorporating knowledge graphs, memory graphs and decision graphs on steroids. If they work as designed, context becomes the most valuable asset in the system—a detailed map of preferences, behaviors, and past actions enabling sensemaking. Foundation Capital recently [articulated a vision](https://foundationcapital.com/context-graphs-ais-trillion-dollar-opportunity/) of Context Graphs needed to underpin agents. According to them, agents “sit **in the execution path**. They see the full context at decision time: what inputs were gathered across systems, what policy was evaluated, what exception route was invoked, who approved, and what state was written. If you persist those traces, you get something that doesn’t exist in most enterprises today: a queryable record of how decisions were made.” Later they posit, “Over time, that context graph becomes the real source of truth for autonomy—because it explains not just _what_ happened, but _why it was allowed_ to happen.”\n\nThis sounds elegant until one asks: Who controls access to this truth? Can it be subpoenaed? Can it be hacked? What happens when the context is wrong––when your agent believes you’re allergic to shellfish when you’re not, or when your political preferences recorded three years ago no longer reflect your views? These questions don’t have answers yet, because the systems are too new. But they will need answers soon, especially if context becomes core infrastructure to agentic interaction.\n\nAnimesh Kortana has [framed the problem](https://www.linkedin.com/pulse/how-build-context-graph-animesh-koratana-6abve/) in temporal terms: we’ve built infrastructure for the “state clock” (what’s true now) while neglecting the “event clock” (what happened, when, and why). Your CRM today stores the final deal value but not the negotiation that produced it. This made sense when humans were the reasoning layer. Agents, however, cannot intuit missing causality––they need explicit event histories.\n\nDifferent options for building Context Graphs are emerging. Geo, a tool built by Yaniv Tal—co-founder of The Graph, an indexing protocol for blockchain data—imagines decision context going beyond internal enterprise processes and extending into personal knowledge management. The Geo browser allows users to build a personal knowledge graph automatically as they browse, creating a searchable, AI-queryable history of everything you’ve read, watched, or interacted with online. It’s like having a research assistant who never forgets anything you’ve told them.\n\nThere are other projects using cryptographic proofs to ensure that the information an agent draws from is authentic and hasn’t been tampered with. This matters enormously when agents are making consequential decisions. A medical AI agent prescribing medication based on a patient’s history needs to be certain that history is accurate. A financial agent managing investments needs to trust that market data hasn’t been manipulated.\n\nThis matters enormously when agents are making consequential decisions. A medical AI agent prescribing medication based on a patient’s history needs to be certain that history is accurate. A financial agent managing investments needs to trust that market data hasn’t been manipulated.\n\nRather than storing context as persistent memory, a different architectural approach called [Recursive Language Models](https://alexzhang13.github.io/blog/2025/rlm/) (RLMs) treats context as an external environment that agents actively decompose—programmatically examining inputs and recursively calling themselves on specific snippets. This handles inputs up to 100x beyond standard context windows, not by extending memory but by changing how agents interact with information.\n\nThe implications are significant. An agent coordinating a supply chain might not need a Context Graph storing every relationship if it can recursively examine relevant subsets on demand. This reduces infrastructure costs while potentially increasing robustness. But it raises new verification questions: if context is actively reconstructed rather than passively stored, who controls the decomposition strategy? There may be no canonical record to audit, only ephemeral context reconstructions.\n\nBuilding Context Graphs requires two layers, as Kirk Marple of Graphlit [explains](https://www.graphlit.com/blog/context-layer-ai-agents-need). First, “operational context,” such as, who is Sarah Chen across email, Slack, and meeting transcripts—the same person or three different entities? Who owns which account? These are identity resolution and relationship modeling problems. Once this foundation exists, you can build “decision context”: which policy was evaluated, what exception was invoked, who approved and based on what precedent.\n\nThe result is supposed to be a single, high-fidelity source of truth for agent swarms—allowing complex workflows to span multiple services without losing coherence. This infrastructure is what makes verifiable actions possible: without persistent, queryable context, there’s no way to prove an agent acted correctly or trace why a decision was made. But this raises a question that proponents tend to elide: What happens when different agents have access to different context? When does partial information become misinformation? And who decides what belongs in the canonical record?\n\nThe explosion of generative AI has made it trivially easy to create content and increasingly difficult to determine its origin. Text, images, code, music—all can be synthesized in seconds, often indistinguishably from human work. This collapse of provenance makes trust nearly impossible.\n\nAttribution aims to address this through cryptographic proof of origin and contribution. Chris Dixon of Andreessen Horowitz has argued it is essential to restoring trust online. Peter Wang, cofounder of Anaconda, has proposed a suite of licenses called AMPL (AI Model Public License) to give creators accountability around works and derivatives, similar to Creative Commons.\n\nIn a world of AI agents, this question becomes urgent. When an agent completes a transaction on your behalf—booking a flight, purchasing insurance, ordering groceries—how do we know the transaction happened? How would you prove agent actions within an enterprise workflow? And how do we ensure that the right parties are compensated?\n\nAt the technical level, attribution relies on “Provenance Graphs”—immutable, time-stamped records structured as Directed Acyclic Graphs (DAGs) that track the full lifecycle of digital work. If an image was created by an AI model trained on a particular dataset, refined by a human designer, then modified by another agent, the Provenance Graph captures all of it, step by step.\n\nStartups like CoreTx are building attribution-native architectures where provenance tracking is foundational rather than retrofitted—treating memory, intent, and credit allocation as aspects of a single unified system.\n\nThis goes beyond traditional royalty structures because AI can continuously weigh contributions, giving greater weight to recent or innovative inputs. This could enable faster innovation cycles, particularly in cross-disciplinary settings like scientific research, which has long been hampered by peer review inefficiencies and irreproducibility.\n\nThe mathematical foundation is evolving rapidly. Data Shapley, which uses game theory to assign each contributor a value reflecting their marginal contribution, was previously too computationally expensive for large systems. Recent breakthroughs enable attribution calculation in a single training run. When an agent synthesizes context from multiple sources to make a recommendation, [Data Shapley-style techniques](https://arxiv.org/pdf/2406.11011) can decompose the recommendation’s value across each input, enabling proportional compensation (or even proportional influence on synthesis).\n\nTraditional attribution systems rely on cookies, which track your movements across the Web and credit conversions to the last ad you clicked. This model is breaking down for two reasons. First, privacy regulations and browser changes are killing third-party cookies. When Google tested disabling them for 1% of Chrome users in the first quarter of 2024, programmatic-advertising revenue dropped by 21-34%. Rather than proceeding with deprecation, Google quietly reversed course in July, revealing the economic dependency: losing those cookies would cost shareholders approximately $2 billion annually. Alternative identifier networks like LiveRamp achieve only 28% match rates because they depend on users sharing the same login credentials across sites, which happens infrequently. The entire attribution infrastructure is held together by duct tape and stockholder pressure.\n\nSecond, AI agents make cookies obsolete. Agents spin up fresh browser instances for every request, eliminating persistent identifiers. They can trivially strip affiliate links or insert their own, farming commissions in ways that are undetectable for agents running on local operating systems. Without cryptographic proof of genuine engagement, every impression metric becomes as unreliable as blockchain “active wallets.”\n\nInstead of relying on tracking pixels and cookies, transactions are recorded on a blockchain or verified through zero-knowledge proofs. When an agent completes a purchase, it generates a cryptographic receipt proving that the transaction occurred, who facilitated it, and which party deserves credit. Companies like Opacity Network are building these systems, enabling agents to verify attributable outcomes across platforms while preserving privacy.\n\nVarious standards are emerging. ERC-8004, from the Ethereum community, introduces “trustless agents” through two registries: an Identity Registry assigning each agent a portable AgentID, and a Reputation Registry standardizing on-chain performance feedback. This creates something like a passport for agents––a portable identity with work history and trust score.\n\nThe economic outcome is “_verified contribution_”—fractional, tokenized ownership of intellectual property based on proven contribution history. The appeal is obvious: no more fraud, no more ambiguity about who gets paid. Provenance Graphs make this possible by creating an auditable trail from object origin to use or incorporation—the infrastructure needed to replace the noise with real, compensable actions. The “value” field that has always existed in analytics, is upgraded with verifiable actions and crypto rails, turning attribution into a real market primitive rather than just a historical reporting metric, that feeds future pricing, budget allocation, and automated spend.\n\nBut the implementation is complex. Cryptographic attribution requires infrastructure that doesn’t yet exist at scale. It also requires agreement on standards—who decides what counts as a valid attribution event or the weighting algorithms? If the verification layer is controlled by a small number of companies, we’ve simply traded one oligopoly for another. If contribution is continuously recalculated, who prevents gaming of the system? Also, what happens when the graph is incomplete––when someone’s contribution isn’t recorded because they didn’t use the right tools or platform? Third, and most important: does the existence of a cryptographic receipt prove that value was actually created, or merely that a transaction occurred?\n\nHow do you predict the behavior of a market before it exits? Traditional economic models assume rational actors optimizing predictably. AI agents are not rational in any human sense—on the one hand, they can be more economically rational, on the other hand, they can be tricked, manipulated, or behave in [unanticipated (or anticipated) ways](https://arxiv.org/abs/2406.01382). The solution is simulation: modeling emergent market dynamics by observing autonomous AI actors in controlled environments.\n\nMicrosoft’s Magentic Marketplace is a simulated economy where researchers observe how agents respond to incentives and threats. ElizaOS uses coordinated AI workers in synthetic environments (agents playing Minecraft, essentially) to generate behavioral data. Stanford [Smallville](https://arxiv.org/pdf/2304.03442) back in 2023 was an early example of observing generative agents in an artificial society: inspired by the Sims game, 25 agents went about their days in a simulated town and showed that “Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools.” Increasingly, simulation of multi-agent[“societies of thought”](https://arxiv.org/abs/2601.10825)—essentially conversations and debate between varying personas—is producing better chains of reasoning. Additional research by Ben Manning and John Horton at MIT investigates AI agents as [“a general vessel through which theory can be flexibly applied to any setting](https://arxiv.org/pdf/2508.17407)” to accurately simulate human data.\n\nSuch simulations serve varying purposes. First, they can help us better understand real humans, since models have absorbed a lot about how humans behave and how to predict that behavior. Second, they can increasingly allow developers to test agent behavior before deployment—to see how a shopping agent responds to price fluctuations or how a travel agent handles rebooking during a hypothetical hurricane. Third, they generate training data. Watching agents interact in simulation provides behavioral patterns that can improve real-world deployment. Create the scenario and watch how it performs.\n\nRecent work demonstrates how simulation can generate training data for predictive agents at scale. Researchers at [OpenForecaster](https://arxiv.org/pdf/2512.25070) synthesized over 52,000 forecasting questions from news events, training an 8-billion-parameter model that matched models 10x larger while addressing the “information leakage” problem through time-stamped snapshots. [Complementary work](https://benjaminmanning.io/files/optimize.pdf)at MIT showed that theory-grounded AI agents can predict human behavior in entirely novel strategic games, outperforming Nash equilibria and cognitive hierarchy models by 2.4x-3.4x—but only when validated across multiple distinct scenarios. Both studies point to the same challenge: creating simulation environments that accurately represent uncertainty without inadvertently encoding answers or overfitting to specific contexts.\n\nThe stakes for simulation extend beyond optimization. The Google DeepMind team[characterizes](https://arxiv.org/abs/2509.10147) this testing imperative more starkly: we’re on a trajectory toward spontaneous emergence of a ‘sandbox economy’—agents transacting at scales and speeds beyond human oversight. The choice, they argue, isn’t whether this economy emerges but whether it emerges _intentionally_ (designed for safety) or _accidentally_ (with uncontrolled risks). Their framework distinguishes between permeable economies (agents interacting with the human economy) and impermeable ones (sealed testing environments). The challenge is that the most useful agent economies will be highly permeable—enabling agents to book flights, negotiate deals, coordinate supply chains—which is precisely what makes them systemically risky.\n\nSimulation becomes not just optimization but existential: the ability to stress-test market mechanisms before deploying them into an economy where trillions of dollars flow through agent decisions. These behavioral network graphs—the infrastructure underlying simulation—map how agents interact, compete, and coordinate under different conditions. They’re what make it possible to ask: will this system produce real value or just automate fraud at a higher scale?\n\nCompanies like Modulus Labs and Zama are building these simulation environments, often using zero-knowledge proofs to allow agents to run privately—meaning competitors can’t see your testing strategies or reverse-engineer your agent’s decision-making. The goal is to create a sandbox where agents can fail safely, learning through iteration without real-world consequences.\n\nBut simulation raises philosophical questions. How do we know the simulation is accurate? A model is only as good as its assumptions, and we must make inductive leaps to believe predictions sourced from black-boxes hold true. If the simulation doesn’t account for edge cases—sudden regulatory changes or an unexpected market shock—the agent will be unprepared when they occur. Moreover, agents trained in simulation may develop strategies that work in the synthetic world but fail in reality’s messiness.\n\nThese challenges echo older debates about structured prediction. [The Delphi Method](https://www.rand.org/content/dam/rand/pubs/papers/2008/P3558.pdf), developed by Olaf Helmer at RAND in the 1950s and 60s, pioneered systematic approaches to forecasting uncertain futures through structured expert consensus. Helmer’s framework emphasized sequential interrogation with feedback loops, avoiding the biases of committee groupthink while building toward convergence. The method’s core insight—that forecasting requires explicit mechanisms to surface assumptions, challenge them, and refine predictions iteratively—applies directly to agent simulation. Where Delphi used human experts and questionnaires, modern simulation uses synthetic agents and behavioral networks, not possible in 1967! But both face the same fundamental problem: how do you validate a model of the future before that future exists?\n\nThere’s also overfitting risk. An agent optimized for a specific simulation might perform brilliantly in testing and poorly in deployment. This is a well-known problem in machine learning, and there’s no reason to believe it won’t plague agentic systems as well.\n\nThe historical parallel is instructive. Helmer and his colleagues recognized early on that prediction methods could converge on false certainty if not carefully designed—what they called “reliability of estimates.” They built safeguards through anonymity, controlled feedback, and explicit documentation of reasoning. Modern agent simulation faces an updated version of this challenge: agents can converge on strategies that work brilliantly in the synthetic environment while failing in reality’s messier contexts. The solution, both then and now, involves making the reasoning process explicit and testable. Helmer’s work emphasized that the value of structured forecasting wasn’t just in getting predictions right, but in understanding why experts believed what they believed. Similarly, the value of agent simulation may lie less in perfectly predicting agent behavior than in making the embedded assumptions—about incentives, constraints, and goals—visible and debuggable.\n\nMore fundamentally: if we’re simulating markets before they exist, we’re essentially choosing which market dynamics to encode. These are not neutral choices. A simulation that prioritizes efficiency over fairness will train agents to optimize for efficiency. A simulation that ignores power asymmetries will produce agents blind to them. Part of what’s different is the scale of what we can simulate. We are building not just tools but world-models—and the assumptions embedded in those models will shape the economy they help create.\n\nIf these four primitives—Intention, Context, Attribution, and Simulation—take hold, their most immediate impact may be on digital advertising; a $600 billion industry built on a model that is visibly failing.\n\nThe traditional model worked thus: advertisers paid to put their message in front of as many people as possible, hoping some small fraction would click, and an even smaller fraction would convert. Publishers got paid for impressions—eyeballs, not outcomes. The system was riddled with fraud: bot traffic, fake impressions, click farms. Tim Hwang’s _Subprime Attention Crisis_ laid bare the economics: billions in wasted spend, widespread privacy violations, and a market built on assets whose value could never truly be known.\n\nThe agentic model proposes something different. Instead of chasing clicks, it rewards utility. The focus shifts from impressions to conversion efficiency: Did the agent actually fulfill the user’s intent? The tools to make this transformation possible are arriving. One concrete example of this shift: Google Research is [already modeling advertisers](https://research.google/blog/mechanism-design-for-large-language-models/) as self-interested LLM agents and coordinating ad creation through auction mechanisms rather than impressions or clicks.\n\nZero-knowledge transport-layer security (zkTLS) allows agents to prove they completed actions—clicked an ad, watched a video, made a purchase—without exposing underlying user data. Stablecoins eliminate the payment friction inherent in cross-border and micropayment-heavy transactions. Together, these reduce infrastructure costs: where traditional programmatic advertising keeps only 55 cents of every dollar for publishers (45 cents lost to intermediaries), crypto-enabled systems could collapse margins dramatically, [according to a report by venture firm Escape Velocity](https://ev3.xyz/research/letters/Advertising_from_First_Principles/).\n\nPayments would be automated by smart contracts, triggered only upon verifiable proof of fulfillment. Each primitive plays a role: Intention (via the Intents Graph) allows advertising to shift from demographic targeting to fulfilling explicit, programmatic desires. Attribution (via the Provenance Graph) eliminates fraud through verified contribution. Context (via the Context Graph) allows agents to access high-fidelity, user-controlled data for dynamic creative optimization—ads that are relevant without being invasive. Simulation (via behavioral network graphs) lets advertisers stress-test bidding and placement strategies in advance, optimizing for maximum return before any budget is spent.\n\nIt’s an elegant vision that conveniently aligns advertiser interests (efficiency) with user interests (privacy and relevance). Whether it will actually work—whether the infrastructure can be deployed at scale, the incentives aligned, and the inevitable attempts at exploitation repelled—remains to be seen.\n\nThe history of the Internet suggests caution. Every previous attempt to fix advertising has produced, eventually, new and more sophisticated versions of old problems. But the fundamental architecture being proposed here is different: graphs that make actions verifiable rather than inferred, infrastructure that replaces fake clicks with cryptographic proof of real choices.\n\nThe agentic economy is not inevitable. It is a possibility, not a prophecy—a set of architectural answers to the endemic trust failures of Web 2.0. The four primitives create, in theory, a closed loop: goal setting, knowledge pooling, value assignment, and resilience testing. Each graph reinforces the others, and value can flow across them unobstructed. Verified spend can depend on verified context, which can utilize verified contribution, all of it stress-tested through continuous simulation and learning. Yet the path forward is a delicate one. Some of the primitives will combine in different permutations, and this may also take a while. Likely, the result is not one cohesive end state, but rather a slew of different companies. As Seref Yarar from Index Network described “simulation, for example, is needed in both enterprise and consumer markets,” so we will see it emerge in various forms.\n\nThe primary challenge for this agentic future, even advocates acknowledge, is governance. Autonomy has limits. For high-stakes transactions, human oversight remains essential—a reality acknowledged in what some developers call the Meta AI [“Agents Rule of Two.”](https://ai.meta.com/blog/practical-ai-agent-security/) In this framework, agents are limited to two sensitive capabilities: reading untrusted input, accessing private data, _or_ taking external action. The third requires explicit human approval. An agent with unchecked access to your bank account, your e-mail, and purchases authority is currently a liability, not a feature.\n\nThe constraint reintroduces the very friction the system aims to eliminate: delay, human error, the possibility of bad judgment. Striking the right balance—enough autonomy to be useful, enough control to be safe—requires not just technical solutions but legal and ethical frameworks for non-human economic actors.\n\nWhat rights, if any, does an AgentID have? Who is liable when an agent makes a mistake? Who is responsible when an agent trained on biased data produces discriminatory outcomes? These questions barely exist in current law. If people will outsource their decisions to agents, how that power is ceded or reversed matters.\n\nIf the agentic economy does take hold and scale to a trillion agents, as some predict, value capture will shift dramatically. The extractive model of Web 2.0—proprietary data harvested and monetized without consent—could give way to transparent, auditable protocol fees. Compensation would flow from verifiable execution and context maintenance. Power would accrue not to platforms collecting the most data but to the entities maintaining the most trusted infrastructure. And, potentially, to the agents doing the best work.\n\nThis topic has only just started and several critical questions remain not just unanswered but largely unasked:\n\n**On Intention:** How do we prevent the broadcast of intent from becoming a new form of surveillance? If every desire is machine-readable, who reads it and what do they do with that information? Will intention match volition, or will even personal agents have a new hill to climb: maximizing the velocity of their owner’s consumption, decluttering, addiction, therapy, repetition?\n\n**On Context:** Who controls the canonical record? When different actors have access to different contexts, who adjudicates disputes about what actually happened? What are the consequences when the context is incomplete or wrong?\n\n**On Attribution:** Who designs the weighting algorithms that determine contribution value? How do we prevent these systems from encoding existing inequalities—rewarding those who already have access to the right platforms and tools while excluding those who don’t?\n\n**On Simulation:** What assumptions are we encoding into our simulations? Are we training agents to optimize for efficiency at the expense of fairness? Are we building world-models that reflect how things are or how we wish them to be? How will simulation and prediction interact, including in the use of prediction markets as inputs.\n\nMore broadly: Who decides what problems these primitives solve? The framing is technical—verification, trust, efficiency—but the problems are social and political. The attention economy failed not because we lacked cryptographic tools but because the incentives were misaligned from the start. Advertisers wanted reach; platforms wanted engagement; users wanted neither but tolerated both because there were no alternatives. Freemium means you are the product, paid for at the hands of the advertising value chain. Turns out everything is computer _and_ everything is advertising. We must also admit that fraud is not only a byproduct but perhaps an inherent go-to-market strategy emergent in every system; what are the new fraud formats of the agentic age?\n\nWill the agentic economy offer genuine alternatives, or merely new forms of lock-in? Will the primitives serve users or extract value from them? Under-explored here is how users will be able to mix and match agents and flows of activity, arranging relationships around privacy, consent and permissioning of agents, data, and assets along different identities, wallets or keys. For the self-motivated and crypto-enthusiast, the agentic economy opens up tools and parameters to play with end-to-end design and their level of control; for many others, the architecture of convenience may prevail over that of sovereignty. Will the dominant convenient infrastructure be open or proprietary? These questions cannot be answered by examining the technology alone. The work of building the agentic economy is, fundamentally, the work of deciding who has power in digital life. It’s about whether we can scale infrastructure that makes real actions verifiable—that moves us from an economy of fake clicks and bot traffic to one where value flows from cryptographically proven utility. The skeptic might observe that trust, once lost, is not easily rebuilt—and that the people asking us to trust them now are often the same ones who broke that trust before. The technology may be new. The question it poses is old: whether the incentives can be aligned in a way that serves more than the architects themselves.\n\nRegardless, we may be clicking a lot less and following new intentions to their logical limit.\n\nWhatever they might be.\n\n_*NB: I have been working on these ideas since before the weekend of Clawdbot/Moltbot/OpenClaw and hope to grapple with how that phenomenon unfolds further parts of the agentic economy separately. Sometimes “done is better than perfect,” and in this case, I’m using writing to explore the potentialities of the primitives as they evolve (which they will; as will my thinking)._\n\n_*With special thanks to the brain trust who read, reviewed, or commented on parts of this piece to improve my thinking – all errors in understanding are my own (and hope to be corrected): Seref Yarar, Samuel Klein, Shane Mac, Simran Chana, Oliver Zahn, Jove Oliver, Philipp Kruger, Scott Cohen_\n",
        "pipelineQuality": 1,
        "pipelinePassed": true,
        "pipelineReasons": [],
        "words": 5741,
        "links": 32,
        "headings": 1,
        "tokenRecallVsJina": 1,
        "headingRecallVsJina": 1,
        "linkRecallVsJina": 1,
        "lengthRatioVsJina": 1,
        "overlapScore": 1
      },
      {
        "engine": "cloudflare_markdown",
        "ok": false,
        "error": "Forced engine cloudflare_markdown failed: Markdown negotiation returned HTML instead of markdown."
      }
    ]
  }
]